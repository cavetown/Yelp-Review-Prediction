{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU RNN\n",
    "This file is to self implement a GRU RNN in hopes for the Yelp Dataset Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import langdetect\n",
    "import operator\n",
    "import joblib\n",
    "import itertools\n",
    "\n",
    "# import utils\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU_rnn class and Computation Graph Building\n",
    "\n",
    "__init__: this part of the code takes in parameters to be used for matrix dimensions later\n",
    "\n",
    "__graph__: main part that constructs the computation graph. It first declares placeholders and variables that be used at the start. The __step__ function performs computations for all the gates and states for all layers then stores the results and returns it stacked\n",
    "\n",
    "tf.scan is used for faster computation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "class GRU_rnn():\n",
    "    def __init__(self, state_size, num_classes, num_layers,\n",
    "                ckpt_path='ckpt/gru2/',\n",
    "                model_name='gru2'):\n",
    "        # Initialize parameter variables for GRU\n",
    "        self.state_size = state_size\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # build graph ops\n",
    "        \n",
    "        def __graph__():\n",
    "            tf.reset_default_graph() # Clears graph stack\n",
    "            X_input = tf.placeholder(shape=[None, None], dtype=tf.int32)\n",
    "            y_input = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "            \n",
    "            # embeddings\n",
    "            embs = tf.get_variable('emb', [num_classes, state_size])\n",
    "            rnn_inputs = tf.nn.embedding_lookup(emb, X_input)\n",
    "            # initial hidden state\n",
    "            inital_state = tf.placeholder(shape=[num_layers, None, state_size],\n",
    "                                         dtype=tf.float32, name='initial_state')\n",
    "            # initializer\n",
    "            xavier_init = tf.contrib.layers.xavier_initializer\n",
    "            # Params\n",
    "            W = tf.get_variable('W',\n",
    "                               shape=[num_layers, 3, self.state_size, self.state_size],\n",
    "                               initializer=xavier_init())\n",
    "            U = tf.get_variable('U', \n",
    "                                shape=[num_layers, 3, self.state_size, self.state_size],\n",
    "                               initializer=xavier_init())\n",
    "            b = tf.get_variable('b',\n",
    "                               shape=[num_layers, self.state_size],\n",
    "                               initializer=tf.constant_initializer(0.))\n",
    "            \n",
    "            def step(st_1, x):\n",
    "                st = []\n",
    "                x_in = x\n",
    "                for i in range(num_layers):\n",
    "                    # Update Gate\n",
    "                    z = tf.sigmoid(tf.matmul(x_in, U[i][0]) + tf.matmul(st_1[i], W[i][0]))\n",
    "                    # Reset Gate\n",
    "                    r = tf.sigmoid(tf.matmul(x_in, U[i][1]) + tf.matmul(st_1[i], W[i][1]))\n",
    "                    # hidden gate\n",
    "                    h = tf.tanh(tf.matmul(x_in, U[i][2]) + tf.matmul((r*st_1[i]), W[i][2]))\n",
    "                    # New State\n",
    "                    st_i = (1-z)*h + (z*st_1[i])\n",
    "                    x_in = st_i\n",
    "                    st.append(st_i)\n",
    "                return tf.stack(st)\n",
    "            \n",
    "            ###\n",
    "            # tf scan operation for faster computation\n",
    "            ###\n",
    "            states = tf.scan(step,\n",
    "                            tf.transpose(rnn_inputs, [1,0,2]),\n",
    "                            initializer=init_state)\n",
    "            ###\n",
    "            # Get last state before reshape\n",
    "            ###\n",
    "            last_state = states[-1]\n",
    "            \n",
    "            ###\n",
    "            # Predictions\n",
    "            ###\n",
    "            V = tf.get_variable('V', shape=[state_size, num_classes],\n",
    "                               initializer=xavier_init())\n",
    "            bo = tf.get_variable('bo', shape=[num_classes],\n",
    "                                initializer=tf.constant_initializer(0.))\n",
    "            \n",
    "            states = tf.transpose(states, [1,2,0,3])[-1]\n",
    "            # Flatten to 2d for matmul with V\n",
    "            states_flat = tf.reshape(states, [-1, state_size])\n",
    "            logits = tf.matmul(states_flat, V) + bo\n",
    "            # predictions\n",
    "            predictions = tf.nn.softmax(logits)\n",
    "            \n",
    "            #losses = tf.keras.losses.sparse_categorical_crossentropy()\n",
    "            losses = tf.nn.spare_softmax_cross_entropy_with_logits(logits, y_input)\n",
    "            loss = tf.reduce_mean(losses)\n",
    "            \n",
    "            train_op = tf.train.AdagradOptimizer(learning_rate=0.1).minimize(loss)\n",
    "            \n",
    "            self.X_input = X_input\n",
    "            self.y_input = y_input\n",
    "            self.loss = loss\n",
    "            self.train_op = train_op\n",
    "            self.predictions = predictions\n",
    "            self.last_state = last_state\n",
    "            self.init_state = init_state\n",
    "        sys.stdout.write('\\n<log> Building Graph...')\n",
    "        __graph__()\n",
    "        sys.stdout.write('</log>\\n')\n",
    "    \n",
    "    \"\"\"\n",
    "    Training\n",
    "    \"\"\"\n",
    "    def train(self, train_set, epochs=100):\n",
    "        # training session\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            train_loss = 0\n",
    "            try:\n",
    "                for i in range(epochs):\n",
    "                    for j in range(300):\n",
    "                        X_input, y_input = train_set.__next__()\n",
    "                        batch_size = X_input.shape[0]\n",
    "                        _, train_loss_ = sess.run([self.train_op, self.loss], feed_dict={\n",
    "                            self.X_input : X_input,\n",
    "                            self.y_input: y_input.flatten(),\n",
    "                            self.init_state : np.zeros([self.num_layers, batch_size, self.state_size])\n",
    "                        })\n",
    "                        train_loss += train_loss_\n",
    "                    print('[{}] loss : {}'.format(i,train_loss/300))\n",
    "                    train_loss = 0\n",
    "            except KeyboardInterrupt:\n",
    "                print('interrupted by user at ' + str(i))\n",
    "                \n",
    "            ###\n",
    "            # Training ends here, save checkpoint\n",
    "            ###\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, self.ckpt_path + self.model_name, global_step=i)\n",
    "    \"\"\"\n",
    "    Generate Characters\n",
    "    \"\"\"\n",
    "    def generate(self, idx2word, word2idx, num_words=100, div=' '):\n",
    "        ###\n",
    "        # Generate Text\n",
    "        ###\n",
    "        random_init_word = random.choice(idx2word)\n",
    "        current_word = word2idx[random_init_word]\n",
    "        \n",
    "        ###\n",
    "        # Start sess\n",
    "        ###\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # restore session\n",
    "            ckpt = tf.train.get_checkpoint_state(self.ckpt_path)\n",
    "            saver = tf.train.Saver()\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            # Generate Operation\n",
    "            words = [current_word]\n",
    "            state = None\n",
    "            for i in range(num_words):\n",
    "                if state:\n",
    "                    feed_dict = {self.X_input : np.array([current_word]).reshape([1,1]),\n",
    "                                self.init_state : state_}\n",
    "                else:\n",
    "                    feed_dict = {self.X_input : np.array([current_word]).reshape([1,1]),\n",
    "                                self.init_state : np.zeros([self.num_layers, 1, self.state_size])}\n",
    "                # Forward Prop\n",
    "                preds, state_ = sess.run([self.predictions, self.last_state], feed_dict=feed_dict)\n",
    "                \n",
    "                # set flag to true\n",
    "                state = True\n",
    "                \n",
    "                # set new word\n",
    "                current_word = np.random.choice(preds.shape[-1], 1, p=np.squeeze(preds))[0]\n",
    "                # add to list of words\n",
    "                words.append(current_word)\n",
    "                \n",
    "        return div.join([idx2word[w] for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Arguments\n",
    "Function underneath allows user to specify whether they want to continue training the network or generate words (along with specify the number of words). Generally good habit to include to so command line can run the code in multiple ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# parse arguments\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Stacked Gated Recurrent Unit RNN for Text Hallucination, built with tf.scan')\n",
    "    group = parser.add_mutually_exclusive_group(required=True)\n",
    "    group.add_argument('-g', '--generate', action='store_true',\n",
    "                        help='generate text')\n",
    "    group.add_argument('-t', '--train', action='store_true',\n",
    "                        help='train model')\n",
    "    parser.add_argument('-n', '--num_words', required=False, type=int,\n",
    "                        help='number of words to generate')\n",
    "    args = vars(parser.parse_args())\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_batch_gen(X_input, y_input, batch_size, seq_len):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# main function\n",
    "if __name__ == '__main__':\n",
    "    # parse arguments\n",
    "    args = parse_args()\n",
    "    #\n",
    "    # fetch data\n",
    "    ###\n",
    "    # This part will be different depending how our data is loaded\n",
    "    # Look into GLoVE embedding**\n",
    "    ###\n",
    "    #\n",
    "    # create the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
