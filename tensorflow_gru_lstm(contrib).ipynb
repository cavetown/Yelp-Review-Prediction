{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU LSTM\n",
    "Attempts to create a GRU LSTM model to use for Yelp Dataset Challenge\n",
    "Based off of TensorFlow Tutorial: https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "\n",
    "import utils\n",
    "import util\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gru_lstm class\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.flags\n",
    "flags.DEFINE_string(\"data_path\", None, \"Where the training/test data is stored.\")\n",
    "flags.DEFINE_string(\"save_path\", None, \"Model output directory.\")\n",
    "flags.DEFINE_string(\"training_files\", \"person_match.train2\", \"training file (default: None)\")  #for sentence semantic similarity use \"train_snli.txt\"\n",
    "\n",
    "flags.DEFINE_integer(\"embedding_dim\", 300, \"Dimensionality of character embedding (default: 300)\")\n",
    "flags.DEFINE_float(\"dropout_keep_prob\", 0.8, \"Dropout keep probability (default: 1.0)\")\n",
    "flags.DEFINE_integer(\"hidden_units\", 100, \"Number of hidden units (default:50)\")\n",
    "flags.DEFINE_integer(\"num_layers\", 2, \"Number of units (default: 2)\")\n",
    "\n",
    "# Training parameters\n",
    "flags.DEFINE_integer(\"batch_size\", 128, \"Batch Size (default: 64)\")\n",
    "flags.DEFINE_integer(\"num_epochs\", 30, \"Number of training epochs (default: 200)\")\n",
    "flags.DEFINE_integer(\"evaluate_every\", 1000, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "flags.DEFINE_integer(\"checkpoint_every\", 1000, \"Save model after this many steps (default: 100)\")\n",
    "\n",
    "# Misc Parameters\n",
    "flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses tf.contrib to generate GRU cell\n",
    "# Saves implementing own GRUCell from scratch\n",
    "# Based off of PTB example: # https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py\n",
    "\n",
    "def data_type():\n",
    "    return tf.float32\n",
    "\n",
    "class gru_lstm(object):\n",
    "    def __init__(self, X_inputs, y_labels, num_steps, batch_size, is_training, \n",
    "                 vocab_size, embedding_size, state_size, num_layers, ckpt_path='ckpt/gru2/'):\n",
    "        \n",
    "        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "        self.vocab_size = vocab_size # Dont know if need\n",
    "        self.embedding_size = embedding_size\n",
    "        self.state_size = hidden_units\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self._is_training = is_training\n",
    "        \n",
    "        self._cell = None\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, state_size], dtype=data_type())\n",
    "        \n",
    "        inputs = tf.nn.embedding_lookup(embedding)\n",
    "        if is_training and keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "        \n",
    "        # Output is Neural Network\n",
    "        output, state = self.stacked_gru_graph(X_inputs, state_size, keep_prob, num_layers, is_training)\n",
    "        \n",
    "        W = tf.get_variable(\"W\", [size, vocab_size], dtype=data_type())\n",
    "        b = tf.get_variable(\"b\", [vocab_size], dtype=data_type())\n",
    "        \n",
    "        \n",
    "        logits = tf.nn.xw_plus_b(output, W, b)\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "        \n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logits,\n",
    "                                                y_labels,\n",
    "                                                tf.ones([self.batch_size, self.num_steps], dtype=data_type()),\n",
    "                                                average_across_timesteps=False,\n",
    "                                                average_across_batch=True)\n",
    "                                               \n",
    "        \n",
    "        self._cost = tf.reduce_sum(loss)\n",
    "        self._final_state = state\n",
    "        \n",
    "        self._learn_rate = tf.Variable(0.0, trainable=False) # Learning Rate\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars),\n",
    "                                          config.max_grad_norm)\n",
    "        train_vars = tf.trainable_variables() # Returns all variables specified trainable=True\n",
    "        \n",
    "        optimizer = tf.train.RMSPropOptimizer(self._learn_rate) # Using RMSProp for RNN's rather than SGD as usual\n",
    "        self._train_op = optimizer.apply_gradients(zip(gradients, train_vars),\n",
    "                                                   global_step=tf.train.get_or_create_global_step())\n",
    "        \n",
    "        self._new_learn_rate = tf.placeholder(tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "        self._learn_update = tf.assign(self._learn_rate, self._new_learn_rate)\n",
    "    \n",
    "    def gru_cell(self, hidden_state_size, keep_prob, is_training, layer, num_layers):\n",
    "    \n",
    "        assert isinstance(keep_prob, float)\n",
    "        assert isinstance(hidden_state_size, int)\n",
    "        assert isinstance(is_training, bool)\n",
    "        assert isinstance(layer, int)\n",
    "        assert isinstance(num_layers, int)      \n",
    "        if num_layers <= 0:\n",
    "            raise AssertionError(\"Need positive number of layers\")\n",
    "    \n",
    "        cell = tf.contrib.rnn.GRUCell(hidden_state_size)\n",
    "    \n",
    "        if layer == num_layers - 1:\n",
    "            return tf.contrib.rnn.OutputProjectionWrapper(cell,\n",
    "                                                          output_size=6)\n",
    "        elif is_training and keep_prob < 1:\n",
    "            return tf.contrib.rnn.DropoutWrapper(cell,\n",
    "                                                 output_keep_prob=keep_prob)\n",
    "        else:\n",
    "            return cell\n",
    "        \n",
    "    def stacked_gru_graph(self, X_inputs, state_size, keep_prob, num_layers, is_training):\n",
    "        \n",
    "        assert isinstance(state_size, int)\n",
    "        assert isinstance(keep_prob, float)\n",
    "        if keep_prob < 0 or keep_prob > 1:\n",
    "            raise AssertionError(\"needs to be a value between 0 and 1\")\n",
    "        assert isinstance(num_layers, int)\n",
    "        assert isinstance(is_training, bool)\n",
    "        \n",
    "        stacked_gru = tf.contrib.rnn.MultiRNNCell(\n",
    "            cells=[gru_cell(state_size, keep_prob, _, num_layers, is_training) for _ in range(num_layers)], \n",
    "            state_is_tupl=True)\n",
    "        \n",
    "        inputs = tf.unstack(X_inputs, num=num_steps, axis=1)\n",
    "        \n",
    "        self._initial_state = cell.zero_state(config.batch_size, data_type())\n",
    "        \n",
    "        outputs, state = tf.contrib.rnn.static_rnn(cell, inputs,\n",
    "                                                   initial_state=self._initial_state)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    def assign_lr(self, session, lr_value):\n",
    "        \"\"\"\n",
    "        :param: lr_value- new learning rate value multiplied by decay rate\n",
    "        \"\"\"\n",
    "        assert isinstance(lr_value, float)\n",
    "        session.run(self._learn_update, feed_dict={self._new_learn_rate: lr_value})\n",
    "        \n",
    "    def export_ops(self, name):\n",
    "        \"\"\"Exports ops to collections.\"\"\"\n",
    "        self._name = name\n",
    "        ops = {util.with_prefix(self._name, \"cost\"): self._cost}\n",
    "        if self._is_training:\n",
    "            ops.update(lr=self._lr, new_lr=self._new_lr, lr_update=self._lr_update)\n",
    "        for name, op in ops.items():\n",
    "            tf.add_to_collection(name, op)\n",
    "        self._initial_state_name = util.with_prefix(self._name, \"initial\")\n",
    "        self._final_state_name = util.with_prefix(self._name, \"final\")\n",
    "        util.export_state_tuples(self._initial_state, self._initial_state_name)\n",
    "        util.export_state_tuples(self._final_state, self._final_state_name)\n",
    "        \n",
    "    def import_ops(self):\n",
    "    \"\"\"Imports ops from collections.\"\"\"\n",
    "        if self._is_training:\n",
    "            self._train_op = tf.get_collection_ref(\"train_op\")[0]\n",
    "            self._learn_rate = tf.get_collection_ref(\"lr\")[0]\n",
    "            self._new_learn_rate = tf.get_collection_ref(\"new_lr\")[0]\n",
    "            self._learn_update = tf.get_collection_ref(\"lr_update\")[0]\n",
    "            \n",
    "        self._cost = tf.get_collection_ref(util.with_prefix(self._name, \"cost\"))[0]\n",
    "        num_replicas = 0\n",
    "        self._initial_state = util.import_state_tuples(self._initial_state,\n",
    "                                                       self._initial_state_name,\n",
    "                                                       num_replicas)\n",
    "        self._final_state = util.import_state_tuples(self._final_state,\n",
    "                                                     self._final_state_name,\n",
    "                                                     num_replicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(session, model, eval_op=None, verbose=False):\n",
    "    \"\"\"Runs the model on the given data.\"\"\"\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "\n",
    "    fetches = {\n",
    "        \"cost\": model.cost,\n",
    "        \"final_state\": model.final_state,\n",
    "    }\n",
    "    if eval_op is not None:\n",
    "        fetches[\"eval_op\"] = eval_op\n",
    "\n",
    "    for step in range(model.input.epoch_size):\n",
    "        feed_dict = {}\n",
    "        for i, (c, h) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "\n",
    "    vals = session.run(fetches, feed_dict)\n",
    "    cost = vals[\"cost\"]\n",
    "    state = vals[\"final_state\"]\n",
    "\n",
    "    costs += cost\n",
    "    iters += model.input.num_steps\n",
    "\n",
    "    if verbose and step % (model.input.epoch_size // 10) == 10:\n",
    "        print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "            (step * 1.0 / model.input.epoch_size, np.exp(costs / iters), \n",
    "             iters * model.input.batch_size * max(1, FLAGS.num_gpus) /\n",
    "             (time.time() - start_time)))\n",
    "\n",
    "    return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data():\n",
    "    pass\n",
    "\n",
    "def get_test_data():\n",
    "    pass\n",
    "\n",
    "def main(_):\n",
    "    raw_data = reader.ptb_raw_data(FLAGS.data_path)\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        initializer = tf.random_uniform_initializer(-0.1,0.1)\n",
    "        with tf.name_scope(\"Train\"):\n",
    "            train_input = get_train_data()\n",
    "            with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "                model = gru_lstm(X_inputs, y_labels, num_steps, batch_size, \n",
    "                                 is_training,vocab_size, embedding_size, \n",
    "                                 state_size, num_layers)\n",
    "            tf.summary.scalar(\"Training Loss\", m.cost)\n",
    "            tf.summary.scalar(\"Learning Rate\", m.lr)\n",
    "            \n",
    "        with tf.name_scope(\"Test\"):\n",
    "            test_input = get_test_data()\n",
    "        \n",
    "        \n",
    "        models = {\"Train\": model, \"Test\": model_test}\n",
    "        for name, model in models.items():\n",
    "            model.export_ops(name)\n",
    "        \n",
    "        metagraph = tf.train.export_meta_graph()\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        tf.train.import_meta_graph(metagraph)\n",
    "        for model in models.values():\n",
    "            model.import_ops()\n",
    "        sv = tf.train.Supervisor(logdir=FLAGS.save_path)\n",
    "        config_proto = tf.ConfigProto(allow_soft_placement=False)\n",
    "    \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "state_size = 1024\n",
    "batch_size = 128\n",
    "time_steps = 2\n",
    "num_features = 100\n",
    "keep_prob = 0.8 # Keep 80% if outputs after when implementing dropout\n",
    "\n",
    "# Based off of TensorFlow PTB example found at \n",
    "# https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py\n",
    "\n",
    "words_in_dataset = tf.placeholder(tf.float32, [time_steps, batch_size, num_features])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initial state of LSTM\n",
    "# initial_state = state = stacked_gru.zero_state(batch_size, tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
