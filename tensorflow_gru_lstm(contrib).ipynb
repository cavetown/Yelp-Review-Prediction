{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU LSTM\n",
    "Attempts to create a GRU LSTM model to use for Yelp Dataset Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "\n",
    "import utils\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gru_lstm class\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.flags\n",
    "flags.DEFINE_string(\"data_path\", None,\n",
    "                    \"Where the training/test data is stored.\")\n",
    "flags.DEFINE_string(\"save_path\", None,\n",
    "                    \"Model output directory.\")\n",
    "flags.DEFINE_integer(\"embedding_dim\", 300, \"Dimensionality of character embedding (default: 300)\")\n",
    "flags.DEFINE_float(\"dropout_keep_prob\", 1.0, \"Dropout keep probability (default: 1.0)\")\n",
    "flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularizaion lambda (default: 0.0)\")\n",
    "flags.DEFINE_string(\"training_files\", \"person_match.train2\", \"training file (default: None)\")  #for sentence semantic similarity use \"train_snli.txt\"\n",
    "flags.DEFINE_integer(\"hidden_units\", 50, \"Number of hidden units (default:50)\")\n",
    "\n",
    "# Training parameters\n",
    "flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "flags.DEFINE_integer(\"num_epochs\", 300, \"Number of training epochs (default: 200)\")\n",
    "flags.DEFINE_integer(\"evaluate_every\", 1000, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "flags.DEFINE_integer(\"checkpoint_every\", 1000, \"Save model after this many steps (default: 100)\")\n",
    "# Misc Parameters\n",
    "flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses tf.contrib to generate GRU cell\n",
    "# Saves implementing own GRUCell from scratch\n",
    "# Based off of PTB example: # https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py\n",
    "\n",
    "def data_type():\n",
    "    \"\"\"\n",
    "    Good for modying data type needed instead of having to change every single one\n",
    "    \"\"\"\n",
    "    return tf.float32\n",
    "\n",
    "class gru_lstm(object):\n",
    "    def __init__(self, X_inputs, y_labels, num_steps, batch_size, is_training, \n",
    "                 vocab_size, embedding_size, state_size, num_layers):\n",
    "        \n",
    "        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.state_size = hidden_units\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self._is_training = is_training\n",
    "\n",
    "        self._cell = None\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, size], dtype=data_type())\n",
    "        \n",
    "        inputs = tf.nn.embedding_lookup(embedding)\n",
    "        if is_training and keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "        \n",
    "        # Output is Neural Network\n",
    "        output, state = self.stacked_gru_graph(X_inputs, state_size, keep_prob, num_layers, is_training)\n",
    "        W = tf.get_variable(\"W\", [size, vocab_size], dtype=data_type())\n",
    "        b = tf.get_variable(\"b\", [vocab_size], dtype=data_type())\n",
    "        \n",
    "        \n",
    "        logits = tf.nn.xw_plus_b(output, W, b)\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "        \n",
    "        ###\n",
    "        # seq2seq.sequence_loss is weighted cross entropy over sequence of logits\n",
    "        ###\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logits,\n",
    "                                                y_labels,\n",
    "                                                tf.ones([self.batch_size, self.num_steps], dtype=data_type()),\n",
    "                                                average_across_timesteps=False,\n",
    "                                                average_across_batch=True)\n",
    "                                               \n",
    "        \n",
    "        self._cost = tf.reduce_sum(loss)\n",
    "        self._final_state = state\n",
    "        \n",
    "        self._learn_rate = tf.Variable(0.0, trainable=False) # Learning Rate\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars),\n",
    "                                          config.max_grad_norm)\n",
    "        train_vars = tf.trainable_variables() # Returns all variables specified trainable=True\n",
    "        \n",
    "        optimizer = tf.train.RMSPropOptimizer(self._learn_rate) # Using RMSProp for RNN's rather than SGD as usual\n",
    "        self._train_op = optimizer.apply_gradients(zip(gradients, train_vars),\n",
    "                                                   global_step=tf.train.get_or_create_global_step())\n",
    "        \n",
    "        self._new_learn_rate = tf.placeholder(tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "        self._learn_update = tf.assign(self._learn_rate, self._new_learn_rate)\n",
    "    \n",
    "    def gru_cell(self, hidden_state_size, keep_prob, is_training, layer, num_layers):\n",
    "    \n",
    "        assert isinstance(keep_prob, float)\n",
    "        assert isinstance(hidden_state_size, int)\n",
    "        assert isinstance(is_training, bool)\n",
    "        assert isinstance(layer, int)\n",
    "        assert isinstance(num_layers, int)\n",
    "        \n",
    "        if num_layers <= 0:\n",
    "            raise AssertionError(\"Need positive number of layers\")\n",
    "    \n",
    "        cell = tf.contrib.rnn.GRUCell(hidden_state_size)\n",
    "    \n",
    "        if layer == num_layers - 1:\n",
    "            return tf.contrib.rnn.OutputProjectionWrapper(cell,\n",
    "                                                          output_size=6)\n",
    "        elif is_training and keep_prob < 1:\n",
    "            return tf.contrib.rnn.DropoutWrapper(cell,\n",
    "                                                 output_keep_prob=keep_prob)\n",
    "        else:\n",
    "            return cell\n",
    "        \n",
    "    def stacked_gru_graph(self, X_inputs, state_size, keep_prob, num_layers, is_training):\n",
    "        \n",
    "        assert isinstance(state_size, int)\n",
    "        assert isinstance(keep_prob, float)\n",
    "        if keep_prob < 0 or keep_prob > 1:\n",
    "            raise AssertionError(\"needs to be a value between 0 and 1\")\n",
    "        assert isinstance(num_layers, int)\n",
    "        assert isinstance(is_training, bool)\n",
    "        \n",
    "        stacked_gru = tf.contrib.rnn.MultiRNNCell(\n",
    "            cells=[gru_cell(state_size, keep_prob, _, num_layers, is_training) for _ in range(num_layers)], \n",
    "            state_is_tupl=True)\n",
    "        \n",
    "        inputs = tf.unstack(X_inputs, num=num_steps, axis=1)\n",
    "        \n",
    "        self._initial_state = cell.zero_state(config.batch_size, data_type())\n",
    "        \n",
    "        outputs, state = tf.contrib.rnn.static_rnn(cell, inputs,\n",
    "                                                   initial_state=self._initial_state)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    def assign_lr(self, session, lr_value):\n",
    "        \"\"\"\n",
    "        :param: lr_value- new learning rate value multiplied by decay rate\n",
    "        \"\"\"\n",
    "        assert isinstance(lr_value, float)\n",
    "        session.run(self._learn_update, feed_dict={self._new_learn_rate: lr_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(session, model, eval_op=None, verbose=False):\n",
    "    \"\"\"Runs the model on the given data.\"\"\"\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "\n",
    "    fetches = {\n",
    "        \"cost\": model.cost,\n",
    "        \"final_state\": model.final_state,\n",
    "    }\n",
    "    if eval_op is not None:\n",
    "        fetches[\"eval_op\"] = eval_op\n",
    "\n",
    "    for step in range(model.input.epoch_size):\n",
    "        feed_dict = {}\n",
    "        for i, (c, h) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "\n",
    "    vals = session.run(fetches, feed_dict)\n",
    "    cost = vals[\"cost\"]\n",
    "    state = vals[\"final_state\"]\n",
    "\n",
    "    costs += cost\n",
    "    iters += model.input.num_steps\n",
    "\n",
    "    if verbose and step % (model.input.epoch_size // 10) == 10:\n",
    "        print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "            (step * 1.0 / model.input.epoch_size, np.exp(costs / iters), \n",
    "             iters * model.input.batch_size * max(1, FLAGS.num_gpus) /\n",
    "             (time.time() - start_time)))\n",
    "\n",
    "    return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    raw_data = reader.ptb_raw_data(FLAGS.data_path)\n",
    "    with tf.Graph().as_default():\n",
    "        initializer = tf.random_uniform_initializer(-0.1,0.1)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "state_size = 1024\n",
    "batch_size = 128\n",
    "time_steps = 2\n",
    "num_features = 100\n",
    "keep_prob = 0.8 # Keep 80% if outputs after when implementing dropout\n",
    "\n",
    "# Based off of TensorFlow PTB example found at \n",
    "# https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py\n",
    "\n",
    "words_in_dataset = tf.placeholder(tf.float32, [time_steps, batch_size, num_features])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initial state of LSTM\n",
    "# initial_state = state = stacked_gru.zero_state(batch_size, tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
