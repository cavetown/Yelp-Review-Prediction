{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU LSTM\n",
    "Attempts to create a GRU LSTM model to use for Yelp Dataset Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "\n",
    "import utils\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gru_lstm class\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses tf.contrib to generate GRU cell\n",
    "# Saves implementing own GRUCell from scratch\n",
    "# Based off of PTB example: # https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py\n",
    "\n",
    "def data_type():\n",
    "    \"\"\"\n",
    "    Good for modying data type needed instead of having to change every single one\n",
    "    \"\"\"\n",
    "    return tf.float32\n",
    "\n",
    "class gru_lstm(object):\n",
    "    def __init__(self, X_inputs, y_labels, num_steps, batch_size, is_training, \n",
    "                 vocab_size, embedding_size, state_size,, num_layers):\n",
    "        \n",
    "        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.state_size = hidden_units\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self._is_training = is_training\n",
    "\n",
    "        self._cell = None\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, size], dtype=data_type())\n",
    "        \n",
    "        inputs = tf.nn.embedding_lookup(embedding)\n",
    "        if is_training and keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "        \n",
    "        # Output is Neural Network\n",
    "        output, state = self.stacked_gru_graph(X_inputs, state_size, keep_prob, num_layers, is_training)\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [size, vocab_size], dtype=data_type())\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size], dtype=data_type())\n",
    "        \n",
    "        \n",
    "        logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "        \n",
    "        ###\n",
    "        # seq2seq.sequence_loss is weighted cross entropy over sequence of logits\n",
    "        ###\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logits,\n",
    "                                                y_labels,\n",
    "                                                tf.ones([self.batch_size, self.num_steps], dtype=data_type()),\n",
    "                                                average_across_timesteps=False,\n",
    "                                                average_across_batch=True)\n",
    "                                               \n",
    "        \n",
    "        self._cost = tf.reduce_sum(loss)\n",
    "        self._final_state = state\n",
    "        \n",
    "        self._learn_rate = tf.Variable(0.0, trainable=False) # Learning Rate\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars),\n",
    "                                          config.max_grad_norm)\n",
    "        train_vars = tf.trainable_variables() # Returns all variables specified trainable=True\n",
    "        \n",
    "        optimizer = tf.train.RMSPropOptimizer(self._learn_rate) # Using RMSProp for RNN's rather than SGD as usual\n",
    "        self._train_op = optimizer.apply_gradients(zip(gradients, train_vars),\n",
    "                                                   global_step=tf.train.get_or_create_global_step())\n",
    "        \n",
    "        self._new_learn_rate = tf.placeholder(tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "        self._learn_update = tf.assign(self._learn_rate, self._new_learn_rate)\n",
    "    \n",
    "    def gru_cell(self, hidden_state_size, keep_prob, is_training, layer, num_layers):\n",
    "    \n",
    "        assert isinstance(keep_prob, float)\n",
    "        assert isinstance(hidden_state_size, int)\n",
    "        assert isinstance(is_training, bool)\n",
    "        assert isinstance(layer, int)\n",
    "        assert isinstance(num_layers, int)\n",
    "        \n",
    "        if num_layers <= 0:\n",
    "            raise AssertionError(\"Need positive number of layers\")\n",
    "    \n",
    "        cell = tf.contrib.rnn.GRUCell(hidden_state_size)\n",
    "    \n",
    "        if layer == num_layers - 1:\n",
    "            return tf.contrib.rnn.OutputProjectionWrapper(cell,\n",
    "                                                          output_size=6)\n",
    "        elif is_training and keep_prob < 1:\n",
    "            return tf.contrib.rnn.DropoutWrapper(cell,\n",
    "                                                 output_keep_prob=keep_prob)\n",
    "        else:\n",
    "            return cell\n",
    "        \n",
    "    def stacked_gru_graph(self, X_inputs, state_size, keep_prob, num_layers, is_training):\n",
    "        \n",
    "        assert isinstance(state_size, int)\n",
    "        assert isinstance(keep_prob, float)\n",
    "        if keep_prob < 0 or keep_prob > 1:\n",
    "            raise AssertionError(\"needs to be a value between 0 and 1\")\n",
    "        assert isinstance(num_layers, int)\n",
    "        assert isinstance(is_training, bool)\n",
    "        \n",
    "        stacked_gru = tf.contrib.rnn.MultiRNNCell(\n",
    "            cells=[gru_cell(state_size, keep_prob, _, num_layers, is_training) for _ in range(num_layers)], \n",
    "            state_is_tupl=True)\n",
    "        \n",
    "        inputs = tf.unstack(X_inputs, num=num_steps, axis=1)\n",
    "        \n",
    "        self._initial_state = cell.zero_state(config.batch_size, data_type())\n",
    "        \n",
    "        outputs, state = tf.contrib.rnn.static_rnn(cell, inputs,\n",
    "                                                   initial_state=self._initial_state)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    def assign_lr(self, session, lr_value):\n",
    "        \"\"\"\n",
    "        :param: lr_value- new learning rate value multiplied by decay rate\n",
    "        \"\"\"\n",
    "        assert isinstance(lr_value, float)\n",
    "        session.run(self._learn_update, feed_dict={self._new_learn_rate: lr_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "state_size = 1024\n",
    "batch_size = 128\n",
    "time_steps = 2\n",
    "num_features = 100\n",
    "keep_prob = 0.8 # Keep 80% if outputs after when implementing dropout\n",
    "\n",
    "# Based off of TensorFlow PTB example found at \n",
    "# https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py\n",
    "\n",
    "words_in_dataset = tf.placeholder(tf.float32, [time_steps, batch_size, num_features])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initial state of LSTM\n",
    "# initial_state = state = stacked_gru.zero_state(batch_size, tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
