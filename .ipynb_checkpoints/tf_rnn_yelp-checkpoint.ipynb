{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Review Prediction\n",
    "Objective: Construct and train a Neural Network that would be able to predict the number of star ratings from a Yelp review.    \n",
    "Dataset used: https://www.yelp.com/dataset/challenge  \n",
    "\n",
    "Steps:  \n",
    "1) Data Preprocessing  \n",
    "2) Deep Learning Preprocessing  \n",
    "3) Network Training  \n",
    "4) Network Testing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "from langdetect import detect\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_ml import ConfusionMatrix\n",
    "import seaborn\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import nltk, re, time\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import namedtuple\n",
    "from contractions import get_contractions\n",
    "\n",
    "import operator\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "import os \n",
    "alreadyPickled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thoma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "num_layers = 2\n",
    "num_classes = 6\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "rnn_size = 64\n",
    "num_layers = 2\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "keep_probability = 0.8\n",
    "max_sequence_length = 750\n",
    "\n",
    "IS_TRAINING = True\n",
    "IS_TESTING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "If the data is already pickled, then can skip embedding and data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picklefiles(filename, stuff):\n",
    "    save_stuff = open(filename, \"wb\")\n",
    "    pickle.dump(stuff, save_stuff)\n",
    "    save_stuff.close()\n",
    "def loadfiles(filename):\n",
    "    saved_stuff = open(filename,\"rb\")\n",
    "    stuff = pickle.load(saved_stuff)\n",
    "    saved_stuff.close()\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "The following few cells preprocess the data for me. Clean test defines a function where it removes stop_words (found here https://gist.github.com/sebleier/554280). These words typically have no beneficial meaning to any reviews and are thus wasted features. I also strip punctuation and turn everything lower case. These procedures can be considered pretty standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()    \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 69047 restaurants\n"
     ]
    }
   ],
   "source": [
    "restId = []\n",
    "for line in open('./data/dataset/business.json', 'r'):\n",
    "    data = json.loads(line)\n",
    "    if 'Restaurants' in data['categories'] or 'Food' in data['categories']:\n",
    "        restId.append(data['business_id'])\n",
    "print(\"There are %d restaurants\" % (len(restId)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing the reviews and star ratings\n",
    "Here, I narrow down my reviews to english only reviews. I do this since restaurants make up around 60% of the yelp reviews. This way review types and wording may stay relatively similar for more accurate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 4999\n",
      "10000 9999\n",
      "15000 14999\n",
      "20000 19999\n",
      "25000 24999\n",
      "30000 29999\n",
      "35000 34999\n",
      "40000 39999\n",
      "45000 44999\n",
      "50000 49999\n",
      "55000 54999\n",
      "60000 59999\n",
      "65000 64999\n",
      "70000 69999\n",
      "75000 74999\n",
      "80000 79999\n",
      "85000 84999\n",
      "90000 89999\n",
      "95000 94999\n",
      "100000 99999\n",
      "105000 104999\n",
      "110000 109999\n",
      "115000 114999\n",
      "120000 119999\n",
      "125000 124999\n",
      "130000 129999\n",
      "135000 134999\n",
      "140000 139999\n",
      "145000 144999\n",
      "150000 149999\n",
      "155000 154999\n",
      "160000 159999\n",
      "165000 164999\n",
      "170000 169999\n",
      "175000 174999\n",
      "180000 179999\n",
      "185000 184999\n",
      "190000 189999\n",
      "195000 194999\n",
      "200000 199999\n",
      "205000 204999\n",
      "210000 209999\n",
      "215000 214999\n",
      "220000 219999\n",
      "225000 224999\n",
      "230000 229999\n",
      "235000 234999\n",
      "240000 239999\n",
      "245000 244999\n",
      "250000 249999\n",
      "255000 254999\n",
      "260000 259999\n",
      "265000 264999\n",
      "270000 269999\n",
      "275000 274999\n",
      "280000 279999\n",
      "285000 284999\n",
      "290000 289999\n",
      "295000 294999\n",
      "300000 299999\n",
      "305000 304999\n",
      "310000 309999\n",
      "315000 314999\n",
      "320000 319999\n",
      "325000 324999\n",
      "330000 329999\n",
      "335000 334999\n",
      "340000 339999\n",
      "345000 344999\n",
      "350000 349999\n",
      "355000 354999\n",
      "360000 359999\n",
      "365000 364999\n",
      "370000 369999\n",
      "375000 374999\n",
      "380000 379999\n",
      "385000 384999\n",
      "390000 389999\n",
      "395000 394999\n",
      "400000 399999\n",
      "405000 404999\n",
      "410000 409999\n",
      "415000 414999\n",
      "420000 419999\n",
      "425000 424999\n",
      "430000 429999\n",
      "435000 434999\n",
      "440000 439999\n",
      "445000 444999\n",
      "450000 449999\n",
      "455000 454999\n",
      "460000 459999\n",
      "465000 464999\n",
      "470000 469999\n",
      "475000 474999\n",
      "480000 479999\n",
      "485000 484999\n",
      "490000 489999\n",
      "495000 494999\n",
      "500000 499999\n",
      "505000 504999\n",
      "510000 509999\n",
      "515000 514999\n",
      "520000 519999\n",
      "525000 524999\n",
      "530000 529999\n",
      "535000 534999\n",
      "540000 539999\n",
      "545000 544999\n",
      "550000 549999\n",
      "555000 554999\n",
      "560000 559999\n",
      "565000 564999\n",
      "570000 569999\n",
      "575000 574999\n",
      "580000 579999\n",
      "585000 584999\n",
      "590000 589999\n",
      "595000 594999\n",
      "600000 599999\n",
      "605000 604999\n",
      "610000 609999\n",
      "615000 614999\n",
      "620000 619999\n",
      "625000 624999\n",
      "630000 629999\n",
      "635000 634999\n",
      "640000 639999\n",
      "645000 644999\n",
      "650000 649999\n",
      "655000 654999\n",
      "660000 659999\n",
      "665000 664999\n",
      "670000 669999\n",
      "675000 674999\n",
      "680000 679999\n",
      "685000 684999\n",
      "690000 689999\n",
      "695000 694999\n",
      "700000 699999\n",
      "705000 704999\n",
      "710000 709999\n",
      "715000 714999\n",
      "720000 719999\n",
      "725000 724999\n",
      "730000 729999\n",
      "735000 734999\n",
      "740000 739999\n",
      "745000 744999\n",
      "750000 749999\n",
      "755000 754999\n",
      "760000 759999\n",
      "765000 764999\n",
      "770000 769999\n",
      "775000 774999\n",
      "780000 779999\n",
      "785000 784999\n",
      "790000 789999\n",
      "795000 794999\n",
      "800000 799999\n",
      "805000 804999\n",
      "810000 809999\n",
      "815000 814999\n",
      "820000 819999\n",
      "825000 824999\n",
      "830000 829999\n",
      "835000 834999\n",
      "840000 839999\n",
      "845000 844999\n",
      "850000 849999\n",
      "855000 854999\n",
      "860000 859999\n",
      "865000 864999\n",
      "870000 869999\n",
      "875000 874999\n",
      "880000 879999\n",
      "885000 884999\n",
      "890000 889999\n",
      "895000 894999\n",
      "900000 899999\n",
      "905000 904999\n",
      "910000 909999\n",
      "915000 914999\n",
      "920000 919999\n",
      "925000 924999\n",
      "930000 929999\n",
      "935000 934999\n",
      "940000 939999\n",
      "945000 944999\n",
      "950000 949999\n",
      "955000 954999\n",
      "960000 959999\n",
      "965000 964999\n",
      "970000 969999\n",
      "975000 974999\n",
      "980000 979999\n",
      "985000 984999\n",
      "990000 989999\n",
      "995000 994999\n",
      "1000000 999999\n"
     ]
    }
   ],
   "source": [
    "contractions = get_contractions()\n",
    "\n",
    "revs_list = [[]]\n",
    "stars_list = [[]]\n",
    "num = 1000000 # Number of review read\n",
    "k = 0 # Count\n",
    "nolang = [[]]\n",
    "for line in open('./data/dataset/review.json', 'r', encoding='utf-8'):\n",
    "    if k >= num:\n",
    "        break\n",
    "    data = json.loads(line)\n",
    "    text = data['text']\n",
    "    star = data['stars']\n",
    "    ID = data['business_id']\n",
    "    # Check language\n",
    "    if text == None:\n",
    "        continue\n",
    "    if star == None:\n",
    "        continue\n",
    "    if ID not in restId:\n",
    "        continue\n",
    "    try:\n",
    "        if detect(text) == 'en':\n",
    "            revs_list.append(clean_text(text))\n",
    "            stars_list.append(star)\n",
    "            k += 1\n",
    "            # Notify for every 5000 reviews\n",
    "            if len(revs_list) % 5000 == 0:\n",
    "                print(len(revs_list), k)\n",
    "    except:\n",
    "        nolang.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love staff love meat love place prepare long line around lunch dinner hours ask want meat lean something maybe cannot remember say want fatty get half sour pickle hot pepper hand cut french fries\n",
      "1000001 1000001\n"
     ]
    }
   ],
   "source": [
    "print(revs_list[1])\n",
    "print(len(revs_list), len(stars_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000001, 1) (1000001, 1) (1000001, 2)\n",
      "(1000001, 2)\n",
      "                                                text stars\n",
      "0                                                 []    []\n",
      "1  love staff love meat love place prepare long l...     5\n",
      "2  super simple place amazing nonetheless around ...     5\n",
      "3  small unassuming place changes menu every ofte...     5\n",
      "4  lester located beautiful neighborhood since 19...     5\n"
     ]
    }
   ],
   "source": [
    "np_revs = np.asarray([revs_list]).T\n",
    "np_stars = np.asarray([stars_list]).T\n",
    "stacked_revs = np.hstack((np_revs, np_stars))\n",
    "categories = ['text', 'stars']\n",
    "print(np_revs.shape, np_stars.shape, stacked_revs.shape)\n",
    "df_reviews_processing = pd.DataFrame(stacked_revs, columns=categories)\n",
    "print(df_reviews_processing.shape)\n",
    "print(df_reviews_processing.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Dropping Nones: Shape 1000001,2\n",
      "After Dropping Nones: Shape 1000000,2\n"
     ]
    }
   ],
   "source": [
    "df_reviews_processing[['stars']] = df_reviews_processing[['stars']].apply(pd.to_numeric)\n",
    "\n",
    "# Grabbing only numbers that are of numerical value (get rid of None, NaN, etc)\n",
    "print(\"Before Dropping Nones: Shape %d,%d\" % (df_reviews_processing.shape[0], df_reviews_processing.shape[1]))\n",
    "\n",
    "df_reviews_processing = df_reviews_processing[np.isfinite(df_reviews_processing['stars'])]\n",
    "df_reviews = df_reviews_processing[np.isfinite(df_reviews_processing['stars'])]\n",
    "df_reviews = df_reviews.dropna()\n",
    "df_reviews = df_reviews.reset_index(drop=True)\n",
    "\n",
    "print(\"After Dropping Nones: Shape %d,%d\" % (df_reviews.shape[0], df_reviews.shape[1]))\n",
    "\n",
    "df_reviews.to_csv(\"./csvs/reviews_df_processed.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance the Dataset\n",
    "Want to balance the dataset, such that we have an equal number of reviews for each different category.  \n",
    "For example, if our distribution of reviews is [200,500,100,300,400], for [1,2,3,4,5] stars, respectively, then I will only take 100 of each review  \n",
    "I do this so we have an equal representation of all labels when he train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataframe(df, category=['stars']):\n",
    "    \"\"\"\n",
    "    :param df: pandas.DataFrame\n",
    "    :param categorical_columns: iterable of categorical columns names contained in {df}\n",
    "    :return: balanced pandas.DataFrame\n",
    "    \"\"\"    \n",
    "    if category is None or not all([col in df.columns for col in category]):\n",
    "        raise ValueError('Please provide one or more columns containing categorical variables')\n",
    "\n",
    "    lowest_count = df.groupby(category).apply(lambda x: x.shape[0]).min()\n",
    "    df = df.groupby(category).apply( \n",
    "        lambda x: x.sample(lowest_count)).drop(category, axis=1).reset_index().set_index('level_1')\n",
    "\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pre) 1 star ratings: 114331\n",
      "(pre) 2 star ratings: 94828\n",
      "(pre) 3 star ratings: 134510\n",
      "(pre) 4 star ratings: 269225\n",
      "(pre) 5 star ratings: 387071\n",
      "(post) 1 star ratings: 94828\n",
      "(post) 2 star ratings: 94828\n",
      "(post) 3 star ratings: 94828\n",
      "(post) 4 star ratings: 94828\n",
      "(post) 5 star ratings: 94828\n"
     ]
    }
   ],
   "source": [
    "df_reviews = pd.read_csv(\"./csvs/reviews_df_processed.csv\")\n",
    "df_reviews['len'] = df_reviews.text.str.len()\n",
    "\n",
    "df_reviews = df_reviews[df_reviews['len'].between(10, 4000)]\n",
    "df_reviews[['stars']] = df_reviews[['stars']].apply(pd.to_numeric)\n",
    "\n",
    "print(\"(pre) 1 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 1])))\n",
    "print(\"(pre) 2 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 2])))\n",
    "print(\"(pre) 3 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 3])))\n",
    "print(\"(pre) 4 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 4])))\n",
    "print(\"(pre) 5 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 5])))\n",
    "\n",
    "df_balanced = balance_dataframe(df_reviews, \n",
    "                                category=['stars'])\n",
    "\n",
    "df_balanced.to_csv('balanced_reviews1000.csv', encoding='utf-8')\n",
    "print(\"(post) 1 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 1])))\n",
    "print(\"(post) 2 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 2])))\n",
    "print(\"(post) 3 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 3])))\n",
    "print(\"(post) 4 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 4])))\n",
    "print(\"(post) 5 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 5])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         stars  Unnamed: 0                                               text  \\\n",
      "level_1                                                                         \n",
      "4          4.0           4  love coming yes place always needs floor swept...   \n",
      "6          4.0           6  would guess would able get fairly decent vietn...   \n",
      "8          3.0           8  bad love gluten free vegan version cheese curd...   \n",
      "9          4.0           9  currently parents new favourite restaurant com...   \n",
      "10         3.0          10  server little rude ordered calamari duck confi...   \n",
      "\n",
      "           len  \n",
      "level_1         \n",
      "4        314.0  \n",
      "6        376.0  \n",
      "8        153.0  \n",
      "9        266.0  \n",
      "10       107.0  \n",
      "566.0\n",
      "660.0\n",
      "721.0\n",
      "1045.0\n"
     ]
    }
   ],
   "source": [
    "print(df_balanced.head())\n",
    "print(np.percentile(df_balanced.len, 80))\n",
    "print(np.percentile(df_balanced.len, 85))\n",
    "print(np.percentile(df_balanced.len, 87.5))\n",
    "print(np.percentile(df_balanced.len, 95))\n",
    "max_sequence_length = 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "Using ConceptNet Numberbatch instead of GLoVE (supposedly outperforms GLoVE embeddings)  \n",
    "https://github.com/commonsense/conceptnet-numberbatch\n",
    "  \n",
    "On top of the embeddings, we also keep track of commonly used words in the reviews that Embeddings don't cover. This way we could have higher test accuracy when words we come across words like these. This is specified by a threshold value. Currently, threshold is set to 20 occuraces.  \n",
    "  \n",
    "  \n",
    "We also process the reviews a bit more, sorting them into comparable lengths. This way, there is less padding necessary and (possibly) faster computation time when training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 143090\n"
     ]
    }
   ],
   "source": [
    "word_counts = {}\n",
    "count_words(word_counts, df_balanced.text)            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_path='./embeddings/numberbatch-en.txt'\n",
    "def load_embeddings(path='./embeddings/numberbatch-en.txt'):\n",
    "    embeddings_index = {}\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            embedding = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = embedding\n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = load_embeddings(embed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "Here we will create a dictionary __word2int__ that will map each word to respective value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 143090\n",
      "Number of words we will use: 66738\n",
      "Percent of words we will use: 46.64%\n"
     ]
    }
   ],
   "source": [
    "#dictionary to tokenizer words\n",
    "word2int = {} \n",
    "threshold = 20\n",
    "token_index = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        word2int[word] = token_index\n",
    "        token_index += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "special_characters = [\"<unk>\",\"<pad>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for c in special_characters:\n",
    "    word2int[c] = len(word2int)\n",
    "    \n",
    "usage_ratio = round(len(word2int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(word2int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66738 66738\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(word2int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in word2int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in embeddings, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(word2int). \n",
    "print(len(word_embedding_matrix), len(word2int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, pred=False):\n",
    "    if pred:\n",
    "        seq = []\n",
    "        for word in text.split():\n",
    "            if word in word2int:\n",
    "                seq.append(word2int[word])\n",
    "            else:\n",
    "                seq.append(word2int[\"<unk>\"])\n",
    "        return seq\n",
    "    else:\n",
    "        seq = []\n",
    "        for s in text:\n",
    "            temp_seq = []\n",
    "            for word in s.split():\n",
    "                if word in word2int:\n",
    "                    temp_seq.append(word2int[word])\n",
    "                else:\n",
    "                    temp_seq.append(word2int[\"<unk>\"])\n",
    "            seq.append(temp_seq)\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = convert_to_ints(df_balanced['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the ratings into One-Hot representation\n",
    "ratings = df_balanced.stars.values.astype(int)\n",
    "ratings_cat = tf.keras.utils.to_categorical(ratings)\n",
    "X_train, X_test, y_train, y_test = train_test_split(seq, ratings_cat, test_size=0.2, random_state=9)\n",
    "with pd.HDFStore('x_y_test_train.h5') as h:\n",
    "    h['X_train'] = pd.DataFrame(X_train)\n",
    "    h['X_test'] = pd.DataFrame(X_test)\n",
    "    h['y_train'] = pd.DataFrame(y_train)\n",
    "    h['y_test'] = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Batches\n",
    "Gets batches. These will be called later to then fill our X and y placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(batch):\n",
    "    # Want to pad this way since tensorflow preprocessing pads with 0's, which can eventually lead to zero tensors\n",
    "    lengths = []\n",
    "    for text in batch:\n",
    "        lengths.append(len(text))\n",
    "    max_length = max(lengths)\n",
    "    pad_text = tf.keras.preprocessing.sequence.pad_sequences(batch, \n",
    "                                                             maxlen=max_length, \n",
    "                                                             padding='post', \n",
    "                                                             value=word2int['<pad>'])\n",
    "    return pad_text\n",
    "\n",
    "def get_batches(x, y, batch_size):\n",
    "    # Make sure to not exceed amount of data\n",
    "    for batch_i in range(0, len(x)//batch_size):\n",
    "        start = batch_i * batch_size\n",
    "        end = start+batch_size\n",
    "        batch_x = x[start:end]\n",
    "        labels = y[start:end]\n",
    "        pad_batch_x = np.array(pad_batch(batch_x))\n",
    "        yield pad_batch_x, labels\n",
    "        \n",
    "def get_test_batches(x, batch_size):\n",
    "    for batch_i in range(0, len(x)//batch_size):\n",
    "        start = batch_i * batch_size\n",
    "        end = start+batch_size\n",
    "        batch = x[start:end]\n",
    "        pad_batch_test = np.array(pad_batch(batch))\n",
    "        yield pad_batch_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "if alreadyPickled == False:\n",
    "    \n",
    "    picklefiles(\"./data/pickles/balanced_reviews.p\",df_balanced)\n",
    "    picklefiles(\"./data/pickles/category_ratings.p\",ratings_cat)\n",
    "    picklefiles(\"./data/pickles/word_embedding_matrix.p\",word_embedding_matrix)\n",
    "    picklefiles(\"./data/pickles/word2int.p\", word2int)\n",
    "    \n",
    "if alreadyPickled == True:\n",
    "    \n",
    "    word_embedding_matrix = loadfiles(\"./data/pickles/word_embedding_matrix.p\")\n",
    "    ratings_cat = loadfiles(\"./data/pickles/category_ratings.p\")\n",
    "    df_balanced = pd.read_csv('balanced_reviews.csv')\n",
    "    balanced_reviews = loadfiles(\"./data/pickles/balanced_reviews.p\")\n",
    "    word2int = loadfiles('./data/pickles/word2int.p')\n",
    "    with pd.HDFStore('x_y_test_train.h5') as h:\n",
    "        X_train = h['X_train'].values\n",
    "        X_test = h['X_test'].values\n",
    "        y_train = h['y_train'].values\n",
    "        y_test = h['y_test'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Graph\n",
    "Here we start building our computational graph. We use a 2 layer GRU Recurrent Neural Network. We define placeholders for learning rate and dropout since these are variables that we could potentially want to vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    # Should be [batch_size x review length]\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    # Should be [batch_size x num_classes]\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    return input_data, labels, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is built.\n",
      "./graph\n"
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        input_data, labels, lr, keep_prob = model_inputs()\n",
    "        weight = tf.Variable(tf.truncated_normal([rnn_size, num_classes], stddev=(1/np.sqrt(rnn_size*num_classes))))\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "\n",
    "    embeddings = word_embedding_matrix\n",
    "    embs = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "\n",
    "    with tf.name_scope(\"RNN_Layers\"):\n",
    "        \n",
    "        stacked_rnn = []\n",
    "        for layer in range(num_layers):\n",
    "            cell_fw = tf.contrib.rnn.GRUCell(rnn_size)\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw,\n",
    "                                                    output_keep_prob=keep_prob)\n",
    "            stacked_rnn.append(cell_fw)\n",
    "        multilayer_cell = tf.contrib.rnn.MultiRNNCell(stacked_rnn, state_is_tuple=True)\n",
    "\n",
    "        \n",
    "    with tf.name_scope(\"init_state\"):\n",
    "        initial_state = multilayer_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    with tf.name_scope(\"Forward_Pass\"):\n",
    "        output, final_state = tf.nn.dynamic_rnn(multilayer_cell,\n",
    "                                           embs,\n",
    "                                           dtype=tf.float32)\n",
    "\n",
    "    with tf.name_scope(\"Predictions\"):\n",
    "        last = output[:, -1, :]\n",
    "        predictions = tf.exp(tf.matmul(last, weight) + bias)\n",
    "        tf.summary.histogram('predictions', predictions)\n",
    "        \n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predictions, labels=labels))        \n",
    "        tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    # Optimizer \n",
    "    with tf.name_scope('train'):    \n",
    "        optimizer = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "    \n",
    "    # Predictions comes out as 6 output layer, so need to \"change\" to one hot\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correctPred = tf.equal(tf.argmax(predictions,1), tf.argmax(labels,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    export_nodes = ['input_data', 'labels', 'keep_prob', 'lr', 'initial_state', 'final_state',\n",
    "                    'accuracy', 'predictions', 'cost', 'optimizer', 'merged']\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "print(\"Graph is built.\")\n",
    "graph_location = \"./graph\"\n",
    "\n",
    "Graph = namedtuple('train_graph', export_nodes)\n",
    "local_dict = locals()\n",
    "graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "print(graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(train_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 8 # Make 8 update checks per epoch\n",
    "update_check = (len(seq)//batch_size//per_epoch)-1\n",
    "keep_probability = 0.75\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X, y):\n",
    "    whole_data = np.insert(X, 0, y, axis=1)\n",
    "    np.random.shuffle(whole_data)\n",
    "    labels = whole_data[0,:]\n",
    "    data = whole_data[1,:]\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "I've been keeping track of the tensorboard summaries so it'll allow me to visualize the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saves/best_model.ckpt\n",
      "Starting\n",
      "Finished first\n",
      "Epoch   1/5 Batch   20/5926 - Loss:  1.003, Acc:  0.641, Seconds: 11.68\n",
      "Epoch   1/5 Batch   40/5926 - Loss:  0.953, Acc:  0.484, Seconds: 10.30\n",
      "Epoch   1/5 Batch   60/5926 - Loss:  0.958, Acc:  0.516, Seconds: 10.22\n",
      "Epoch   1/5 Batch   80/5926 - Loss:  0.938, Acc:  0.594, Seconds: 17.57\n",
      "Epoch   1/5 Batch  100/5926 - Loss:  0.920, Acc:  0.641, Seconds: 14.56\n",
      "Epoch   1/5 Batch  120/5926 - Loss:  0.909, Acc:  0.625, Seconds: 25.55\n",
      "Epoch   1/5 Batch  140/5926 - Loss:  0.951, Acc:  0.609, Seconds: 20.59\n",
      "Epoch   1/5 Batch  160/5926 - Loss:  0.950, Acc:  0.562, Seconds: 22.66\n",
      "Epoch   1/5 Batch  180/5926 - Loss:  0.922, Acc:  0.594, Seconds: 51.26\n",
      "Epoch   1/5 Batch  200/5926 - Loss:  0.893, Acc:  0.625, Seconds: 31.70\n",
      "Epoch   1/5 Batch  220/5926 - Loss:  0.954, Acc:  0.562, Seconds: 17.27\n",
      "Epoch   1/5 Batch  240/5926 - Loss:  0.926, Acc:  0.531, Seconds: 23.84\n",
      "Epoch   1/5 Batch  260/5926 - Loss:  0.969, Acc:  0.547, Seconds: 54.81\n",
      "Epoch   1/5 Batch  280/5926 - Loss:  0.910, Acc:  0.609, Seconds: 24.38\n",
      "Epoch   1/5 Batch  300/5926 - Loss:  0.907, Acc:  0.562, Seconds: 54.42\n",
      "Epoch   1/5 Batch  320/5926 - Loss:  0.939, Acc:  0.562, Seconds: 45.30\n",
      "Epoch   1/5 Batch  340/5926 - Loss:  0.944, Acc:  0.609, Seconds: 34.69\n",
      "Epoch   1/5 Batch  360/5926 - Loss:  0.954, Acc:  0.656, Seconds: 33.13\n",
      "Epoch   1/5 Batch  380/5926 - Loss:  0.937, Acc:  0.703, Seconds: 51.65\n",
      "Epoch   1/5 Batch  400/5926 - Loss:  0.912, Acc:  0.594, Seconds: 39.38\n",
      "Epoch   1/5 Batch  420/5926 - Loss:  0.919, Acc:  0.578, Seconds: 9.43\n",
      "Epoch   1/5 Batch  440/5926 - Loss:  0.942, Acc:  0.562, Seconds: 10.88\n",
      "Epoch   1/5 Batch  460/5926 - Loss:  0.959, Acc:  0.500, Seconds: 7.67\n",
      "Epoch   1/5 Batch  480/5926 - Loss:  0.940, Acc:  0.656, Seconds: 9.67\n",
      "Epoch   1/5 Batch  500/5926 - Loss:  0.921, Acc:  0.547, Seconds: 19.38\n",
      "Epoch   1/5 Batch  520/5926 - Loss:  0.928, Acc:  0.453, Seconds: 16.31\n",
      "Epoch   1/5 Batch  540/5926 - Loss:  0.946, Acc:  0.578, Seconds: 9.39\n",
      "Epoch   1/5 Batch  560/5926 - Loss:  0.894, Acc:  0.719, Seconds: 18.29\n",
      "Epoch   1/5 Batch  580/5926 - Loss:  0.925, Acc:  0.672, Seconds: 13.35\n",
      "Epoch   1/5 Batch  600/5926 - Loss:  0.919, Acc:  0.516, Seconds: 12.50\n",
      "Epoch   1/5 Batch  620/5926 - Loss:  0.935, Acc:  0.656, Seconds: 10.20\n",
      "Epoch   1/5 Batch  640/5926 - Loss:  0.911, Acc:  0.641, Seconds: 7.73\n",
      "Epoch   1/5 Batch  660/5926 - Loss:  0.927, Acc:  0.594, Seconds: 12.86\n",
      "Epoch   1/5 Batch  680/5926 - Loss:  0.923, Acc:  0.516, Seconds: 12.18\n",
      "Epoch   1/5 Batch  700/5926 - Loss:  0.924, Acc:  0.641, Seconds: 13.40\n",
      "Epoch   1/5 Batch  720/5926 - Loss:  0.883, Acc:  0.672, Seconds: 8.93\n",
      "Epoch   1/5 Batch  740/5926 - Loss:  0.931, Acc:  0.609, Seconds: 14.60\n",
      "Epoch   1/5 Batch  760/5926 - Loss:  0.907, Acc:  0.656, Seconds: 23.42\n",
      "Epoch   1/5 Batch  780/5926 - Loss:  0.892, Acc:  0.672, Seconds: 12.98\n",
      "Epoch   1/5 Batch  800/5926 - Loss:  0.974, Acc:  0.578, Seconds: 10.26\n",
      "Epoch   1/5 Batch  820/5926 - Loss:  0.937, Acc:  0.531, Seconds: 15.13\n",
      "Epoch   1/5 Batch  840/5926 - Loss:  0.955, Acc:  0.484, Seconds: 6.93\n",
      "Epoch   1/5 Batch  860/5926 - Loss:  0.974, Acc:  0.594, Seconds: 10.82\n",
      "Epoch   1/5 Batch  880/5926 - Loss:  0.962, Acc:  0.531, Seconds: 17.59\n",
      "Epoch   1/5 Batch  900/5926 - Loss:  0.943, Acc:  0.625, Seconds: 11.99\n",
      "Epoch   1/5 Batch  920/5926 - Loss:  0.901, Acc:  0.578, Seconds: 19.92\n",
      "Average loss for this update: 0.933\n",
      "New Record!\n",
      "Epoch   1/5 Batch  940/5926 - Loss:  0.887, Acc:  0.625, Seconds: 18.99\n",
      "Epoch   1/5 Batch  960/5926 - Loss:  0.926, Acc:  0.453, Seconds: 15.77\n",
      "Epoch   1/5 Batch  980/5926 - Loss:  0.917, Acc:  0.641, Seconds: 11.99\n",
      "Epoch   1/5 Batch 1000/5926 - Loss:  0.917, Acc:  0.547, Seconds: 13.35\n",
      "Epoch   1/5 Batch 1020/5926 - Loss:  0.975, Acc:  0.531, Seconds: 7.48\n",
      "Epoch   1/5 Batch 1040/5926 - Loss:  0.898, Acc:  0.656, Seconds: 11.27\n",
      "Epoch   1/5 Batch 1060/5926 - Loss:  0.907, Acc:  0.609, Seconds: 15.75\n",
      "Epoch   1/5 Batch 1080/5926 - Loss:  0.909, Acc:  0.594, Seconds: 10.68\n",
      "Epoch   1/5 Batch 1100/5926 - Loss:  0.907, Acc:  0.656, Seconds: 12.77\n",
      "Epoch   1/5 Batch 1120/5926 - Loss:  0.947, Acc:  0.641, Seconds: 17.40\n",
      "Epoch   1/5 Batch 1140/5926 - Loss:  0.965, Acc:  0.562, Seconds: 14.91\n",
      "Epoch   1/5 Batch 1160/5926 - Loss:  0.924, Acc:  0.641, Seconds: 11.31\n",
      "Epoch   1/5 Batch 1180/5926 - Loss:  0.959, Acc:  0.625, Seconds: 11.51\n",
      "Epoch   1/5 Batch 1200/5926 - Loss:  0.933, Acc:  0.625, Seconds: 12.17\n",
      "Epoch   1/5 Batch 1220/5926 - Loss:  0.914, Acc:  0.516, Seconds: 19.15\n",
      "Epoch   1/5 Batch 1240/5926 - Loss:  0.928, Acc:  0.516, Seconds: 7.71\n",
      "Epoch   1/5 Batch 1260/5926 - Loss:  0.965, Acc:  0.609, Seconds: 17.72\n",
      "Epoch   1/5 Batch 1280/5926 - Loss:  0.922, Acc:  0.656, Seconds: 12.61\n",
      "Epoch   1/5 Batch 1300/5926 - Loss:  0.902, Acc:  0.484, Seconds: 9.10\n",
      "Epoch   1/5 Batch 1320/5926 - Loss:  0.929, Acc:  0.641, Seconds: 8.65\n",
      "Epoch   1/5 Batch 1340/5926 - Loss:  0.918, Acc:  0.641, Seconds: 14.02\n",
      "Epoch   1/5 Batch 1360/5926 - Loss:  0.895, Acc:  0.656, Seconds: 15.52\n",
      "Epoch   1/5 Batch 1380/5926 - Loss:  0.884, Acc:  0.625, Seconds: 11.60\n",
      "Epoch   1/5 Batch 1400/5926 - Loss:  0.923, Acc:  0.594, Seconds: 10.37\n",
      "Epoch   1/5 Batch 1420/5926 - Loss:  0.927, Acc:  0.562, Seconds: 9.26\n",
      "Epoch   1/5 Batch 1440/5926 - Loss:  0.904, Acc:  0.625, Seconds: 9.61\n",
      "Epoch   1/5 Batch 1460/5926 - Loss:  0.939, Acc:  0.625, Seconds: 15.82\n",
      "Epoch   1/5 Batch 1480/5926 - Loss:  0.945, Acc:  0.594, Seconds: 12.16\n",
      "Epoch   1/5 Batch 1500/5926 - Loss:  0.951, Acc:  0.547, Seconds: 16.11\n",
      "Epoch   1/5 Batch 1520/5926 - Loss:  0.931, Acc:  0.547, Seconds: 14.11\n",
      "Epoch   1/5 Batch 1540/5926 - Loss:  0.881, Acc:  0.609, Seconds: 9.15\n",
      "Epoch   1/5 Batch 1560/5926 - Loss:  0.885, Acc:  0.578, Seconds: 9.93\n",
      "Epoch   1/5 Batch 1580/5926 - Loss:  0.891, Acc:  0.609, Seconds: 19.74\n",
      "Epoch   1/5 Batch 1600/5926 - Loss:  0.874, Acc:  0.594, Seconds: 8.90\n",
      "Epoch   1/5 Batch 1620/5926 - Loss:  0.933, Acc:  0.594, Seconds: 18.84\n",
      "Epoch   1/5 Batch 1640/5926 - Loss:  0.881, Acc:  0.656, Seconds: 10.49\n",
      "Epoch   1/5 Batch 1660/5926 - Loss:  0.946, Acc:  0.578, Seconds: 20.75\n",
      "Epoch   1/5 Batch 1680/5926 - Loss:  0.897, Acc:  0.641, Seconds: 11.87\n",
      "Epoch   1/5 Batch 1700/5926 - Loss:  0.886, Acc:  0.625, Seconds: 11.29\n",
      "Epoch   1/5 Batch 1720/5926 - Loss:  0.940, Acc:  0.578, Seconds: 9.56\n",
      "Epoch   1/5 Batch 1740/5926 - Loss:  0.904, Acc:  0.578, Seconds: 14.73\n",
      "Epoch   1/5 Batch 1760/5926 - Loss:  0.908, Acc:  0.531, Seconds: 21.24\n",
      "Epoch   1/5 Batch 1780/5926 - Loss:  0.937, Acc:  0.594, Seconds: 8.03\n",
      "Epoch   1/5 Batch 1800/5926 - Loss:  0.886, Acc:  0.641, Seconds: 18.89\n",
      "Epoch   1/5 Batch 1820/5926 - Loss:  0.908, Acc:  0.672, Seconds: 10.44\n",
      "Epoch   1/5 Batch 1840/5926 - Loss:  0.966, Acc:  0.547, Seconds: 20.59\n",
      "Average loss for this update: 0.919\n",
      "New Record!\n",
      "Epoch   1/5 Batch 1860/5926 - Loss:  0.946, Acc:  0.594, Seconds: 13.08\n",
      "Epoch   1/5 Batch 1880/5926 - Loss:  0.911, Acc:  0.641, Seconds: 15.59\n",
      "Epoch   1/5 Batch 1900/5926 - Loss:  0.904, Acc:  0.656, Seconds: 9.94\n",
      "Epoch   1/5 Batch 1920/5926 - Loss:  0.905, Acc:  0.703, Seconds: 15.38\n",
      "Epoch   1/5 Batch 1940/5926 - Loss:  0.936, Acc:  0.609, Seconds: 19.32\n",
      "Epoch   1/5 Batch 1960/5926 - Loss:  0.912, Acc:  0.594, Seconds: 22.45\n",
      "Epoch   1/5 Batch 1980/5926 - Loss:  0.919, Acc:  0.609, Seconds: 16.74\n",
      "Epoch   1/5 Batch 2000/5926 - Loss:  0.924, Acc:  0.500, Seconds: 11.21\n",
      "Epoch   1/5 Batch 2020/5926 - Loss:  0.970, Acc:  0.531, Seconds: 11.56\n",
      "Epoch   1/5 Batch 2040/5926 - Loss:  0.938, Acc:  0.641, Seconds: 12.33\n",
      "Epoch   1/5 Batch 2060/5926 - Loss:  0.948, Acc:  0.641, Seconds: 8.95\n",
      "Epoch   1/5 Batch 2080/5926 - Loss:  0.924, Acc:  0.594, Seconds: 8.12\n",
      "Epoch   1/5 Batch 2100/5926 - Loss:  0.919, Acc:  0.703, Seconds: 17.66\n",
      "Epoch   1/5 Batch 2120/5926 - Loss:  0.907, Acc:  0.641, Seconds: 13.96\n",
      "Epoch   1/5 Batch 2140/5926 - Loss:  0.874, Acc:  0.516, Seconds: 10.27\n",
      "Epoch   1/5 Batch 2160/5926 - Loss:  0.969, Acc:  0.516, Seconds: 15.96\n",
      "Epoch   1/5 Batch 2180/5926 - Loss:  0.870, Acc:  0.672, Seconds: 6.83\n",
      "Epoch   1/5 Batch 2200/5926 - Loss:  0.923, Acc:  0.531, Seconds: 23.18\n",
      "Epoch   1/5 Batch 2220/5926 - Loss:  0.940, Acc:  0.656, Seconds: 12.94\n",
      "Epoch   1/5 Batch 2240/5926 - Loss:  0.915, Acc:  0.578, Seconds: 8.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/5 Batch 2260/5926 - Loss:  0.943, Acc:  0.594, Seconds: 11.36\n",
      "Epoch   1/5 Batch 2280/5926 - Loss:  0.883, Acc:  0.578, Seconds: 22.80\n",
      "Epoch   1/5 Batch 2300/5926 - Loss:  0.906, Acc:  0.469, Seconds: 23.86\n",
      "Epoch   1/5 Batch 2320/5926 - Loss:  0.939, Acc:  0.547, Seconds: 18.48\n",
      "Epoch   1/5 Batch 2340/5926 - Loss:  0.869, Acc:  0.641, Seconds: 11.63\n",
      "Epoch   1/5 Batch 2360/5926 - Loss:  0.929, Acc:  0.625, Seconds: 9.57\n",
      "Epoch   1/5 Batch 2380/5926 - Loss:  0.898, Acc:  0.641, Seconds: 15.35\n",
      "Epoch   1/5 Batch 2400/5926 - Loss:  0.890, Acc:  0.578, Seconds: 16.31\n",
      "Epoch   1/5 Batch 2420/5926 - Loss:  0.906, Acc:  0.625, Seconds: 16.39\n",
      "Epoch   1/5 Batch 2440/5926 - Loss:  0.871, Acc:  0.672, Seconds: 13.20\n",
      "Epoch   1/5 Batch 2460/5926 - Loss:  0.935, Acc:  0.578, Seconds: 10.06\n",
      "Epoch   1/5 Batch 2480/5926 - Loss:  0.940, Acc:  0.656, Seconds: 13.08\n",
      "Epoch   1/5 Batch 2500/5926 - Loss:  0.905, Acc:  0.578, Seconds: 11.80\n",
      "Epoch   1/5 Batch 2520/5926 - Loss:  0.914, Acc:  0.594, Seconds: 10.23\n",
      "Epoch   1/5 Batch 2540/5926 - Loss:  0.910, Acc:  0.625, Seconds: 10.50\n",
      "Epoch   1/5 Batch 2560/5926 - Loss:  0.870, Acc:  0.547, Seconds: 13.86\n",
      "Epoch   1/5 Batch 2580/5926 - Loss:  0.918, Acc:  0.562, Seconds: 12.08\n",
      "Epoch   1/5 Batch 2600/5926 - Loss:  0.919, Acc:  0.531, Seconds: 13.49\n",
      "Epoch   1/5 Batch 2620/5926 - Loss:  0.868, Acc:  0.609, Seconds: 18.73\n",
      "Epoch   1/5 Batch 2640/5926 - Loss:  0.904, Acc:  0.641, Seconds: 7.45\n",
      "Epoch   1/5 Batch 2660/5926 - Loss:  0.890, Acc:  0.641, Seconds: 17.98\n",
      "Epoch   1/5 Batch 2680/5926 - Loss:  0.893, Acc:  0.641, Seconds: 14.23\n",
      "Epoch   1/5 Batch 2700/5926 - Loss:  0.889, Acc:  0.578, Seconds: 16.55\n",
      "Epoch   1/5 Batch 2720/5926 - Loss:  0.905, Acc:  0.625, Seconds: 14.48\n",
      "Epoch   1/5 Batch 2740/5926 - Loss:  0.935, Acc:  0.656, Seconds: 11.13\n",
      "Epoch   1/5 Batch 2760/5926 - Loss:  0.896, Acc:  0.656, Seconds: 10.68\n",
      "Average loss for this update: 0.913\n",
      "New Record!\n",
      "Epoch   1/5 Batch 2780/5926 - Loss:  0.934, Acc:  0.562, Seconds: 9.11\n",
      "Epoch   1/5 Batch 2800/5926 - Loss:  0.958, Acc:  0.562, Seconds: 7.58\n",
      "Epoch   1/5 Batch 2820/5926 - Loss:  0.914, Acc:  0.562, Seconds: 22.80\n",
      "Epoch   1/5 Batch 2840/5926 - Loss:  0.876, Acc:  0.625, Seconds: 18.37\n",
      "Epoch   1/5 Batch 2860/5926 - Loss:  0.897, Acc:  0.688, Seconds: 20.71\n",
      "Epoch   1/5 Batch 2880/5926 - Loss:  0.914, Acc:  0.625, Seconds: 13.32\n",
      "Epoch   1/5 Batch 2900/5926 - Loss:  0.898, Acc:  0.625, Seconds: 12.08\n",
      "Epoch   1/5 Batch 2920/5926 - Loss:  0.927, Acc:  0.516, Seconds: 6.80\n",
      "Epoch   1/5 Batch 2940/5926 - Loss:  0.886, Acc:  0.641, Seconds: 8.65\n",
      "Epoch   1/5 Batch 2960/5926 - Loss:  0.899, Acc:  0.578, Seconds: 14.70\n",
      "Epoch   1/5 Batch 2980/5926 - Loss:  0.953, Acc:  0.719, Seconds: 16.68\n",
      "Epoch   1/5 Batch 3000/5926 - Loss:  0.948, Acc:  0.516, Seconds: 8.29\n",
      "Epoch   1/5 Batch 3020/5926 - Loss:  0.900, Acc:  0.562, Seconds: 8.82\n",
      "Epoch   1/5 Batch 3040/5926 - Loss:  0.875, Acc:  0.641, Seconds: 14.71\n",
      "Epoch   1/5 Batch 3060/5926 - Loss:  0.892, Acc:  0.688, Seconds: 9.16\n",
      "Epoch   1/5 Batch 3080/5926 - Loss:  0.879, Acc:  0.609, Seconds: 10.72\n",
      "Epoch   1/5 Batch 3100/5926 - Loss:  0.911, Acc:  0.625, Seconds: 16.22\n",
      "Epoch   1/5 Batch 3120/5926 - Loss:  0.921, Acc:  0.672, Seconds: 10.04\n",
      "Epoch   1/5 Batch 3140/5926 - Loss:  0.919, Acc:  0.516, Seconds: 12.99\n",
      "Epoch   1/5 Batch 3160/5926 - Loss:  0.906, Acc:  0.641, Seconds: 16.61\n",
      "Epoch   1/5 Batch 3180/5926 - Loss:  0.909, Acc:  0.609, Seconds: 13.20\n",
      "Epoch   1/5 Batch 3200/5926 - Loss:  0.876, Acc:  0.562, Seconds: 10.71\n",
      "Epoch   1/5 Batch 3220/5926 - Loss:  0.925, Acc:  0.625, Seconds: 9.51\n",
      "Epoch   1/5 Batch 3240/5926 - Loss:  0.887, Acc:  0.719, Seconds: 16.76\n",
      "Epoch   1/5 Batch 3260/5926 - Loss:  0.909, Acc:  0.594, Seconds: 10.14\n",
      "Epoch   1/5 Batch 3280/5926 - Loss:  0.904, Acc:  0.641, Seconds: 9.71\n",
      "Epoch   1/5 Batch 3300/5926 - Loss:  0.931, Acc:  0.688, Seconds: 21.38\n",
      "Epoch   1/5 Batch 3320/5926 - Loss:  0.920, Acc:  0.547, Seconds: 13.49\n",
      "Epoch   1/5 Batch 3340/5926 - Loss:  0.890, Acc:  0.641, Seconds: 14.36\n",
      "Epoch   1/5 Batch 3360/5926 - Loss:  0.886, Acc:  0.562, Seconds: 11.52\n",
      "Epoch   1/5 Batch 3380/5926 - Loss:  0.894, Acc:  0.594, Seconds: 14.30\n",
      "Epoch   1/5 Batch 3400/5926 - Loss:  0.896, Acc:  0.609, Seconds: 12.56\n",
      "Epoch   1/5 Batch 3420/5926 - Loss:  0.898, Acc:  0.688, Seconds: 15.66\n",
      "Epoch   1/5 Batch 3440/5926 - Loss:  0.924, Acc:  0.625, Seconds: 13.33\n",
      "Epoch   1/5 Batch 3460/5926 - Loss:  0.901, Acc:  0.578, Seconds: 12.77\n",
      "Epoch   1/5 Batch 3480/5926 - Loss:  0.853, Acc:  0.578, Seconds: 18.79\n",
      "Epoch   1/5 Batch 3500/5926 - Loss:  0.891, Acc:  0.578, Seconds: 13.98\n",
      "Epoch   1/5 Batch 3520/5926 - Loss:  0.886, Acc:  0.703, Seconds: 14.23\n",
      "Epoch   1/5 Batch 3540/5926 - Loss:  0.939, Acc:  0.531, Seconds: 11.48\n",
      "Epoch   1/5 Batch 3560/5926 - Loss:  0.921, Acc:  0.547, Seconds: 15.52\n",
      "Epoch   1/5 Batch 3580/5926 - Loss:  0.897, Acc:  0.656, Seconds: 8.99\n",
      "Epoch   1/5 Batch 3600/5926 - Loss:  0.929, Acc:  0.672, Seconds: 13.46\n",
      "Epoch   1/5 Batch 3620/5926 - Loss:  0.898, Acc:  0.703, Seconds: 6.26\n",
      "Epoch   1/5 Batch 3640/5926 - Loss:  0.861, Acc:  0.656, Seconds: 13.24\n",
      "Epoch   1/5 Batch 3660/5926 - Loss:  0.871, Acc:  0.672, Seconds: 14.58\n",
      "Epoch   1/5 Batch 3680/5926 - Loss:  0.896, Acc:  0.672, Seconds: 13.98\n",
      "Epoch   1/5 Batch 3700/5926 - Loss:  0.896, Acc:  0.625, Seconds: 18.44\n",
      "Average loss for this update: 0.904\n",
      "New Record!\n",
      "Epoch   1/5 Batch 3720/5926 - Loss:  0.920, Acc:  0.641, Seconds: 20.27\n",
      "Epoch   1/5 Batch 3740/5926 - Loss:  0.893, Acc:  0.641, Seconds: 8.80\n",
      "Epoch   1/5 Batch 3760/5926 - Loss:  0.901, Acc:  0.562, Seconds: 14.89\n",
      "Epoch   1/5 Batch 3780/5926 - Loss:  0.925, Acc:  0.609, Seconds: 21.50\n",
      "Epoch   1/5 Batch 3800/5926 - Loss:  0.929, Acc:  0.625, Seconds: 17.66\n",
      "Epoch   1/5 Batch 3820/5926 - Loss:  0.893, Acc:  0.641, Seconds: 17.85\n",
      "Epoch   1/5 Batch 3840/5926 - Loss:  0.884, Acc:  0.594, Seconds: 12.35\n",
      "Epoch   1/5 Batch 3860/5926 - Loss:  0.940, Acc:  0.547, Seconds: 13.63\n",
      "Epoch   1/5 Batch 3880/5926 - Loss:  0.910, Acc:  0.688, Seconds: 17.52\n",
      "Epoch   1/5 Batch 3900/5926 - Loss:  0.903, Acc:  0.594, Seconds: 11.06\n",
      "Epoch   1/5 Batch 3920/5926 - Loss:  0.931, Acc:  0.656, Seconds: 16.99\n",
      "Epoch   1/5 Batch 3940/5926 - Loss:  0.939, Acc:  0.547, Seconds: 10.39\n",
      "Epoch   1/5 Batch 3960/5926 - Loss:  0.934, Acc:  0.578, Seconds: 12.44\n",
      "Epoch   1/5 Batch 3980/5926 - Loss:  0.908, Acc:  0.656, Seconds: 12.67\n",
      "Epoch   1/5 Batch 4000/5926 - Loss:  0.915, Acc:  0.500, Seconds: 10.99\n",
      "Epoch   1/5 Batch 4020/5926 - Loss:  0.897, Acc:  0.578, Seconds: 16.57\n",
      "Epoch   1/5 Batch 4040/5926 - Loss:  0.906, Acc:  0.641, Seconds: 10.38\n",
      "Epoch   1/5 Batch 4060/5926 - Loss:  0.928, Acc:  0.594, Seconds: 8.99\n",
      "Epoch   1/5 Batch 4080/5926 - Loss:  0.905, Acc:  0.641, Seconds: 9.90\n",
      "Epoch   1/5 Batch 4100/5926 - Loss:  0.896, Acc:  0.578, Seconds: 15.62\n",
      "Epoch   1/5 Batch 4120/5926 - Loss:  0.901, Acc:  0.609, Seconds: 13.22\n",
      "Epoch   1/5 Batch 4140/5926 - Loss:  0.915, Acc:  0.641, Seconds: 15.41\n",
      "Epoch   1/5 Batch 4160/5926 - Loss:  0.914, Acc:  0.672, Seconds: 9.38\n",
      "Epoch   1/5 Batch 4180/5926 - Loss:  0.902, Acc:  0.562, Seconds: 8.41\n",
      "Epoch   1/5 Batch 4200/5926 - Loss:  0.889, Acc:  0.562, Seconds: 10.06\n",
      "Epoch   1/5 Batch 4220/5926 - Loss:  0.877, Acc:  0.609, Seconds: 21.31\n",
      "Epoch   1/5 Batch 4240/5926 - Loss:  0.861, Acc:  0.625, Seconds: 11.03\n",
      "Epoch   1/5 Batch 4260/5926 - Loss:  0.890, Acc:  0.656, Seconds: 20.43\n",
      "Epoch   1/5 Batch 4280/5926 - Loss:  0.907, Acc:  0.656, Seconds: 11.69\n",
      "Epoch   1/5 Batch 4300/5926 - Loss:  0.895, Acc:  0.609, Seconds: 11.14\n",
      "Epoch   1/5 Batch 4320/5926 - Loss:  0.904, Acc:  0.625, Seconds: 9.30\n",
      "Epoch   1/5 Batch 4340/5926 - Loss:  0.877, Acc:  0.625, Seconds: 11.89\n",
      "Epoch   1/5 Batch 4360/5926 - Loss:  0.897, Acc:  0.641, Seconds: 9.79\n",
      "Epoch   1/5 Batch 4380/5926 - Loss:  0.879, Acc:  0.547, Seconds: 10.17\n",
      "Epoch   1/5 Batch 4400/5926 - Loss:  0.880, Acc:  0.578, Seconds: 20.70\n",
      "Epoch   1/5 Batch 4420/5926 - Loss:  0.920, Acc:  0.562, Seconds: 12.00\n",
      "Epoch   1/5 Batch 4440/5926 - Loss:  0.895, Acc:  0.609, Seconds: 8.11\n",
      "Epoch   1/5 Batch 4460/5926 - Loss:  0.896, Acc:  0.562, Seconds: 14.43\n",
      "Epoch   1/5 Batch 4480/5926 - Loss:  0.866, Acc:  0.609, Seconds: 8.08\n",
      "Epoch   1/5 Batch 4500/5926 - Loss:  0.896, Acc:  0.656, Seconds: 17.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/5 Batch 4520/5926 - Loss:  0.868, Acc:  0.656, Seconds: 9.82\n",
      "Epoch   1/5 Batch 4540/5926 - Loss:  0.888, Acc:  0.625, Seconds: 11.33\n",
      "Epoch   1/5 Batch 4560/5926 - Loss:  0.970, Acc:  0.672, Seconds: 20.58\n",
      "Epoch   1/5 Batch 4580/5926 - Loss:  0.878, Acc:  0.641, Seconds: 14.37\n",
      "Epoch   1/5 Batch 4600/5926 - Loss:  0.875, Acc:  0.625, Seconds: 11.76\n",
      "Epoch   1/5 Batch 4620/5926 - Loss:  0.858, Acc:  0.625, Seconds: 11.43\n",
      "Average loss for this update: 0.901\n",
      "New Record!\n",
      "Epoch   1/5 Batch 4640/5926 - Loss:  0.918, Acc:  0.625, Seconds: 10.75\n",
      "Epoch   1/5 Batch 4660/5926 - Loss:  0.868, Acc:  0.719, Seconds: 8.94\n",
      "Epoch   1/5 Batch 4680/5926 - Loss:  0.886, Acc:  0.641, Seconds: 15.44\n",
      "Epoch   1/5 Batch 4700/5926 - Loss:  0.899, Acc:  0.656, Seconds: 12.99\n",
      "Epoch   1/5 Batch 4720/5926 - Loss:  0.938, Acc:  0.609, Seconds: 11.25\n",
      "Epoch   1/5 Batch 4740/5926 - Loss:  0.883, Acc:  0.672, Seconds: 15.61\n",
      "Epoch   1/5 Batch 4760/5926 - Loss:  0.907, Acc:  0.531, Seconds: 13.83\n",
      "Epoch   1/5 Batch 4780/5926 - Loss:  0.900, Acc:  0.688, Seconds: 17.27\n",
      "Epoch   1/5 Batch 4800/5926 - Loss:  0.890, Acc:  0.719, Seconds: 21.82\n",
      "Epoch   1/5 Batch 4820/5926 - Loss:  0.942, Acc:  0.594, Seconds: 14.02\n",
      "Epoch   1/5 Batch 4840/5926 - Loss:  0.896, Acc:  0.578, Seconds: 11.35\n",
      "Epoch   1/5 Batch 4860/5926 - Loss:  0.888, Acc:  0.469, Seconds: 10.43\n",
      "Epoch   1/5 Batch 4880/5926 - Loss:  0.916, Acc:  0.500, Seconds: 10.47\n",
      "Epoch   1/5 Batch 4900/5926 - Loss:  0.891, Acc:  0.547, Seconds: 15.15\n",
      "Epoch   1/5 Batch 4920/5926 - Loss:  0.900, Acc:  0.500, Seconds: 17.97\n",
      "Epoch   1/5 Batch 4940/5926 - Loss:  0.897, Acc:  0.688, Seconds: 9.32\n",
      "Epoch   1/5 Batch 4960/5926 - Loss:  0.936, Acc:  0.578, Seconds: 11.79\n",
      "Epoch   1/5 Batch 4980/5926 - Loss:  0.868, Acc:  0.688, Seconds: 15.50\n",
      "Epoch   1/5 Batch 5000/5926 - Loss:  0.888, Acc:  0.516, Seconds: 12.27\n",
      "Epoch   1/5 Batch 5020/5926 - Loss:  0.874, Acc:  0.578, Seconds: 12.98\n",
      "Epoch   1/5 Batch 5040/5926 - Loss:  0.900, Acc:  0.562, Seconds: 11.27\n",
      "Epoch   1/5 Batch 5060/5926 - Loss:  0.870, Acc:  0.516, Seconds: 16.75\n",
      "Epoch   1/5 Batch 5080/5926 - Loss:  0.914, Acc:  0.609, Seconds: 13.50\n",
      "Epoch   1/5 Batch 5100/5926 - Loss:  0.891, Acc:  0.688, Seconds: 11.22\n",
      "Epoch   1/5 Batch 5120/5926 - Loss:  0.928, Acc:  0.625, Seconds: 11.12\n",
      "Epoch   1/5 Batch 5140/5926 - Loss:  0.891, Acc:  0.547, Seconds: 8.44\n",
      "Epoch   1/5 Batch 5160/5926 - Loss:  0.913, Acc:  0.703, Seconds: 13.32\n",
      "Epoch   1/5 Batch 5180/5926 - Loss:  0.862, Acc:  0.672, Seconds: 15.02\n",
      "Epoch   1/5 Batch 5200/5926 - Loss:  0.884, Acc:  0.625, Seconds: 14.11\n",
      "Epoch   1/5 Batch 5220/5926 - Loss:  0.831, Acc:  0.547, Seconds: 12.77\n",
      "Epoch   1/5 Batch 5240/5926 - Loss:  0.901, Acc:  0.703, Seconds: 9.32\n",
      "Epoch   1/5 Batch 5260/5926 - Loss:  0.922, Acc:  0.500, Seconds: 11.57\n",
      "Epoch   1/5 Batch 5280/5926 - Loss:  0.915, Acc:  0.578, Seconds: 9.85\n",
      "Epoch   1/5 Batch 5300/5926 - Loss:  0.903, Acc:  0.594, Seconds: 11.89\n",
      "Epoch   1/5 Batch 5320/5926 - Loss:  0.889, Acc:  0.656, Seconds: 11.07\n",
      "Epoch   1/5 Batch 5340/5926 - Loss:  0.877, Acc:  0.719, Seconds: 15.00\n",
      "Epoch   1/5 Batch 5360/5926 - Loss:  0.854, Acc:  0.547, Seconds: 13.71\n",
      "Epoch   1/5 Batch 5380/5926 - Loss:  0.877, Acc:  0.594, Seconds: 16.24\n",
      "Epoch   1/5 Batch 5400/5926 - Loss:  0.899, Acc:  0.609, Seconds: 22.65\n",
      "Epoch   1/5 Batch 5420/5926 - Loss:  0.903, Acc:  0.641, Seconds: 9.66\n",
      "Epoch   1/5 Batch 5440/5926 - Loss:  0.885, Acc:  0.672, Seconds: 6.61\n",
      "Epoch   1/5 Batch 5460/5926 - Loss:  0.900, Acc:  0.578, Seconds: 21.54\n",
      "Epoch   1/5 Batch 5480/5926 - Loss:  0.906, Acc:  0.688, Seconds: 19.59\n",
      "Epoch   1/5 Batch 5500/5926 - Loss:  0.893, Acc:  0.688, Seconds: 14.53\n",
      "Epoch   1/5 Batch 5520/5926 - Loss:  0.881, Acc:  0.625, Seconds: 17.25\n",
      "Epoch   1/5 Batch 5540/5926 - Loss:  0.900, Acc:  0.594, Seconds: 14.45\n",
      "Average loss for this update: 0.895\n",
      "New Record!\n",
      "Epoch   1/5 Batch 5560/5926 - Loss:  0.930, Acc:  0.453, Seconds: 16.47\n",
      "Epoch   1/5 Batch 5580/5926 - Loss:  0.895, Acc:  0.531, Seconds: 13.60\n",
      "Epoch   1/5 Batch 5600/5926 - Loss:  0.878, Acc:  0.609, Seconds: 11.93\n",
      "Epoch   1/5 Batch 5620/5926 - Loss:  0.905, Acc:  0.500, Seconds: 7.11\n",
      "Epoch   1/5 Batch 5640/5926 - Loss:  0.922, Acc:  0.594, Seconds: 8.52\n",
      "Epoch   1/5 Batch 5660/5926 - Loss:  0.870, Acc:  0.562, Seconds: 13.02\n",
      "Epoch   1/5 Batch 5680/5926 - Loss:  0.915, Acc:  0.703, Seconds: 10.85\n",
      "Epoch   1/5 Batch 5700/5926 - Loss:  0.943, Acc:  0.438, Seconds: 19.77\n",
      "Epoch   1/5 Batch 5720/5926 - Loss:  0.932, Acc:  0.656, Seconds: 7.29\n",
      "Epoch   1/5 Batch 5740/5926 - Loss:  0.933, Acc:  0.688, Seconds: 16.72\n",
      "Epoch   1/5 Batch 5760/5926 - Loss:  0.921, Acc:  0.547, Seconds: 15.64\n",
      "Epoch   1/5 Batch 5780/5926 - Loss:  0.922, Acc:  0.531, Seconds: 13.60\n",
      "Epoch   1/5 Batch 5800/5926 - Loss:  0.911, Acc:  0.594, Seconds: 8.48\n",
      "Epoch   1/5 Batch 5820/5926 - Loss:  0.916, Acc:  0.578, Seconds: 12.37\n",
      "Epoch   1/5 Batch 5840/5926 - Loss:  0.909, Acc:  0.562, Seconds: 17.93\n",
      "Epoch   1/5 Batch 5860/5926 - Loss:  0.892, Acc:  0.609, Seconds: 12.97\n",
      "Epoch   1/5 Batch 5880/5926 - Loss:  0.917, Acc:  0.594, Seconds: 16.63\n",
      "Epoch   1/5 Batch 5900/5926 - Loss:  0.913, Acc:  0.625, Seconds: 12.27\n",
      "Epoch   1/5 Batch 5920/5926 - Loss:  0.891, Acc:  0.641, Seconds: 18.82\n",
      "Starting\n",
      "Finished first\n",
      "Epoch   2/5 Batch   20/5926 - Loss:  0.948, Acc:  0.656, Seconds: 11.82\n",
      "Epoch   2/5 Batch   40/5926 - Loss:  0.920, Acc:  0.500, Seconds: 10.04\n",
      "Epoch   2/5 Batch   60/5926 - Loss:  0.916, Acc:  0.609, Seconds: 7.59\n",
      "Epoch   2/5 Batch   80/5926 - Loss:  0.875, Acc:  0.594, Seconds: 9.52\n",
      "Epoch   2/5 Batch  100/5926 - Loss:  0.867, Acc:  0.562, Seconds: 15.16\n",
      "Epoch   2/5 Batch  120/5926 - Loss:  0.863, Acc:  0.594, Seconds: 19.38\n",
      "Epoch   2/5 Batch  140/5926 - Loss:  0.925, Acc:  0.609, Seconds: 17.26\n",
      "Epoch   2/5 Batch  160/5926 - Loss:  0.893, Acc:  0.594, Seconds: 15.04\n",
      "Epoch   2/5 Batch  180/5926 - Loss:  0.871, Acc:  0.641, Seconds: 18.22\n",
      "Epoch   2/5 Batch  200/5926 - Loss:  0.847, Acc:  0.688, Seconds: 13.13\n",
      "Epoch   2/5 Batch  220/5926 - Loss:  0.913, Acc:  0.625, Seconds: 12.27\n",
      "Epoch   2/5 Batch  240/5926 - Loss:  0.882, Acc:  0.578, Seconds: 7.44\n",
      "Epoch   2/5 Batch  260/5926 - Loss:  0.890, Acc:  0.625, Seconds: 17.30\n",
      "Epoch   2/5 Batch  280/5926 - Loss:  0.870, Acc:  0.547, Seconds: 7.74\n",
      "Epoch   2/5 Batch  300/5926 - Loss:  0.880, Acc:  0.625, Seconds: 15.43\n",
      "Epoch   2/5 Batch  320/5926 - Loss:  0.870, Acc:  0.562, Seconds: 14.42\n",
      "Epoch   2/5 Batch  340/5926 - Loss:  0.914, Acc:  0.656, Seconds: 13.29\n",
      "Epoch   2/5 Batch  360/5926 - Loss:  0.903, Acc:  0.641, Seconds: 12.23\n",
      "Epoch   2/5 Batch  380/5926 - Loss:  0.900, Acc:  0.625, Seconds: 17.71\n",
      "Epoch   2/5 Batch  400/5926 - Loss:  0.885, Acc:  0.641, Seconds: 15.09\n",
      "Epoch   2/5 Batch  420/5926 - Loss:  0.881, Acc:  0.594, Seconds: 10.06\n",
      "Epoch   2/5 Batch  440/5926 - Loss:  0.887, Acc:  0.562, Seconds: 11.26\n",
      "Epoch   2/5 Batch  460/5926 - Loss:  0.913, Acc:  0.562, Seconds: 7.48\n",
      "Epoch   2/5 Batch  480/5926 - Loss:  0.877, Acc:  0.641, Seconds: 8.20\n",
      "Epoch   2/5 Batch  500/5926 - Loss:  0.877, Acc:  0.672, Seconds: 20.98\n",
      "Epoch   2/5 Batch  520/5926 - Loss:  0.892, Acc:  0.469, Seconds: 15.36\n",
      "Epoch   2/5 Batch  540/5926 - Loss:  0.905, Acc:  0.609, Seconds: 9.65\n",
      "Epoch   2/5 Batch  560/5926 - Loss:  0.852, Acc:  0.531, Seconds: 16.98\n",
      "Epoch   2/5 Batch  580/5926 - Loss:  0.873, Acc:  0.578, Seconds: 12.20\n",
      "Epoch   2/5 Batch  600/5926 - Loss:  0.880, Acc:  0.531, Seconds: 12.47\n",
      "Epoch   2/5 Batch  620/5926 - Loss:  0.871, Acc:  0.719, Seconds: 9.01\n",
      "Epoch   2/5 Batch  640/5926 - Loss:  0.881, Acc:  0.656, Seconds: 8.76\n",
      "Epoch   2/5 Batch  660/5926 - Loss:  0.892, Acc:  0.547, Seconds: 12.11\n",
      "Epoch   2/5 Batch  680/5926 - Loss:  0.881, Acc:  0.484, Seconds: 10.35\n",
      "Epoch   2/5 Batch  700/5926 - Loss:  0.902, Acc:  0.609, Seconds: 12.59\n",
      "Epoch   2/5 Batch  720/5926 - Loss:  0.823, Acc:  0.688, Seconds: 8.66\n",
      "Epoch   2/5 Batch  740/5926 - Loss:  0.901, Acc:  0.562, Seconds: 14.46\n",
      "Epoch   2/5 Batch  760/5926 - Loss:  0.876, Acc:  0.641, Seconds: 22.36\n",
      "Epoch   2/5 Batch  780/5926 - Loss:  0.863, Acc:  0.656, Seconds: 10.72\n",
      "Epoch   2/5 Batch  800/5926 - Loss:  0.913, Acc:  0.562, Seconds: 10.74\n",
      "Epoch   2/5 Batch  820/5926 - Loss:  0.915, Acc:  0.531, Seconds: 17.27\n",
      "Epoch   2/5 Batch  840/5926 - Loss:  0.931, Acc:  0.484, Seconds: 6.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/5 Batch  860/5926 - Loss:  0.939, Acc:  0.656, Seconds: 10.30\n",
      "Epoch   2/5 Batch  880/5926 - Loss:  0.911, Acc:  0.578, Seconds: 17.61\n",
      "Epoch   2/5 Batch  900/5926 - Loss:  0.891, Acc:  0.562, Seconds: 10.38\n",
      "Epoch   2/5 Batch  920/5926 - Loss:  0.873, Acc:  0.688, Seconds: 18.47\n",
      "Average loss for this update: 0.889\n",
      "New Record!\n",
      "Epoch   2/5 Batch  940/5926 - Loss:  0.841, Acc:  0.641, Seconds: 17.79\n",
      "Epoch   2/5 Batch  960/5926 - Loss:  0.893, Acc:  0.578, Seconds: 16.22\n",
      "Epoch   2/5 Batch  980/5926 - Loss:  0.861, Acc:  0.625, Seconds: 13.08\n",
      "Epoch   2/5 Batch 1000/5926 - Loss:  0.866, Acc:  0.578, Seconds: 12.84\n",
      "Epoch   2/5 Batch 1020/5926 - Loss:  0.932, Acc:  0.594, Seconds: 7.13\n",
      "Epoch   2/5 Batch 1040/5926 - Loss:  0.871, Acc:  0.641, Seconds: 12.31\n",
      "Epoch   2/5 Batch 1060/5926 - Loss:  0.876, Acc:  0.609, Seconds: 14.44\n",
      "Epoch   2/5 Batch 1080/5926 - Loss:  0.874, Acc:  0.562, Seconds: 10.60\n",
      "Epoch   2/5 Batch 1100/5926 - Loss:  0.868, Acc:  0.594, Seconds: 11.87\n",
      "Epoch   2/5 Batch 1120/5926 - Loss:  0.903, Acc:  0.672, Seconds: 15.36\n",
      "Epoch   2/5 Batch 1140/5926 - Loss:  0.923, Acc:  0.578, Seconds: 15.92\n",
      "Epoch   2/5 Batch 1160/5926 - Loss:  0.865, Acc:  0.609, Seconds: 10.03\n",
      "Epoch   2/5 Batch 1180/5926 - Loss:  0.915, Acc:  0.703, Seconds: 9.29\n",
      "Epoch   2/5 Batch 1200/5926 - Loss:  0.906, Acc:  0.500, Seconds: 11.51\n",
      "Epoch   2/5 Batch 1220/5926 - Loss:  0.891, Acc:  0.516, Seconds: 22.49\n",
      "Epoch   2/5 Batch 1240/5926 - Loss:  0.901, Acc:  0.578, Seconds: 7.68\n",
      "Epoch   2/5 Batch 1260/5926 - Loss:  0.930, Acc:  0.609, Seconds: 16.98\n",
      "Epoch   2/5 Batch 1280/5926 - Loss:  0.879, Acc:  0.656, Seconds: 11.12\n",
      "Epoch   2/5 Batch 1300/5926 - Loss:  0.883, Acc:  0.484, Seconds: 8.38\n",
      "Epoch   2/5 Batch 1320/5926 - Loss:  0.869, Acc:  0.688, Seconds: 9.45\n",
      "Epoch   2/5 Batch 1340/5926 - Loss:  0.875, Acc:  0.609, Seconds: 10.74\n",
      "Epoch   2/5 Batch 1360/5926 - Loss:  0.882, Acc:  0.672, Seconds: 17.14\n",
      "Epoch   2/5 Batch 1380/5926 - Loss:  0.880, Acc:  0.703, Seconds: 16.06\n",
      "Epoch   2/5 Batch 1400/5926 - Loss:  0.887, Acc:  0.656, Seconds: 12.09\n",
      "Epoch   2/5 Batch 1420/5926 - Loss:  0.902, Acc:  0.438, Seconds: 8.85\n",
      "Epoch   2/5 Batch 1440/5926 - Loss:  0.866, Acc:  0.625, Seconds: 10.40\n",
      "Epoch   2/5 Batch 1460/5926 - Loss:  0.883, Acc:  0.578, Seconds: 16.89\n",
      "Epoch   2/5 Batch 1480/5926 - Loss:  0.913, Acc:  0.609, Seconds: 11.33\n",
      "Epoch   2/5 Batch 1500/5926 - Loss:  0.924, Acc:  0.562, Seconds: 16.74\n",
      "Epoch   2/5 Batch 1520/5926 - Loss:  0.889, Acc:  0.594, Seconds: 13.21\n",
      "Epoch   2/5 Batch 1540/5926 - Loss:  0.847, Acc:  0.578, Seconds: 8.96\n",
      "Epoch   2/5 Batch 1560/5926 - Loss:  0.867, Acc:  0.641, Seconds: 10.04\n",
      "Epoch   2/5 Batch 1580/5926 - Loss:  0.853, Acc:  0.625, Seconds: 19.27\n",
      "Epoch   2/5 Batch 1600/5926 - Loss:  0.843, Acc:  0.688, Seconds: 6.93\n",
      "Epoch   2/5 Batch 1620/5926 - Loss:  0.875, Acc:  0.703, Seconds: 17.68\n",
      "Epoch   2/5 Batch 1640/5926 - Loss:  0.857, Acc:  0.625, Seconds: 7.59\n",
      "Epoch   2/5 Batch 1660/5926 - Loss:  0.895, Acc:  0.625, Seconds: 19.26\n",
      "Epoch   2/5 Batch 1680/5926 - Loss:  0.863, Acc:  0.625, Seconds: 10.96\n",
      "Epoch   2/5 Batch 1700/5926 - Loss:  0.859, Acc:  0.609, Seconds: 10.20\n",
      "Epoch   2/5 Batch 1720/5926 - Loss:  0.888, Acc:  0.641, Seconds: 10.77\n",
      "Epoch   2/5 Batch 1740/5926 - Loss:  0.866, Acc:  0.625, Seconds: 14.00\n",
      "Epoch   2/5 Batch 1760/5926 - Loss:  0.869, Acc:  0.562, Seconds: 19.93\n",
      "Epoch   2/5 Batch 1780/5926 - Loss:  0.907, Acc:  0.594, Seconds: 7.82\n",
      "Epoch   2/5 Batch 1800/5926 - Loss:  0.864, Acc:  0.609, Seconds: 13.27\n",
      "Epoch   2/5 Batch 1820/5926 - Loss:  0.866, Acc:  0.625, Seconds: 11.22\n",
      "Epoch   2/5 Batch 1840/5926 - Loss:  0.927, Acc:  0.562, Seconds: 16.87\n",
      "Average loss for this update: 0.883\n",
      "New Record!\n",
      "Epoch   2/5 Batch 1860/5926 - Loss:  0.901, Acc:  0.609, Seconds: 12.59\n",
      "Epoch   2/5 Batch 1880/5926 - Loss:  0.865, Acc:  0.656, Seconds: 16.11\n",
      "Epoch   2/5 Batch 1900/5926 - Loss:  0.881, Acc:  0.578, Seconds: 9.84\n",
      "Epoch   2/5 Batch 1920/5926 - Loss:  0.888, Acc:  0.703, Seconds: 14.17\n",
      "Epoch   2/5 Batch 1940/5926 - Loss:  0.888, Acc:  0.688, Seconds: 17.80\n",
      "Epoch   2/5 Batch 1960/5926 - Loss:  0.859, Acc:  0.625, Seconds: 21.34\n",
      "Epoch   2/5 Batch 1980/5926 - Loss:  0.881, Acc:  0.641, Seconds: 16.35\n",
      "Epoch   2/5 Batch 2000/5926 - Loss:  0.882, Acc:  0.547, Seconds: 11.60\n",
      "Epoch   2/5 Batch 2020/5926 - Loss:  0.928, Acc:  0.547, Seconds: 10.07\n",
      "Epoch   2/5 Batch 2040/5926 - Loss:  0.903, Acc:  0.703, Seconds: 11.29\n",
      "Epoch   2/5 Batch 2060/5926 - Loss:  0.899, Acc:  0.609, Seconds: 8.78\n",
      "Epoch   2/5 Batch 2080/5926 - Loss:  0.895, Acc:  0.594, Seconds: 8.95\n",
      "Epoch   2/5 Batch 2100/5926 - Loss:  0.899, Acc:  0.609, Seconds: 16.53\n",
      "Epoch   2/5 Batch 2120/5926 - Loss:  0.865, Acc:  0.656, Seconds: 15.37\n",
      "Epoch   2/5 Batch 2140/5926 - Loss:  0.849, Acc:  0.609, Seconds: 10.33\n",
      "Epoch   2/5 Batch 2160/5926 - Loss:  0.910, Acc:  0.641, Seconds: 15.78\n",
      "Epoch   2/5 Batch 2180/5926 - Loss:  0.850, Acc:  0.672, Seconds: 6.52\n",
      "Epoch   2/5 Batch 2200/5926 - Loss:  0.873, Acc:  0.641, Seconds: 23.47\n",
      "Epoch   2/5 Batch 2220/5926 - Loss:  0.908, Acc:  0.703, Seconds: 12.90\n",
      "Epoch   2/5 Batch 2240/5926 - Loss:  0.889, Acc:  0.516, Seconds: 8.26\n",
      "Epoch   2/5 Batch 2260/5926 - Loss:  0.907, Acc:  0.625, Seconds: 11.52\n",
      "Epoch   2/5 Batch 2280/5926 - Loss:  0.850, Acc:  0.609, Seconds: 21.43\n",
      "Epoch   2/5 Batch 2300/5926 - Loss:  0.888, Acc:  0.500, Seconds: 22.79\n",
      "Epoch   2/5 Batch 2320/5926 - Loss:  0.928, Acc:  0.578, Seconds: 17.89\n",
      "Epoch   2/5 Batch 2340/5926 - Loss:  0.843, Acc:  0.641, Seconds: 13.45\n",
      "Epoch   2/5 Batch 2360/5926 - Loss:  0.904, Acc:  0.656, Seconds: 10.01\n",
      "Epoch   2/5 Batch 2380/5926 - Loss:  0.878, Acc:  0.547, Seconds: 16.93\n",
      "Epoch   2/5 Batch 2400/5926 - Loss:  0.851, Acc:  0.672, Seconds: 13.20\n",
      "Epoch   2/5 Batch 2420/5926 - Loss:  0.872, Acc:  0.641, Seconds: 17.41\n",
      "Epoch   2/5 Batch 2440/5926 - Loss:  0.840, Acc:  0.703, Seconds: 14.36\n",
      "Epoch   2/5 Batch 2460/5926 - Loss:  0.885, Acc:  0.641, Seconds: 10.59\n",
      "Epoch   2/5 Batch 2480/5926 - Loss:  0.904, Acc:  0.641, Seconds: 12.19\n",
      "Epoch   2/5 Batch 2500/5926 - Loss:  0.889, Acc:  0.641, Seconds: 12.81\n",
      "Epoch   2/5 Batch 2520/5926 - Loss:  0.872, Acc:  0.625, Seconds: 11.64\n",
      "Epoch   2/5 Batch 2540/5926 - Loss:  0.885, Acc:  0.594, Seconds: 9.88\n",
      "Epoch   2/5 Batch 2560/5926 - Loss:  0.845, Acc:  0.578, Seconds: 14.37\n",
      "Epoch   2/5 Batch 2580/5926 - Loss:  0.883, Acc:  0.578, Seconds: 12.93\n",
      "Epoch   2/5 Batch 2600/5926 - Loss:  0.907, Acc:  0.594, Seconds: 13.70\n",
      "Epoch   2/5 Batch 2620/5926 - Loss:  0.838, Acc:  0.547, Seconds: 20.01\n",
      "Epoch   2/5 Batch 2640/5926 - Loss:  0.864, Acc:  0.734, Seconds: 6.62\n",
      "Epoch   2/5 Batch 2660/5926 - Loss:  0.861, Acc:  0.672, Seconds: 25.31\n",
      "Epoch   2/5 Batch 2680/5926 - Loss:  0.865, Acc:  0.672, Seconds: 14.33\n",
      "Epoch   2/5 Batch 2700/5926 - Loss:  0.855, Acc:  0.656, Seconds: 18.67\n",
      "Epoch   2/5 Batch 2720/5926 - Loss:  0.862, Acc:  0.609, Seconds: 15.09\n",
      "Epoch   2/5 Batch 2740/5926 - Loss:  0.898, Acc:  0.688, Seconds: 11.60\n",
      "Epoch   2/5 Batch 2760/5926 - Loss:  0.891, Acc:  0.672, Seconds: 10.38\n",
      "Average loss for this update: 0.88\n",
      "New Record!\n",
      "Epoch   2/5 Batch 2780/5926 - Loss:  0.899, Acc:  0.578, Seconds: 9.74\n",
      "Epoch   2/5 Batch 2800/5926 - Loss:  0.903, Acc:  0.516, Seconds: 7.67\n",
      "Epoch   2/5 Batch 2820/5926 - Loss:  0.892, Acc:  0.625, Seconds: 21.20\n",
      "Epoch   2/5 Batch 2840/5926 - Loss:  0.873, Acc:  0.656, Seconds: 18.92\n",
      "Epoch   2/5 Batch 2860/5926 - Loss:  0.892, Acc:  0.625, Seconds: 21.54\n",
      "Epoch   2/5 Batch 2880/5926 - Loss:  0.890, Acc:  0.578, Seconds: 13.50\n",
      "Epoch   2/5 Batch 2900/5926 - Loss:  0.855, Acc:  0.562, Seconds: 10.94\n",
      "Epoch   2/5 Batch 2920/5926 - Loss:  0.883, Acc:  0.516, Seconds: 7.04\n",
      "Epoch   2/5 Batch 2940/5926 - Loss:  0.866, Acc:  0.703, Seconds: 8.65\n",
      "Epoch   2/5 Batch 2960/5926 - Loss:  0.851, Acc:  0.547, Seconds: 12.93\n",
      "Epoch   2/5 Batch 2980/5926 - Loss:  0.906, Acc:  0.625, Seconds: 15.03\n",
      "Epoch   2/5 Batch 3000/5926 - Loss:  0.912, Acc:  0.625, Seconds: 7.82\n",
      "Epoch   2/5 Batch 3020/5926 - Loss:  0.875, Acc:  0.594, Seconds: 8.93\n",
      "Epoch   2/5 Batch 3040/5926 - Loss:  0.862, Acc:  0.641, Seconds: 14.82\n",
      "Epoch   2/5 Batch 3060/5926 - Loss:  0.857, Acc:  0.688, Seconds: 9.85\n",
      "Epoch   2/5 Batch 3080/5926 - Loss:  0.851, Acc:  0.641, Seconds: 10.90\n",
      "Epoch   2/5 Batch 3100/5926 - Loss:  0.879, Acc:  0.719, Seconds: 14.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/5 Batch 3120/5926 - Loss:  0.890, Acc:  0.609, Seconds: 9.96\n",
      "Epoch   2/5 Batch 3140/5926 - Loss:  0.880, Acc:  0.609, Seconds: 17.51\n",
      "Epoch   2/5 Batch 3160/5926 - Loss:  0.872, Acc:  0.672, Seconds: 18.43\n",
      "Epoch   2/5 Batch 3180/5926 - Loss:  0.876, Acc:  0.672, Seconds: 14.03\n",
      "Epoch   2/5 Batch 3200/5926 - Loss:  0.863, Acc:  0.625, Seconds: 10.91\n",
      "Epoch   2/5 Batch 3220/5926 - Loss:  0.898, Acc:  0.641, Seconds: 9.25\n",
      "Epoch   2/5 Batch 3240/5926 - Loss:  0.867, Acc:  0.719, Seconds: 17.27\n",
      "Epoch   2/5 Batch 3260/5926 - Loss:  0.890, Acc:  0.609, Seconds: 12.68\n",
      "Epoch   2/5 Batch 3280/5926 - Loss:  0.875, Acc:  0.609, Seconds: 9.46\n",
      "Epoch   2/5 Batch 3300/5926 - Loss:  0.905, Acc:  0.656, Seconds: 21.80\n",
      "Epoch   2/5 Batch 3320/5926 - Loss:  0.868, Acc:  0.562, Seconds: 12.49\n",
      "Epoch   2/5 Batch 3340/5926 - Loss:  0.855, Acc:  0.656, Seconds: 15.71\n",
      "Epoch   2/5 Batch 3360/5926 - Loss:  0.854, Acc:  0.562, Seconds: 10.79\n",
      "Epoch   2/5 Batch 3380/5926 - Loss:  0.878, Acc:  0.594, Seconds: 14.76\n",
      "Epoch   2/5 Batch 3400/5926 - Loss:  0.858, Acc:  0.641, Seconds: 11.44\n",
      "Epoch   2/5 Batch 3420/5926 - Loss:  0.866, Acc:  0.641, Seconds: 16.37\n",
      "Epoch   2/5 Batch 3440/5926 - Loss:  0.912, Acc:  0.641, Seconds: 11.78\n",
      "Epoch   2/5 Batch 3460/5926 - Loss:  0.866, Acc:  0.672, Seconds: 13.11\n",
      "Epoch   2/5 Batch 3480/5926 - Loss:  0.849, Acc:  0.609, Seconds: 22.52\n",
      "Epoch   2/5 Batch 3500/5926 - Loss:  0.856, Acc:  0.609, Seconds: 13.16\n",
      "Epoch   2/5 Batch 3520/5926 - Loss:  0.865, Acc:  0.750, Seconds: 13.95\n",
      "Epoch   2/5 Batch 3540/5926 - Loss:  0.920, Acc:  0.641, Seconds: 12.57\n",
      "Epoch   2/5 Batch 3560/5926 - Loss:  0.873, Acc:  0.578, Seconds: 21.35\n",
      "Epoch   2/5 Batch 3580/5926 - Loss:  0.860, Acc:  0.609, Seconds: 9.67\n",
      "Epoch   2/5 Batch 3600/5926 - Loss:  0.888, Acc:  0.578, Seconds: 14.71\n",
      "Epoch   2/5 Batch 3620/5926 - Loss:  0.889, Acc:  0.672, Seconds: 6.47\n",
      "Epoch   2/5 Batch 3640/5926 - Loss:  0.858, Acc:  0.672, Seconds: 13.21\n",
      "Epoch   2/5 Batch 3660/5926 - Loss:  0.854, Acc:  0.656, Seconds: 12.32\n",
      "Epoch   2/5 Batch 3680/5926 - Loss:  0.846, Acc:  0.703, Seconds: 11.56\n",
      "Epoch   2/5 Batch 3700/5926 - Loss:  0.883, Acc:  0.609, Seconds: 17.89\n",
      "Average loss for this update: 0.876\n",
      "New Record!\n",
      "Epoch   2/5 Batch 3720/5926 - Loss:  0.883, Acc:  0.625, Seconds: 17.21\n",
      "Epoch   2/5 Batch 3740/5926 - Loss:  0.874, Acc:  0.641, Seconds: 7.21\n",
      "Epoch   2/5 Batch 3760/5926 - Loss:  0.857, Acc:  0.547, Seconds: 14.48\n",
      "Epoch   2/5 Batch 3780/5926 - Loss:  0.900, Acc:  0.656, Seconds: 21.97\n",
      "Epoch   2/5 Batch 3800/5926 - Loss:  0.884, Acc:  0.594, Seconds: 15.06\n",
      "Epoch   2/5 Batch 3820/5926 - Loss:  0.882, Acc:  0.672, Seconds: 16.80\n",
      "Epoch   2/5 Batch 3840/5926 - Loss:  0.860, Acc:  0.562, Seconds: 11.51\n",
      "Epoch   2/5 Batch 3860/5926 - Loss:  0.910, Acc:  0.594, Seconds: 14.87\n",
      "Epoch   2/5 Batch 3880/5926 - Loss:  0.882, Acc:  0.688, Seconds: 17.47\n",
      "Epoch   2/5 Batch 3900/5926 - Loss:  0.856, Acc:  0.656, Seconds: 10.82\n",
      "Epoch   2/5 Batch 3920/5926 - Loss:  0.914, Acc:  0.609, Seconds: 19.45\n",
      "Epoch   2/5 Batch 3940/5926 - Loss:  0.919, Acc:  0.625, Seconds: 10.70\n",
      "Epoch   2/5 Batch 3960/5926 - Loss:  0.888, Acc:  0.531, Seconds: 10.80\n",
      "Epoch   2/5 Batch 3980/5926 - Loss:  0.886, Acc:  0.656, Seconds: 13.72\n",
      "Epoch   2/5 Batch 4000/5926 - Loss:  0.895, Acc:  0.531, Seconds: 12.19\n",
      "Epoch   2/5 Batch 4020/5926 - Loss:  0.858, Acc:  0.594, Seconds: 16.97\n",
      "Epoch   2/5 Batch 4040/5926 - Loss:  0.864, Acc:  0.516, Seconds: 13.81\n",
      "Epoch   2/5 Batch 4060/5926 - Loss:  0.886, Acc:  0.609, Seconds: 9.19\n",
      "Epoch   2/5 Batch 4080/5926 - Loss:  0.868, Acc:  0.656, Seconds: 11.12\n",
      "Epoch   2/5 Batch 4100/5926 - Loss:  0.874, Acc:  0.594, Seconds: 15.49\n",
      "Epoch   2/5 Batch 4120/5926 - Loss:  0.874, Acc:  0.625, Seconds: 11.51\n",
      "Epoch   2/5 Batch 4140/5926 - Loss:  0.905, Acc:  0.688, Seconds: 14.48\n",
      "Epoch   2/5 Batch 4160/5926 - Loss:  0.892, Acc:  0.625, Seconds: 10.03\n",
      "Epoch   2/5 Batch 4180/5926 - Loss:  0.869, Acc:  0.531, Seconds: 8.39\n",
      "Epoch   2/5 Batch 4200/5926 - Loss:  0.856, Acc:  0.547, Seconds: 9.34\n",
      "Epoch   2/5 Batch 4220/5926 - Loss:  0.874, Acc:  0.594, Seconds: 21.87\n",
      "Epoch   2/5 Batch 4240/5926 - Loss:  0.834, Acc:  0.672, Seconds: 10.94\n",
      "Epoch   2/5 Batch 4260/5926 - Loss:  0.876, Acc:  0.719, Seconds: 20.23\n",
      "Epoch   2/5 Batch 4280/5926 - Loss:  0.880, Acc:  0.656, Seconds: 13.12\n",
      "Epoch   2/5 Batch 4300/5926 - Loss:  0.889, Acc:  0.672, Seconds: 10.87\n",
      "Epoch   2/5 Batch 4320/5926 - Loss:  0.879, Acc:  0.656, Seconds: 9.71\n",
      "Epoch   2/5 Batch 4340/5926 - Loss:  0.867, Acc:  0.641, Seconds: 12.32\n",
      "Epoch   2/5 Batch 4360/5926 - Loss:  0.862, Acc:  0.719, Seconds: 9.02\n",
      "Epoch   2/5 Batch 4380/5926 - Loss:  0.848, Acc:  0.578, Seconds: 12.43\n",
      "Epoch   2/5 Batch 4400/5926 - Loss:  0.877, Acc:  0.625, Seconds: 20.57\n",
      "Epoch   2/5 Batch 4420/5926 - Loss:  0.879, Acc:  0.641, Seconds: 13.08\n",
      "Epoch   2/5 Batch 4440/5926 - Loss:  0.858, Acc:  0.562, Seconds: 8.06\n",
      "Epoch   2/5 Batch 4460/5926 - Loss:  0.866, Acc:  0.609, Seconds: 17.12\n",
      "Epoch   2/5 Batch 4480/5926 - Loss:  0.827, Acc:  0.641, Seconds: 9.96\n",
      "Epoch   2/5 Batch 4500/5926 - Loss:  0.870, Acc:  0.641, Seconds: 17.16\n",
      "Epoch   2/5 Batch 4520/5926 - Loss:  0.841, Acc:  0.656, Seconds: 9.70\n",
      "Epoch   2/5 Batch 4540/5926 - Loss:  0.875, Acc:  0.547, Seconds: 10.95\n",
      "Epoch   2/5 Batch 4560/5926 - Loss:  0.925, Acc:  0.641, Seconds: 20.61\n",
      "Epoch   2/5 Batch 4580/5926 - Loss:  0.856, Acc:  0.609, Seconds: 13.35\n",
      "Epoch   2/5 Batch 4600/5926 - Loss:  0.852, Acc:  0.625, Seconds: 11.51\n",
      "Epoch   2/5 Batch 4620/5926 - Loss:  0.826, Acc:  0.688, Seconds: 12.93\n",
      "Average loss for this update: 0.874\n",
      "New Record!\n",
      "Epoch   2/5 Batch 4640/5926 - Loss:  0.878, Acc:  0.609, Seconds: 10.04\n",
      "Epoch   2/5 Batch 4660/5926 - Loss:  0.848, Acc:  0.672, Seconds: 9.59\n",
      "Epoch   2/5 Batch 4680/5926 - Loss:  0.877, Acc:  0.578, Seconds: 15.71\n",
      "Epoch   2/5 Batch 4700/5926 - Loss:  0.877, Acc:  0.641, Seconds: 12.14\n",
      "Epoch   2/5 Batch 4720/5926 - Loss:  0.904, Acc:  0.688, Seconds: 12.37\n",
      "Epoch   2/5 Batch 4740/5926 - Loss:  0.843, Acc:  0.656, Seconds: 18.04\n",
      "Epoch   2/5 Batch 4760/5926 - Loss:  0.892, Acc:  0.562, Seconds: 14.53\n",
      "Epoch   2/5 Batch 4780/5926 - Loss:  0.883, Acc:  0.609, Seconds: 17.52\n",
      "Epoch   2/5 Batch 4800/5926 - Loss:  0.895, Acc:  0.656, Seconds: 26.33\n",
      "Epoch   2/5 Batch 4820/5926 - Loss:  0.921, Acc:  0.609, Seconds: 14.70\n",
      "Epoch   2/5 Batch 4840/5926 - Loss:  0.871, Acc:  0.594, Seconds: 12.04\n",
      "Epoch   2/5 Batch 4860/5926 - Loss:  0.880, Acc:  0.547, Seconds: 11.56\n",
      "Epoch   2/5 Batch 4880/5926 - Loss:  0.892, Acc:  0.484, Seconds: 10.35\n",
      "Epoch   2/5 Batch 4900/5926 - Loss:  0.867, Acc:  0.578, Seconds: 15.54\n",
      "Epoch   2/5 Batch 4920/5926 - Loss:  0.867, Acc:  0.531, Seconds: 18.99\n",
      "Epoch   2/5 Batch 4940/5926 - Loss:  0.876, Acc:  0.734, Seconds: 9.25\n",
      "Epoch   2/5 Batch 4960/5926 - Loss:  0.906, Acc:  0.547, Seconds: 12.08\n",
      "Epoch   2/5 Batch 4980/5926 - Loss:  0.850, Acc:  0.734, Seconds: 14.72\n",
      "Epoch   2/5 Batch 5000/5926 - Loss:  0.861, Acc:  0.562, Seconds: 13.28\n",
      "Epoch   2/5 Batch 5020/5926 - Loss:  0.839, Acc:  0.562, Seconds: 11.21\n",
      "Epoch   2/5 Batch 5040/5926 - Loss:  0.877, Acc:  0.531, Seconds: 10.58\n",
      "Epoch   2/5 Batch 5060/5926 - Loss:  0.837, Acc:  0.562, Seconds: 17.45\n",
      "Epoch   2/5 Batch 5080/5926 - Loss:  0.890, Acc:  0.547, Seconds: 15.11\n",
      "Epoch   2/5 Batch 5100/5926 - Loss:  0.857, Acc:  0.719, Seconds: 10.72\n",
      "Epoch   2/5 Batch 5120/5926 - Loss:  0.887, Acc:  0.625, Seconds: 13.98\n",
      "Epoch   2/5 Batch 5140/5926 - Loss:  0.860, Acc:  0.609, Seconds: 9.70\n",
      "Epoch   2/5 Batch 5160/5926 - Loss:  0.900, Acc:  0.859, Seconds: 9.90\n",
      "Epoch   2/5 Batch 5180/5926 - Loss:  0.835, Acc:  0.688, Seconds: 16.42\n",
      "Epoch   2/5 Batch 5200/5926 - Loss:  0.863, Acc:  0.625, Seconds: 14.68\n",
      "Epoch   2/5 Batch 5220/5926 - Loss:  0.828, Acc:  0.547, Seconds: 13.62\n",
      "Epoch   2/5 Batch 5240/5926 - Loss:  0.873, Acc:  0.719, Seconds: 9.03\n",
      "Epoch   2/5 Batch 5260/5926 - Loss:  0.905, Acc:  0.531, Seconds: 12.91\n",
      "Epoch   2/5 Batch 5280/5926 - Loss:  0.882, Acc:  0.516, Seconds: 9.52\n",
      "Epoch   2/5 Batch 5300/5926 - Loss:  0.869, Acc:  0.656, Seconds: 11.84\n",
      "Epoch   2/5 Batch 5320/5926 - Loss:  0.891, Acc:  0.703, Seconds: 12.11\n",
      "Epoch   2/5 Batch 5340/5926 - Loss:  0.852, Acc:  0.734, Seconds: 15.43\n",
      "Epoch   2/5 Batch 5360/5926 - Loss:  0.859, Acc:  0.562, Seconds: 14.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/5 Batch 5380/5926 - Loss:  0.870, Acc:  0.688, Seconds: 16.48\n",
      "Epoch   2/5 Batch 5400/5926 - Loss:  0.871, Acc:  0.547, Seconds: 21.23\n",
      "Epoch   2/5 Batch 5420/5926 - Loss:  0.879, Acc:  0.609, Seconds: 9.02\n",
      "Epoch   2/5 Batch 5440/5926 - Loss:  0.865, Acc:  0.625, Seconds: 8.01\n",
      "Epoch   2/5 Batch 5460/5926 - Loss:  0.871, Acc:  0.625, Seconds: 21.12\n",
      "Epoch   2/5 Batch 5480/5926 - Loss:  0.866, Acc:  0.609, Seconds: 19.10\n",
      "Epoch   2/5 Batch 5500/5926 - Loss:  0.882, Acc:  0.656, Seconds: 14.84\n",
      "Epoch   2/5 Batch 5520/5926 - Loss:  0.850, Acc:  0.594, Seconds: 17.48\n",
      "Epoch   2/5 Batch 5540/5926 - Loss:  0.884, Acc:  0.594, Seconds: 17.86\n",
      "Average loss for this update: 0.872\n",
      "New Record!\n",
      "Epoch   2/5 Batch 5560/5926 - Loss:  0.912, Acc:  0.500, Seconds: 15.30\n",
      "Epoch   2/5 Batch 5580/5926 - Loss:  0.867, Acc:  0.641, Seconds: 12.54\n",
      "Epoch   2/5 Batch 5600/5926 - Loss:  0.854, Acc:  0.625, Seconds: 11.40\n",
      "Epoch   2/5 Batch 5620/5926 - Loss:  0.872, Acc:  0.500, Seconds: 8.01\n",
      "Epoch   2/5 Batch 5640/5926 - Loss:  0.894, Acc:  0.641, Seconds: 9.20\n",
      "Epoch   2/5 Batch 5660/5926 - Loss:  0.848, Acc:  0.625, Seconds: 16.54\n",
      "Epoch   2/5 Batch 5680/5926 - Loss:  0.878, Acc:  0.641, Seconds: 12.55\n",
      "Epoch   2/5 Batch 5700/5926 - Loss:  0.916, Acc:  0.453, Seconds: 20.39\n",
      "Epoch   2/5 Batch 5720/5926 - Loss:  0.897, Acc:  0.578, Seconds: 7.47\n",
      "Epoch   2/5 Batch 5740/5926 - Loss:  0.898, Acc:  0.703, Seconds: 16.04\n",
      "Epoch   2/5 Batch 5760/5926 - Loss:  0.892, Acc:  0.609, Seconds: 16.15\n",
      "Epoch   2/5 Batch 5780/5926 - Loss:  0.871, Acc:  0.625, Seconds: 16.23\n",
      "Epoch   2/5 Batch 5800/5926 - Loss:  0.862, Acc:  0.625, Seconds: 8.63\n",
      "Epoch   2/5 Batch 5820/5926 - Loss:  0.890, Acc:  0.625, Seconds: 12.54\n",
      "Epoch   2/5 Batch 5840/5926 - Loss:  0.873, Acc:  0.672, Seconds: 17.10\n",
      "Epoch   2/5 Batch 5860/5926 - Loss:  0.849, Acc:  0.578, Seconds: 12.03\n",
      "Epoch   2/5 Batch 5880/5926 - Loss:  0.890, Acc:  0.578, Seconds: 19.37\n",
      "Epoch   2/5 Batch 5900/5926 - Loss:  0.885, Acc:  0.594, Seconds: 13.60\n",
      "Epoch   2/5 Batch 5920/5926 - Loss:  0.857, Acc:  0.641, Seconds: 16.59\n",
      "Starting\n",
      "Finished first\n",
      "Epoch   3/5 Batch   20/5926 - Loss:  0.938, Acc:  0.641, Seconds: 11.81\n",
      "Epoch   3/5 Batch   40/5926 - Loss:  0.895, Acc:  0.531, Seconds: 11.12\n",
      "Epoch   3/5 Batch   60/5926 - Loss:  0.898, Acc:  0.672, Seconds: 7.96\n",
      "Epoch   3/5 Batch   80/5926 - Loss:  0.862, Acc:  0.656, Seconds: 11.60\n",
      "Epoch   3/5 Batch  100/5926 - Loss:  0.835, Acc:  0.656, Seconds: 14.72\n",
      "Epoch   3/5 Batch  120/5926 - Loss:  0.813, Acc:  0.656, Seconds: 21.38\n",
      "Epoch   3/5 Batch  140/5926 - Loss:  0.894, Acc:  0.703, Seconds: 19.77\n",
      "Epoch   3/5 Batch  160/5926 - Loss:  0.897, Acc:  0.594, Seconds: 15.37\n",
      "Epoch   3/5 Batch  180/5926 - Loss:  0.859, Acc:  0.656, Seconds: 21.81\n",
      "Epoch   3/5 Batch  200/5926 - Loss:  0.834, Acc:  0.672, Seconds: 13.37\n",
      "Epoch   3/5 Batch  220/5926 - Loss:  0.892, Acc:  0.672, Seconds: 12.48\n",
      "Epoch   3/5 Batch  240/5926 - Loss:  0.878, Acc:  0.531, Seconds: 8.00\n",
      "Epoch   3/5 Batch  260/5926 - Loss:  0.892, Acc:  0.594, Seconds: 17.88\n",
      "Epoch   3/5 Batch  280/5926 - Loss:  0.866, Acc:  0.562, Seconds: 10.29\n",
      "Epoch   3/5 Batch  300/5926 - Loss:  0.871, Acc:  0.609, Seconds: 15.79\n",
      "Epoch   3/5 Batch  320/5926 - Loss:  0.842, Acc:  0.562, Seconds: 15.69\n",
      "Epoch   3/5 Batch  340/5926 - Loss:  0.871, Acc:  0.688, Seconds: 13.52\n",
      "Epoch   3/5 Batch  360/5926 - Loss:  0.892, Acc:  0.594, Seconds: 16.41\n",
      "Epoch   3/5 Batch  380/5926 - Loss:  0.872, Acc:  0.641, Seconds: 19.64\n",
      "Epoch   3/5 Batch  400/5926 - Loss:  0.869, Acc:  0.625, Seconds: 18.13\n",
      "Epoch   3/5 Batch  420/5926 - Loss:  0.844, Acc:  0.625, Seconds: 9.52\n",
      "Epoch   3/5 Batch  440/5926 - Loss:  0.866, Acc:  0.609, Seconds: 11.46\n",
      "Epoch   3/5 Batch  460/5926 - Loss:  0.902, Acc:  0.531, Seconds: 7.69\n",
      "Epoch   3/5 Batch  480/5926 - Loss:  0.868, Acc:  0.656, Seconds: 9.41\n",
      "Epoch   3/5 Batch  500/5926 - Loss:  0.857, Acc:  0.531, Seconds: 19.17\n",
      "Epoch   3/5 Batch  520/5926 - Loss:  0.884, Acc:  0.469, Seconds: 15.56\n",
      "Epoch   3/5 Batch  540/5926 - Loss:  0.884, Acc:  0.562, Seconds: 10.07\n",
      "Epoch   3/5 Batch  560/5926 - Loss:  0.833, Acc:  0.594, Seconds: 18.73\n",
      "Epoch   3/5 Batch  580/5926 - Loss:  0.868, Acc:  0.672, Seconds: 12.38\n",
      "Epoch   3/5 Batch  600/5926 - Loss:  0.869, Acc:  0.578, Seconds: 15.15\n",
      "Epoch   3/5 Batch  620/5926 - Loss:  0.861, Acc:  0.703, Seconds: 9.59\n",
      "Epoch   3/5 Batch  640/5926 - Loss:  0.845, Acc:  0.656, Seconds: 8.42\n",
      "Epoch   3/5 Batch  660/5926 - Loss:  0.855, Acc:  0.531, Seconds: 13.83\n",
      "Epoch   3/5 Batch  680/5926 - Loss:  0.856, Acc:  0.562, Seconds: 11.13\n",
      "Epoch   3/5 Batch  700/5926 - Loss:  0.868, Acc:  0.594, Seconds: 13.12\n",
      "Epoch   3/5 Batch  720/5926 - Loss:  0.795, Acc:  0.688, Seconds: 9.34\n",
      "Epoch   3/5 Batch  740/5926 - Loss:  0.863, Acc:  0.547, Seconds: 14.34\n",
      "Epoch   3/5 Batch  760/5926 - Loss:  0.850, Acc:  0.672, Seconds: 22.16\n",
      "Epoch   3/5 Batch  780/5926 - Loss:  0.860, Acc:  0.609, Seconds: 11.23\n",
      "Epoch   3/5 Batch  800/5926 - Loss:  0.894, Acc:  0.516, Seconds: 11.16\n",
      "Epoch   3/5 Batch  820/5926 - Loss:  0.867, Acc:  0.578, Seconds: 18.24\n",
      "Epoch   3/5 Batch  840/5926 - Loss:  0.918, Acc:  0.516, Seconds: 7.73\n",
      "Epoch   3/5 Batch  860/5926 - Loss:  0.900, Acc:  0.703, Seconds: 11.26\n",
      "Epoch   3/5 Batch  880/5926 - Loss:  0.894, Acc:  0.578, Seconds: 18.90\n",
      "Epoch   3/5 Batch  900/5926 - Loss:  0.872, Acc:  0.531, Seconds: 10.57\n",
      "Epoch   3/5 Batch  920/5926 - Loss:  0.851, Acc:  0.656, Seconds: 19.42\n",
      "Average loss for this update: 0.869\n",
      "New Record!\n",
      "Epoch   3/5 Batch  940/5926 - Loss:  0.825, Acc:  0.625, Seconds: 17.46\n",
      "Epoch   3/5 Batch  960/5926 - Loss:  0.864, Acc:  0.656, Seconds: 16.46\n",
      "Epoch   3/5 Batch  980/5926 - Loss:  0.834, Acc:  0.609, Seconds: 11.76\n",
      "Epoch   3/5 Batch 1000/5926 - Loss:  0.849, Acc:  0.562, Seconds: 16.74\n",
      "Epoch   3/5 Batch 1020/5926 - Loss:  0.920, Acc:  0.594, Seconds: 8.01\n",
      "Epoch   3/5 Batch 1040/5926 - Loss:  0.848, Acc:  0.641, Seconds: 11.94\n",
      "Epoch   3/5 Batch 1060/5926 - Loss:  0.832, Acc:  0.594, Seconds: 14.59\n",
      "Epoch   3/5 Batch 1080/5926 - Loss:  0.853, Acc:  0.594, Seconds: 12.67\n",
      "Epoch   3/5 Batch 1100/5926 - Loss:  0.855, Acc:  0.609, Seconds: 13.71\n",
      "Epoch   3/5 Batch 1120/5926 - Loss:  0.893, Acc:  0.703, Seconds: 14.96\n",
      "Epoch   3/5 Batch 1140/5926 - Loss:  0.908, Acc:  0.531, Seconds: 17.40\n",
      "Epoch   3/5 Batch 1160/5926 - Loss:  0.838, Acc:  0.688, Seconds: 10.61\n",
      "Epoch   3/5 Batch 1180/5926 - Loss:  0.896, Acc:  0.688, Seconds: 10.82\n",
      "Epoch   3/5 Batch 1200/5926 - Loss:  0.887, Acc:  0.609, Seconds: 10.90\n",
      "Epoch   3/5 Batch 1220/5926 - Loss:  0.864, Acc:  0.531, Seconds: 19.28\n",
      "Epoch   3/5 Batch 1240/5926 - Loss:  0.859, Acc:  0.609, Seconds: 8.28\n",
      "Epoch   3/5 Batch 1260/5926 - Loss:  0.913, Acc:  0.641, Seconds: 17.41\n",
      "Epoch   3/5 Batch 1280/5926 - Loss:  0.866, Acc:  0.688, Seconds: 11.85\n",
      "Epoch   3/5 Batch 1300/5926 - Loss:  0.859, Acc:  0.516, Seconds: 9.21\n",
      "Epoch   3/5 Batch 1320/5926 - Loss:  0.845, Acc:  0.625, Seconds: 8.51\n",
      "Epoch   3/5 Batch 1340/5926 - Loss:  0.837, Acc:  0.625, Seconds: 11.55\n",
      "Epoch   3/5 Batch 1360/5926 - Loss:  0.857, Acc:  0.641, Seconds: 16.62\n",
      "Epoch   3/5 Batch 1380/5926 - Loss:  0.854, Acc:  0.672, Seconds: 11.91\n",
      "Epoch   3/5 Batch 1400/5926 - Loss:  0.840, Acc:  0.609, Seconds: 9.45\n",
      "Epoch   3/5 Batch 1420/5926 - Loss:  0.857, Acc:  0.516, Seconds: 9.80\n",
      "Epoch   3/5 Batch 1440/5926 - Loss:  0.826, Acc:  0.688, Seconds: 10.71\n",
      "Epoch   3/5 Batch 1460/5926 - Loss:  0.879, Acc:  0.672, Seconds: 15.15\n",
      "Epoch   3/5 Batch 1480/5926 - Loss:  0.892, Acc:  0.641, Seconds: 10.86\n",
      "Epoch   3/5 Batch 1500/5926 - Loss:  0.908, Acc:  0.562, Seconds: 16.58\n",
      "Epoch   3/5 Batch 1520/5926 - Loss:  0.869, Acc:  0.578, Seconds: 16.91\n",
      "Epoch   3/5 Batch 1540/5926 - Loss:  0.835, Acc:  0.625, Seconds: 11.69\n",
      "Epoch   3/5 Batch 1560/5926 - Loss:  0.822, Acc:  0.641, Seconds: 12.99\n",
      "Epoch   3/5 Batch 1580/5926 - Loss:  0.840, Acc:  0.656, Seconds: 19.55\n",
      "Epoch   3/5 Batch 1600/5926 - Loss:  0.833, Acc:  0.594, Seconds: 9.29\n",
      "Epoch   3/5 Batch 1620/5926 - Loss:  0.865, Acc:  0.734, Seconds: 20.59\n",
      "Epoch   3/5 Batch 1640/5926 - Loss:  0.837, Acc:  0.609, Seconds: 8.98\n",
      "Epoch   3/5 Batch 1660/5926 - Loss:  0.878, Acc:  0.656, Seconds: 22.10\n",
      "Epoch   3/5 Batch 1680/5926 - Loss:  0.852, Acc:  0.625, Seconds: 12.43\n",
      "Epoch   3/5 Batch 1700/5926 - Loss:  0.839, Acc:  0.688, Seconds: 10.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3/5 Batch 1720/5926 - Loss:  0.879, Acc:  0.609, Seconds: 9.90\n",
      "Epoch   3/5 Batch 1740/5926 - Loss:  0.854, Acc:  0.531, Seconds: 16.38\n",
      "Epoch   3/5 Batch 1760/5926 - Loss:  0.845, Acc:  0.641, Seconds: 18.67\n",
      "Epoch   3/5 Batch 1780/5926 - Loss:  0.881, Acc:  0.641, Seconds: 7.75\n",
      "Epoch   3/5 Batch 1800/5926 - Loss:  0.864, Acc:  0.641, Seconds: 12.84\n",
      "Epoch   3/5 Batch 1820/5926 - Loss:  0.866, Acc:  0.656, Seconds: 9.11\n",
      "Epoch   3/5 Batch 1840/5926 - Loss:  0.894, Acc:  0.562, Seconds: 15.06\n",
      "Average loss for this update: 0.861\n",
      "New Record!\n",
      "Epoch   3/5 Batch 1860/5926 - Loss:  0.873, Acc:  0.625, Seconds: 12.66\n",
      "Epoch   3/5 Batch 1880/5926 - Loss:  0.867, Acc:  0.688, Seconds: 15.14\n",
      "Epoch   3/5 Batch 1900/5926 - Loss:  0.847, Acc:  0.641, Seconds: 8.99\n",
      "Epoch   3/5 Batch 1920/5926 - Loss:  0.853, Acc:  0.656, Seconds: 15.23\n",
      "Epoch   3/5 Batch 1940/5926 - Loss:  0.884, Acc:  0.672, Seconds: 15.06\n",
      "Epoch   3/5 Batch 1960/5926 - Loss:  0.861, Acc:  0.609, Seconds: 19.19\n",
      "Epoch   3/5 Batch 1980/5926 - Loss:  0.874, Acc:  0.625, Seconds: 14.54\n",
      "Epoch   3/5 Batch 2000/5926 - Loss:  0.869, Acc:  0.516, Seconds: 10.16\n",
      "Epoch   3/5 Batch 2020/5926 - Loss:  0.891, Acc:  0.562, Seconds: 9.49\n",
      "Epoch   3/5 Batch 2040/5926 - Loss:  0.891, Acc:  0.703, Seconds: 11.06\n",
      "Epoch   3/5 Batch 2060/5926 - Loss:  0.863, Acc:  0.688, Seconds: 8.37\n",
      "Epoch   3/5 Batch 2080/5926 - Loss:  0.856, Acc:  0.625, Seconds: 7.71\n",
      "Epoch   3/5 Batch 2100/5926 - Loss:  0.854, Acc:  0.734, Seconds: 15.92\n",
      "Epoch   3/5 Batch 2120/5926 - Loss:  0.830, Acc:  0.609, Seconds: 13.42\n",
      "Epoch   3/5 Batch 2140/5926 - Loss:  0.827, Acc:  0.547, Seconds: 14.28\n",
      "Epoch   3/5 Batch 2160/5926 - Loss:  0.900, Acc:  0.609, Seconds: 14.46\n",
      "Epoch   3/5 Batch 2180/5926 - Loss:  0.800, Acc:  0.625, Seconds: 7.49\n",
      "Epoch   3/5 Batch 2200/5926 - Loss:  0.851, Acc:  0.594, Seconds: 20.74\n",
      "Epoch   3/5 Batch 2220/5926 - Loss:  0.875, Acc:  0.656, Seconds: 12.34\n",
      "Epoch   3/5 Batch 2240/5926 - Loss:  0.864, Acc:  0.531, Seconds: 8.43\n",
      "Epoch   3/5 Batch 2260/5926 - Loss:  0.875, Acc:  0.609, Seconds: 10.84\n",
      "Epoch   3/5 Batch 2280/5926 - Loss:  0.838, Acc:  0.531, Seconds: 21.65\n",
      "Epoch   3/5 Batch 2300/5926 - Loss:  0.850, Acc:  0.547, Seconds: 22.09\n",
      "Epoch   3/5 Batch 2320/5926 - Loss:  0.894, Acc:  0.594, Seconds: 16.81\n",
      "Epoch   3/5 Batch 2340/5926 - Loss:  0.824, Acc:  0.656, Seconds: 12.66\n",
      "Epoch   3/5 Batch 2360/5926 - Loss:  0.901, Acc:  0.656, Seconds: 8.35\n",
      "Epoch   3/5 Batch 2380/5926 - Loss:  0.851, Acc:  0.625, Seconds: 16.68\n",
      "Epoch   3/5 Batch 2400/5926 - Loss:  0.837, Acc:  0.625, Seconds: 12.38\n",
      "Epoch   3/5 Batch 2420/5926 - Loss:  0.841, Acc:  0.641, Seconds: 16.32\n",
      "Epoch   3/5 Batch 2440/5926 - Loss:  0.818, Acc:  0.641, Seconds: 13.64\n",
      "Epoch   3/5 Batch 2460/5926 - Loss:  0.871, Acc:  0.719, Seconds: 10.36\n",
      "Epoch   3/5 Batch 2480/5926 - Loss:  0.888, Acc:  0.609, Seconds: 12.30\n",
      "Epoch   3/5 Batch 2500/5926 - Loss:  0.869, Acc:  0.594, Seconds: 10.92\n",
      "Epoch   3/5 Batch 2520/5926 - Loss:  0.847, Acc:  0.594, Seconds: 11.15\n",
      "Epoch   3/5 Batch 2540/5926 - Loss:  0.868, Acc:  0.656, Seconds: 10.40\n",
      "Epoch   3/5 Batch 2560/5926 - Loss:  0.827, Acc:  0.625, Seconds: 14.16\n",
      "Epoch   3/5 Batch 2580/5926 - Loss:  0.889, Acc:  0.625, Seconds: 11.13\n",
      "Epoch   3/5 Batch 2600/5926 - Loss:  0.883, Acc:  0.578, Seconds: 13.52\n",
      "Epoch   3/5 Batch 2620/5926 - Loss:  0.816, Acc:  0.578, Seconds: 19.29\n",
      "Epoch   3/5 Batch 2640/5926 - Loss:  0.838, Acc:  0.641, Seconds: 6.51\n",
      "Epoch   3/5 Batch 2660/5926 - Loss:  0.849, Acc:  0.625, Seconds: 18.89\n",
      "Epoch   3/5 Batch 2680/5926 - Loss:  0.846, Acc:  0.609, Seconds: 14.07\n",
      "Epoch   3/5 Batch 2700/5926 - Loss:  0.838, Acc:  0.594, Seconds: 17.77\n",
      "Epoch   3/5 Batch 2720/5926 - Loss:  0.871, Acc:  0.641, Seconds: 14.18\n",
      "Epoch   3/5 Batch 2740/5926 - Loss:  0.888, Acc:  0.688, Seconds: 11.01\n",
      "Epoch   3/5 Batch 2760/5926 - Loss:  0.874, Acc:  0.656, Seconds: 10.04\n",
      "Average loss for this update: 0.859\n",
      "New Record!\n",
      "Epoch   3/5 Batch 2780/5926 - Loss:  0.881, Acc:  0.578, Seconds: 8.85\n",
      "Epoch   3/5 Batch 2800/5926 - Loss:  0.885, Acc:  0.656, Seconds: 9.11\n",
      "Epoch   3/5 Batch 2820/5926 - Loss:  0.875, Acc:  0.516, Seconds: 19.23\n",
      "Epoch   3/5 Batch 2840/5926 - Loss:  0.853, Acc:  0.672, Seconds: 18.99\n",
      "Epoch   3/5 Batch 2860/5926 - Loss:  0.867, Acc:  0.641, Seconds: 20.86\n",
      "Epoch   3/5 Batch 2880/5926 - Loss:  0.874, Acc:  0.641, Seconds: 13.76\n",
      "Epoch   3/5 Batch 2900/5926 - Loss:  0.822, Acc:  0.578, Seconds: 11.38\n",
      "Epoch   3/5 Batch 2920/5926 - Loss:  0.871, Acc:  0.484, Seconds: 6.25\n",
      "Epoch   3/5 Batch 2940/5926 - Loss:  0.847, Acc:  0.656, Seconds: 7.83\n",
      "Epoch   3/5 Batch 2960/5926 - Loss:  0.849, Acc:  0.625, Seconds: 11.42\n",
      "Epoch   3/5 Batch 2980/5926 - Loss:  0.870, Acc:  0.609, Seconds: 13.08\n",
      "Epoch   3/5 Batch 3000/5926 - Loss:  0.892, Acc:  0.594, Seconds: 7.49\n",
      "Epoch   3/5 Batch 3020/5926 - Loss:  0.850, Acc:  0.656, Seconds: 11.80\n",
      "Epoch   3/5 Batch 3040/5926 - Loss:  0.827, Acc:  0.672, Seconds: 13.58\n",
      "Epoch   3/5 Batch 3060/5926 - Loss:  0.831, Acc:  0.688, Seconds: 9.41\n",
      "Epoch   3/5 Batch 3080/5926 - Loss:  0.830, Acc:  0.625, Seconds: 9.85\n",
      "Epoch   3/5 Batch 3100/5926 - Loss:  0.871, Acc:  0.641, Seconds: 13.95\n",
      "Epoch   3/5 Batch 3120/5926 - Loss:  0.868, Acc:  0.641, Seconds: 9.03\n",
      "Epoch   3/5 Batch 3140/5926 - Loss:  0.875, Acc:  0.625, Seconds: 13.42\n",
      "Epoch   3/5 Batch 3160/5926 - Loss:  0.857, Acc:  0.703, Seconds: 17.19\n",
      "Epoch   3/5 Batch 3180/5926 - Loss:  0.842, Acc:  0.719, Seconds: 13.50\n",
      "Epoch   3/5 Batch 3200/5926 - Loss:  0.834, Acc:  0.578, Seconds: 9.89\n",
      "Epoch   3/5 Batch 3220/5926 - Loss:  0.875, Acc:  0.734, Seconds: 9.63\n",
      "Epoch   3/5 Batch 3240/5926 - Loss:  0.825, Acc:  0.766, Seconds: 15.94\n",
      "Epoch   3/5 Batch 3260/5926 - Loss:  0.860, Acc:  0.656, Seconds: 10.60\n",
      "Epoch   3/5 Batch 3280/5926 - Loss:  0.863, Acc:  0.562, Seconds: 8.55\n",
      "Epoch   3/5 Batch 3300/5926 - Loss:  0.905, Acc:  0.672, Seconds: 20.53\n",
      "Epoch   3/5 Batch 3320/5926 - Loss:  0.859, Acc:  0.578, Seconds: 11.60\n",
      "Epoch   3/5 Batch 3340/5926 - Loss:  0.854, Acc:  0.734, Seconds: 14.24\n",
      "Epoch   3/5 Batch 3360/5926 - Loss:  0.853, Acc:  0.656, Seconds: 10.20\n",
      "Epoch   3/5 Batch 3380/5926 - Loss:  0.858, Acc:  0.562, Seconds: 13.34\n",
      "Epoch   3/5 Batch 3400/5926 - Loss:  0.860, Acc:  0.625, Seconds: 11.00\n",
      "Epoch   3/5 Batch 3420/5926 - Loss:  0.842, Acc:  0.734, Seconds: 14.24\n",
      "Epoch   3/5 Batch 3440/5926 - Loss:  0.862, Acc:  0.641, Seconds: 12.26\n",
      "Epoch   3/5 Batch 3460/5926 - Loss:  0.862, Acc:  0.625, Seconds: 14.76\n",
      "Epoch   3/5 Batch 3480/5926 - Loss:  0.809, Acc:  0.578, Seconds: 18.93\n",
      "Epoch   3/5 Batch 3500/5926 - Loss:  0.834, Acc:  0.609, Seconds: 13.14\n",
      "Epoch   3/5 Batch 3520/5926 - Loss:  0.842, Acc:  0.734, Seconds: 12.96\n",
      "Epoch   3/5 Batch 3540/5926 - Loss:  0.875, Acc:  0.656, Seconds: 11.00\n",
      "Epoch   3/5 Batch 3560/5926 - Loss:  0.844, Acc:  0.609, Seconds: 15.02\n",
      "Epoch   3/5 Batch 3580/5926 - Loss:  0.847, Acc:  0.656, Seconds: 8.17\n",
      "Epoch   3/5 Batch 3600/5926 - Loss:  0.877, Acc:  0.672, Seconds: 12.90\n",
      "Epoch   3/5 Batch 3620/5926 - Loss:  0.872, Acc:  0.656, Seconds: 6.63\n",
      "Epoch   3/5 Batch 3640/5926 - Loss:  0.833, Acc:  0.688, Seconds: 13.48\n",
      "Epoch   3/5 Batch 3660/5926 - Loss:  0.824, Acc:  0.656, Seconds: 11.95\n",
      "Epoch   3/5 Batch 3680/5926 - Loss:  0.838, Acc:  0.734, Seconds: 12.42\n",
      "Epoch   3/5 Batch 3700/5926 - Loss:  0.851, Acc:  0.578, Seconds: 16.08\n",
      "Average loss for this update: 0.855\n",
      "New Record!\n",
      "Epoch   3/5 Batch 3720/5926 - Loss:  0.841, Acc:  0.594, Seconds: 17.41\n",
      "Epoch   3/5 Batch 3740/5926 - Loss:  0.865, Acc:  0.656, Seconds: 5.95\n",
      "Epoch   3/5 Batch 3760/5926 - Loss:  0.842, Acc:  0.547, Seconds: 13.80\n",
      "Epoch   3/5 Batch 3780/5926 - Loss:  0.886, Acc:  0.594, Seconds: 22.07\n",
      "Epoch   3/5 Batch 3800/5926 - Loss:  0.860, Acc:  0.641, Seconds: 15.22\n",
      "Epoch   3/5 Batch 3820/5926 - Loss:  0.862, Acc:  0.656, Seconds: 16.22\n",
      "Epoch   3/5 Batch 3840/5926 - Loss:  0.830, Acc:  0.656, Seconds: 10.64\n",
      "Epoch   3/5 Batch 3860/5926 - Loss:  0.896, Acc:  0.578, Seconds: 16.93\n",
      "Epoch   3/5 Batch 3880/5926 - Loss:  0.874, Acc:  0.656, Seconds: 15.36\n",
      "Epoch   3/5 Batch 3900/5926 - Loss:  0.858, Acc:  0.672, Seconds: 10.20\n",
      "Epoch   3/5 Batch 3920/5926 - Loss:  0.897, Acc:  0.562, Seconds: 16.18\n",
      "Epoch   3/5 Batch 3940/5926 - Loss:  0.899, Acc:  0.516, Seconds: 9.79\n",
      "Epoch   3/5 Batch 3960/5926 - Loss:  0.880, Acc:  0.547, Seconds: 9.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3/5 Batch 3980/5926 - Loss:  0.845, Acc:  0.641, Seconds: 12.10\n",
      "Epoch   3/5 Batch 4000/5926 - Loss:  0.879, Acc:  0.562, Seconds: 11.10\n",
      "Epoch   3/5 Batch 4020/5926 - Loss:  0.856, Acc:  0.516, Seconds: 17.55\n",
      "Epoch   3/5 Batch 4040/5926 - Loss:  0.849, Acc:  0.609, Seconds: 10.98\n",
      "Epoch   3/5 Batch 4060/5926 - Loss:  0.870, Acc:  0.641, Seconds: 8.97\n",
      "Epoch   3/5 Batch 4080/5926 - Loss:  0.842, Acc:  0.641, Seconds: 9.79\n",
      "Epoch   3/5 Batch 4100/5926 - Loss:  0.872, Acc:  0.516, Seconds: 15.68\n",
      "Epoch   3/5 Batch 4120/5926 - Loss:  0.848, Acc:  0.531, Seconds: 11.42\n",
      "Epoch   3/5 Batch 4140/5926 - Loss:  0.874, Acc:  0.672, Seconds: 14.44\n",
      "Epoch   3/5 Batch 4160/5926 - Loss:  0.894, Acc:  0.625, Seconds: 9.25\n",
      "Epoch   3/5 Batch 4180/5926 - Loss:  0.858, Acc:  0.578, Seconds: 8.83\n",
      "Epoch   3/5 Batch 4200/5926 - Loss:  0.863, Acc:  0.578, Seconds: 9.45\n",
      "Epoch   3/5 Batch 4220/5926 - Loss:  0.842, Acc:  0.688, Seconds: 20.95\n",
      "Epoch   3/5 Batch 4240/5926 - Loss:  0.815, Acc:  0.656, Seconds: 9.96\n",
      "Epoch   3/5 Batch 4260/5926 - Loss:  0.856, Acc:  0.688, Seconds: 19.25\n",
      "Epoch   3/5 Batch 4280/5926 - Loss:  0.848, Acc:  0.688, Seconds: 11.94\n",
      "Epoch   3/5 Batch 4300/5926 - Loss:  0.875, Acc:  0.656, Seconds: 13.42\n",
      "Epoch   3/5 Batch 4320/5926 - Loss:  0.862, Acc:  0.688, Seconds: 9.21\n",
      "Epoch   3/5 Batch 4340/5926 - Loss:  0.852, Acc:  0.672, Seconds: 10.78\n",
      "Epoch   3/5 Batch 4360/5926 - Loss:  0.833, Acc:  0.656, Seconds: 11.58\n",
      "Epoch   3/5 Batch 4380/5926 - Loss:  0.847, Acc:  0.609, Seconds: 9.63\n",
      "Epoch   3/5 Batch 4400/5926 - Loss:  0.849, Acc:  0.609, Seconds: 23.23\n",
      "Epoch   3/5 Batch 4420/5926 - Loss:  0.857, Acc:  0.641, Seconds: 13.82\n",
      "Epoch   3/5 Batch 4440/5926 - Loss:  0.865, Acc:  0.531, Seconds: 7.73\n",
      "Epoch   3/5 Batch 4460/5926 - Loss:  0.851, Acc:  0.641, Seconds: 13.24\n",
      "Epoch   3/5 Batch 4480/5926 - Loss:  0.825, Acc:  0.672, Seconds: 8.33\n",
      "Epoch   3/5 Batch 4500/5926 - Loss:  0.860, Acc:  0.688, Seconds: 17.17\n",
      "Epoch   3/5 Batch 4520/5926 - Loss:  0.818, Acc:  0.641, Seconds: 8.45\n",
      "Epoch   3/5 Batch 4540/5926 - Loss:  0.856, Acc:  0.609, Seconds: 11.66\n",
      "Epoch   3/5 Batch 4560/5926 - Loss:  0.914, Acc:  0.547, Seconds: 18.79\n",
      "Epoch   3/5 Batch 4580/5926 - Loss:  0.854, Acc:  0.672, Seconds: 13.34\n",
      "Epoch   3/5 Batch 4600/5926 - Loss:  0.839, Acc:  0.656, Seconds: 10.86\n",
      "Epoch   3/5 Batch 4620/5926 - Loss:  0.813, Acc:  0.672, Seconds: 11.58\n",
      "Average loss for this update: 0.858\n",
      "No Improvement.\n",
      "Epoch   3/5 Batch 4640/5926 - Loss:  0.881, Acc:  0.594, Seconds: 11.12\n",
      "Epoch   3/5 Batch 4660/5926 - Loss:  0.817, Acc:  0.703, Seconds: 7.39\n",
      "Epoch   3/5 Batch 4680/5926 - Loss:  0.857, Acc:  0.594, Seconds: 14.60\n",
      "Epoch   3/5 Batch 4700/5926 - Loss:  0.857, Acc:  0.641, Seconds: 12.06\n",
      "Epoch   3/5 Batch 4720/5926 - Loss:  0.877, Acc:  0.609, Seconds: 10.26\n",
      "Epoch   3/5 Batch 4740/5926 - Loss:  0.845, Acc:  0.672, Seconds: 14.12\n",
      "Epoch   3/5 Batch 4760/5926 - Loss:  0.867, Acc:  0.500, Seconds: 12.72\n",
      "Epoch   3/5 Batch 4780/5926 - Loss:  0.871, Acc:  0.719, Seconds: 17.18\n",
      "Epoch   3/5 Batch 4800/5926 - Loss:  0.864, Acc:  0.703, Seconds: 21.27\n",
      "Epoch   3/5 Batch 4820/5926 - Loss:  0.906, Acc:  0.656, Seconds: 12.88\n",
      "Epoch   3/5 Batch 4840/5926 - Loss:  0.853, Acc:  0.641, Seconds: 10.78\n",
      "Epoch   3/5 Batch 4860/5926 - Loss:  0.845, Acc:  0.484, Seconds: 9.93\n",
      "Epoch   3/5 Batch 4880/5926 - Loss:  0.846, Acc:  0.438, Seconds: 15.60\n",
      "Epoch   3/5 Batch 4900/5926 - Loss:  0.848, Acc:  0.578, Seconds: 14.78\n",
      "Epoch   3/5 Batch 4920/5926 - Loss:  0.866, Acc:  0.594, Seconds: 17.69\n",
      "Epoch   3/5 Batch 4940/5926 - Loss:  0.860, Acc:  0.734, Seconds: 9.85\n",
      "Epoch   3/5 Batch 4960/5926 - Loss:  0.879, Acc:  0.562, Seconds: 11.43\n",
      "Epoch   3/5 Batch 4980/5926 - Loss:  0.822, Acc:  0.719, Seconds: 13.54\n",
      "Epoch   3/5 Batch 5000/5926 - Loss:  0.853, Acc:  0.578, Seconds: 13.12\n",
      "Epoch   3/5 Batch 5020/5926 - Loss:  0.827, Acc:  0.578, Seconds: 10.90\n",
      "Epoch   3/5 Batch 5040/5926 - Loss:  0.853, Acc:  0.500, Seconds: 11.32\n",
      "Epoch   3/5 Batch 5060/5926 - Loss:  0.833, Acc:  0.625, Seconds: 16.85\n",
      "Epoch   3/5 Batch 5080/5926 - Loss:  0.880, Acc:  0.562, Seconds: 13.80\n",
      "Epoch   3/5 Batch 5100/5926 - Loss:  0.829, Acc:  0.703, Seconds: 11.18\n",
      "Epoch   3/5 Batch 5120/5926 - Loss:  0.886, Acc:  0.656, Seconds: 11.74\n",
      "Epoch   3/5 Batch 5140/5926 - Loss:  0.837, Acc:  0.578, Seconds: 8.01\n",
      "Epoch   3/5 Batch 5160/5926 - Loss:  0.882, Acc:  0.781, Seconds: 9.69\n",
      "Epoch   3/5 Batch 5180/5926 - Loss:  0.818, Acc:  0.656, Seconds: 15.70\n",
      "Epoch   3/5 Batch 5200/5926 - Loss:  0.840, Acc:  0.656, Seconds: 13.30\n",
      "Epoch   3/5 Batch 5220/5926 - Loss:  0.792, Acc:  0.625, Seconds: 12.72\n",
      "Epoch   3/5 Batch 5240/5926 - Loss:  0.842, Acc:  0.703, Seconds: 8.57\n",
      "Epoch   3/5 Batch 5260/5926 - Loss:  0.866, Acc:  0.516, Seconds: 11.68\n",
      "Epoch   3/5 Batch 5280/5926 - Loss:  0.883, Acc:  0.547, Seconds: 9.95\n",
      "Epoch   3/5 Batch 5300/5926 - Loss:  0.847, Acc:  0.625, Seconds: 11.10\n",
      "Epoch   3/5 Batch 5320/5926 - Loss:  0.846, Acc:  0.703, Seconds: 10.50\n",
      "Epoch   3/5 Batch 5340/5926 - Loss:  0.851, Acc:  0.688, Seconds: 16.54\n",
      "Epoch   3/5 Batch 5360/5926 - Loss:  0.834, Acc:  0.578, Seconds: 14.02\n",
      "Epoch   3/5 Batch 5380/5926 - Loss:  0.847, Acc:  0.656, Seconds: 14.92\n",
      "Epoch   3/5 Batch 5400/5926 - Loss:  0.856, Acc:  0.625, Seconds: 20.63\n",
      "Epoch   3/5 Batch 5420/5926 - Loss:  0.858, Acc:  0.672, Seconds: 8.59\n",
      "Epoch   3/5 Batch 5440/5926 - Loss:  0.870, Acc:  0.609, Seconds: 6.73\n",
      "Epoch   3/5 Batch 5460/5926 - Loss:  0.852, Acc:  0.672, Seconds: 20.59\n",
      "Epoch   3/5 Batch 5480/5926 - Loss:  0.845, Acc:  0.766, Seconds: 18.37\n",
      "Epoch   3/5 Batch 5500/5926 - Loss:  0.875, Acc:  0.672, Seconds: 13.80\n",
      "Epoch   3/5 Batch 5520/5926 - Loss:  0.826, Acc:  0.641, Seconds: 15.94\n",
      "Epoch   3/5 Batch 5540/5926 - Loss:  0.856, Acc:  0.625, Seconds: 15.34\n",
      "Average loss for this update: 0.853\n",
      "New Record!\n",
      "Epoch   3/5 Batch 5560/5926 - Loss:  0.876, Acc:  0.516, Seconds: 16.09\n",
      "Epoch   3/5 Batch 5580/5926 - Loss:  0.836, Acc:  0.594, Seconds: 12.42\n",
      "Epoch   3/5 Batch 5600/5926 - Loss:  0.839, Acc:  0.641, Seconds: 11.44\n",
      "Epoch   3/5 Batch 5620/5926 - Loss:  0.861, Acc:  0.609, Seconds: 7.53\n",
      "Epoch   3/5 Batch 5640/5926 - Loss:  0.864, Acc:  0.641, Seconds: 11.12\n",
      "Epoch   3/5 Batch 5660/5926 - Loss:  0.828, Acc:  0.562, Seconds: 12.76\n",
      "Epoch   3/5 Batch 5680/5926 - Loss:  0.867, Acc:  0.641, Seconds: 11.10\n",
      "Epoch   3/5 Batch 5700/5926 - Loss:  0.885, Acc:  0.484, Seconds: 20.57\n",
      "Epoch   3/5 Batch 5720/5926 - Loss:  0.874, Acc:  0.594, Seconds: 6.89\n",
      "Epoch   3/5 Batch 5740/5926 - Loss:  0.884, Acc:  0.688, Seconds: 17.35\n",
      "Epoch   3/5 Batch 5760/5926 - Loss:  0.864, Acc:  0.547, Seconds: 15.16\n",
      "Epoch   3/5 Batch 5780/5926 - Loss:  0.857, Acc:  0.641, Seconds: 14.18\n",
      "Epoch   3/5 Batch 5800/5926 - Loss:  0.857, Acc:  0.641, Seconds: 7.87\n",
      "Epoch   3/5 Batch 5820/5926 - Loss:  0.868, Acc:  0.578, Seconds: 12.04\n",
      "Epoch   3/5 Batch 5840/5926 - Loss:  0.855, Acc:  0.609, Seconds: 17.81\n",
      "Epoch   3/5 Batch 5860/5926 - Loss:  0.836, Acc:  0.594, Seconds: 12.18\n",
      "Epoch   3/5 Batch 5880/5926 - Loss:  0.872, Acc:  0.578, Seconds: 16.42\n",
      "Epoch   3/5 Batch 5900/5926 - Loss:  0.869, Acc:  0.609, Seconds: 12.84\n",
      "Epoch   3/5 Batch 5920/5926 - Loss:  0.852, Acc:  0.594, Seconds: 16.83\n",
      "Starting\n",
      "Finished first\n",
      "Epoch   4/5 Batch   20/5926 - Loss:  0.907, Acc:  0.625, Seconds: 10.90\n",
      "Epoch   4/5 Batch   40/5926 - Loss:  0.854, Acc:  0.562, Seconds: 8.55\n",
      "Epoch   4/5 Batch   60/5926 - Loss:  0.881, Acc:  0.609, Seconds: 7.19\n",
      "Epoch   4/5 Batch   80/5926 - Loss:  0.856, Acc:  0.688, Seconds: 12.02\n",
      "Epoch   4/5 Batch  100/5926 - Loss:  0.817, Acc:  0.734, Seconds: 14.57\n",
      "Epoch   4/5 Batch  120/5926 - Loss:  0.825, Acc:  0.672, Seconds: 20.19\n",
      "Epoch   4/5 Batch  140/5926 - Loss:  0.890, Acc:  0.641, Seconds: 15.32\n",
      "Epoch   4/5 Batch  160/5926 - Loss:  0.868, Acc:  0.594, Seconds: 15.16\n",
      "Epoch   4/5 Batch  180/5926 - Loss:  0.836, Acc:  0.641, Seconds: 17.55\n",
      "Epoch   4/5 Batch  200/5926 - Loss:  0.828, Acc:  0.672, Seconds: 14.56\n",
      "Epoch   4/5 Batch  220/5926 - Loss:  0.893, Acc:  0.656, Seconds: 11.96\n",
      "Epoch   4/5 Batch  240/5926 - Loss:  0.872, Acc:  0.531, Seconds: 7.57\n",
      "Epoch   4/5 Batch  260/5926 - Loss:  0.854, Acc:  0.672, Seconds: 17.13\n",
      "Epoch   4/5 Batch  280/5926 - Loss:  0.837, Acc:  0.625, Seconds: 7.63\n",
      "Epoch   4/5 Batch  300/5926 - Loss:  0.834, Acc:  0.625, Seconds: 15.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4/5 Batch  320/5926 - Loss:  0.840, Acc:  0.641, Seconds: 14.67\n",
      "Epoch   4/5 Batch  340/5926 - Loss:  0.885, Acc:  0.703, Seconds: 12.92\n",
      "Epoch   4/5 Batch  360/5926 - Loss:  0.871, Acc:  0.703, Seconds: 11.82\n",
      "Epoch   4/5 Batch  380/5926 - Loss:  0.866, Acc:  0.703, Seconds: 17.07\n",
      "Epoch   4/5 Batch  400/5926 - Loss:  0.840, Acc:  0.547, Seconds: 14.72\n",
      "Epoch   4/5 Batch  420/5926 - Loss:  0.828, Acc:  0.641, Seconds: 9.73\n",
      "Epoch   4/5 Batch  440/5926 - Loss:  0.875, Acc:  0.609, Seconds: 11.06\n",
      "Epoch   4/5 Batch  460/5926 - Loss:  0.877, Acc:  0.531, Seconds: 7.75\n",
      "Epoch   4/5 Batch  480/5926 - Loss:  0.844, Acc:  0.656, Seconds: 8.83\n",
      "Epoch   4/5 Batch  500/5926 - Loss:  0.853, Acc:  0.594, Seconds: 19.43\n",
      "Epoch   4/5 Batch  520/5926 - Loss:  0.869, Acc:  0.531, Seconds: 19.09\n",
      "Epoch   4/5 Batch  540/5926 - Loss:  0.851, Acc:  0.609, Seconds: 9.27\n",
      "Epoch   4/5 Batch  560/5926 - Loss:  0.812, Acc:  0.656, Seconds: 16.40\n",
      "Epoch   4/5 Batch  580/5926 - Loss:  0.835, Acc:  0.625, Seconds: 12.86\n",
      "Epoch   4/5 Batch  600/5926 - Loss:  0.851, Acc:  0.547, Seconds: 12.44\n",
      "Epoch   4/5 Batch  620/5926 - Loss:  0.850, Acc:  0.656, Seconds: 9.27\n",
      "Epoch   4/5 Batch  640/5926 - Loss:  0.837, Acc:  0.609, Seconds: 7.73\n",
      "Epoch   4/5 Batch  660/5926 - Loss:  0.840, Acc:  0.516, Seconds: 11.72\n",
      "Epoch   4/5 Batch  680/5926 - Loss:  0.846, Acc:  0.531, Seconds: 10.37\n",
      "Epoch   4/5 Batch  700/5926 - Loss:  0.856, Acc:  0.531, Seconds: 12.90\n",
      "Epoch   4/5 Batch  720/5926 - Loss:  0.790, Acc:  0.656, Seconds: 9.21\n",
      "Epoch   4/5 Batch  740/5926 - Loss:  0.856, Acc:  0.594, Seconds: 13.70\n",
      "Epoch   4/5 Batch  760/5926 - Loss:  0.826, Acc:  0.672, Seconds: 20.33\n",
      "Epoch   4/5 Batch  780/5926 - Loss:  0.827, Acc:  0.672, Seconds: 10.84\n",
      "Epoch   4/5 Batch  800/5926 - Loss:  0.885, Acc:  0.531, Seconds: 9.75\n",
      "Epoch   4/5 Batch  820/5926 - Loss:  0.860, Acc:  0.609, Seconds: 13.32\n",
      "Epoch   4/5 Batch  840/5926 - Loss:  0.890, Acc:  0.516, Seconds: 6.19\n",
      "Epoch   4/5 Batch  860/5926 - Loss:  0.888, Acc:  0.656, Seconds: 10.25\n",
      "Epoch   4/5 Batch  880/5926 - Loss:  0.868, Acc:  0.531, Seconds: 17.17\n",
      "Epoch   4/5 Batch  900/5926 - Loss:  0.869, Acc:  0.578, Seconds: 11.32\n",
      "Epoch   4/5 Batch  920/5926 - Loss:  0.826, Acc:  0.688, Seconds: 16.79\n",
      "Average loss for this update: 0.853\n",
      "No Improvement.\n",
      "Epoch   4/5 Batch  940/5926 - Loss:  0.824, Acc:  0.641, Seconds: 17.59\n",
      "Epoch   4/5 Batch  960/5926 - Loss:  0.849, Acc:  0.547, Seconds: 15.38\n",
      "Epoch   4/5 Batch  980/5926 - Loss:  0.826, Acc:  0.688, Seconds: 12.42\n",
      "Epoch   4/5 Batch 1000/5926 - Loss:  0.832, Acc:  0.703, Seconds: 12.52\n",
      "Epoch   4/5 Batch 1020/5926 - Loss:  0.896, Acc:  0.547, Seconds: 7.05\n",
      "Epoch   4/5 Batch 1040/5926 - Loss:  0.846, Acc:  0.594, Seconds: 14.18\n",
      "Epoch   4/5 Batch 1060/5926 - Loss:  0.837, Acc:  0.594, Seconds: 13.86\n",
      "Epoch   4/5 Batch 1080/5926 - Loss:  0.847, Acc:  0.609, Seconds: 10.72\n",
      "Epoch   4/5 Batch 1100/5926 - Loss:  0.838, Acc:  0.656, Seconds: 11.50\n",
      "Epoch   4/5 Batch 1120/5926 - Loss:  0.868, Acc:  0.688, Seconds: 14.22\n",
      "Epoch   4/5 Batch 1140/5926 - Loss:  0.884, Acc:  0.562, Seconds: 14.84\n",
      "Epoch   4/5 Batch 1160/5926 - Loss:  0.817, Acc:  0.656, Seconds: 9.87\n",
      "Epoch   4/5 Batch 1180/5926 - Loss:  0.872, Acc:  0.609, Seconds: 9.81\n",
      "Epoch   4/5 Batch 1200/5926 - Loss:  0.875, Acc:  0.625, Seconds: 10.58\n",
      "Epoch   4/5 Batch 1220/5926 - Loss:  0.841, Acc:  0.594, Seconds: 17.85\n",
      "Epoch   4/5 Batch 1240/5926 - Loss:  0.850, Acc:  0.562, Seconds: 6.59\n",
      "Epoch   4/5 Batch 1260/5926 - Loss:  0.899, Acc:  0.656, Seconds: 16.93\n",
      "Epoch   4/5 Batch 1280/5926 - Loss:  0.843, Acc:  0.672, Seconds: 11.90\n",
      "Epoch   4/5 Batch 1300/5926 - Loss:  0.835, Acc:  0.562, Seconds: 8.52\n",
      "Epoch   4/5 Batch 1320/5926 - Loss:  0.819, Acc:  0.641, Seconds: 8.25\n",
      "Epoch   4/5 Batch 1340/5926 - Loss:  0.824, Acc:  0.656, Seconds: 10.82\n",
      "Epoch   4/5 Batch 1360/5926 - Loss:  0.827, Acc:  0.656, Seconds: 13.90\n",
      "Epoch   4/5 Batch 1380/5926 - Loss:  0.836, Acc:  0.625, Seconds: 13.26\n",
      "Epoch   4/5 Batch 1400/5926 - Loss:  0.855, Acc:  0.641, Seconds: 10.71\n",
      "Epoch   4/5 Batch 1420/5926 - Loss:  0.858, Acc:  0.594, Seconds: 9.31\n",
      "Epoch   4/5 Batch 1440/5926 - Loss:  0.810, Acc:  0.578, Seconds: 9.63\n",
      "Epoch   4/5 Batch 1460/5926 - Loss:  0.875, Acc:  0.641, Seconds: 15.50\n",
      "Epoch   4/5 Batch 1480/5926 - Loss:  0.879, Acc:  0.594, Seconds: 10.63\n",
      "Epoch   4/5 Batch 1500/5926 - Loss:  0.891, Acc:  0.578, Seconds: 14.81\n",
      "Epoch   4/5 Batch 1520/5926 - Loss:  0.864, Acc:  0.641, Seconds: 12.96\n",
      "Epoch   4/5 Batch 1540/5926 - Loss:  0.826, Acc:  0.578, Seconds: 9.21\n",
      "Epoch   4/5 Batch 1560/5926 - Loss:  0.814, Acc:  0.594, Seconds: 9.79\n",
      "Epoch   4/5 Batch 1580/5926 - Loss:  0.825, Acc:  0.672, Seconds: 20.01\n",
      "Epoch   4/5 Batch 1600/5926 - Loss:  0.831, Acc:  0.641, Seconds: 7.31\n",
      "Epoch   4/5 Batch 1620/5926 - Loss:  0.855, Acc:  0.703, Seconds: 16.79\n",
      "Epoch   4/5 Batch 1640/5926 - Loss:  0.827, Acc:  0.766, Seconds: 7.69\n",
      "Epoch   4/5 Batch 1660/5926 - Loss:  0.862, Acc:  0.656, Seconds: 18.91\n",
      "Epoch   4/5 Batch 1680/5926 - Loss:  0.822, Acc:  0.609, Seconds: 12.00\n",
      "Epoch   4/5 Batch 1700/5926 - Loss:  0.796, Acc:  0.641, Seconds: 10.09\n",
      "Epoch   4/5 Batch 1720/5926 - Loss:  0.875, Acc:  0.609, Seconds: 10.04\n",
      "Epoch   4/5 Batch 1740/5926 - Loss:  0.836, Acc:  0.609, Seconds: 17.13\n",
      "Epoch   4/5 Batch 1760/5926 - Loss:  0.832, Acc:  0.625, Seconds: 18.59\n",
      "Epoch   4/5 Batch 1780/5926 - Loss:  0.871, Acc:  0.641, Seconds: 7.53\n",
      "Epoch   4/5 Batch 1800/5926 - Loss:  0.836, Acc:  0.719, Seconds: 13.84\n",
      "Epoch   4/5 Batch 1820/5926 - Loss:  0.830, Acc:  0.656, Seconds: 10.56\n",
      "Epoch   4/5 Batch 1840/5926 - Loss:  0.891, Acc:  0.656, Seconds: 14.88\n",
      "Average loss for this update: 0.847\n",
      "New Record!\n",
      "Epoch   4/5 Batch 1860/5926 - Loss:  0.872, Acc:  0.516, Seconds: 13.16\n",
      "Epoch   4/5 Batch 1880/5926 - Loss:  0.834, Acc:  0.688, Seconds: 15.16\n",
      "Epoch   4/5 Batch 1900/5926 - Loss:  0.834, Acc:  0.609, Seconds: 8.91\n",
      "Epoch   4/5 Batch 1920/5926 - Loss:  0.848, Acc:  0.719, Seconds: 14.00\n",
      "Epoch   4/5 Batch 1940/5926 - Loss:  0.852, Acc:  0.719, Seconds: 15.18\n",
      "Epoch   4/5 Batch 1960/5926 - Loss:  0.840, Acc:  0.672, Seconds: 20.15\n",
      "Epoch   4/5 Batch 1980/5926 - Loss:  0.864, Acc:  0.562, Seconds: 14.98\n",
      "Epoch   4/5 Batch 2000/5926 - Loss:  0.845, Acc:  0.516, Seconds: 11.28\n",
      "Epoch   4/5 Batch 2020/5926 - Loss:  0.871, Acc:  0.547, Seconds: 11.54\n",
      "Epoch   4/5 Batch 2040/5926 - Loss:  0.883, Acc:  0.672, Seconds: 11.26\n",
      "Epoch   4/5 Batch 2060/5926 - Loss:  0.846, Acc:  0.625, Seconds: 8.87\n",
      "Epoch   4/5 Batch 2080/5926 - Loss:  0.855, Acc:  0.578, Seconds: 8.05\n",
      "Epoch   4/5 Batch 2100/5926 - Loss:  0.862, Acc:  0.656, Seconds: 15.24\n",
      "Epoch   4/5 Batch 2120/5926 - Loss:  0.816, Acc:  0.656, Seconds: 13.38\n",
      "Epoch   4/5 Batch 2140/5926 - Loss:  0.811, Acc:  0.547, Seconds: 10.86\n",
      "Epoch   4/5 Batch 2160/5926 - Loss:  0.875, Acc:  0.594, Seconds: 15.50\n",
      "Epoch   4/5 Batch 2180/5926 - Loss:  0.794, Acc:  0.625, Seconds: 5.85\n",
      "Epoch   4/5 Batch 2200/5926 - Loss:  0.821, Acc:  0.688, Seconds: 20.35\n",
      "Epoch   4/5 Batch 2220/5926 - Loss:  0.871, Acc:  0.703, Seconds: 11.38\n",
      "Epoch   4/5 Batch 2240/5926 - Loss:  0.854, Acc:  0.562, Seconds: 8.23\n",
      "Epoch   4/5 Batch 2260/5926 - Loss:  0.849, Acc:  0.703, Seconds: 10.10\n",
      "Epoch   4/5 Batch 2280/5926 - Loss:  0.842, Acc:  0.578, Seconds: 20.25\n",
      "Epoch   4/5 Batch 2300/5926 - Loss:  0.841, Acc:  0.547, Seconds: 21.33\n",
      "Epoch   4/5 Batch 2320/5926 - Loss:  0.876, Acc:  0.594, Seconds: 16.34\n",
      "Epoch   4/5 Batch 2340/5926 - Loss:  0.811, Acc:  0.656, Seconds: 11.66\n",
      "Epoch   4/5 Batch 2360/5926 - Loss:  0.874, Acc:  0.578, Seconds: 8.99\n",
      "Epoch   4/5 Batch 2380/5926 - Loss:  0.858, Acc:  0.625, Seconds: 16.52\n",
      "Epoch   4/5 Batch 2400/5926 - Loss:  0.823, Acc:  0.656, Seconds: 12.60\n",
      "Epoch   4/5 Batch 2420/5926 - Loss:  0.827, Acc:  0.672, Seconds: 16.50\n",
      "Epoch   4/5 Batch 2440/5926 - Loss:  0.811, Acc:  0.703, Seconds: 14.04\n",
      "Epoch   4/5 Batch 2460/5926 - Loss:  0.864, Acc:  0.703, Seconds: 10.18\n",
      "Epoch   4/5 Batch 2480/5926 - Loss:  0.861, Acc:  0.625, Seconds: 13.86\n",
      "Epoch   4/5 Batch 2500/5926 - Loss:  0.855, Acc:  0.703, Seconds: 11.54\n",
      "Epoch   4/5 Batch 2520/5926 - Loss:  0.823, Acc:  0.656, Seconds: 9.45\n",
      "Epoch   4/5 Batch 2540/5926 - Loss:  0.843, Acc:  0.641, Seconds: 9.55\n",
      "Epoch   4/5 Batch 2560/5926 - Loss:  0.812, Acc:  0.672, Seconds: 13.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4/5 Batch 2580/5926 - Loss:  0.854, Acc:  0.641, Seconds: 11.66\n",
      "Epoch   4/5 Batch 2600/5926 - Loss:  0.876, Acc:  0.531, Seconds: 13.40\n",
      "Epoch   4/5 Batch 2620/5926 - Loss:  0.809, Acc:  0.562, Seconds: 18.47\n",
      "Epoch   4/5 Batch 2640/5926 - Loss:  0.837, Acc:  0.625, Seconds: 6.77\n",
      "Epoch   4/5 Batch 2660/5926 - Loss:  0.815, Acc:  0.672, Seconds: 17.49\n",
      "Epoch   4/5 Batch 2680/5926 - Loss:  0.818, Acc:  0.594, Seconds: 14.32\n",
      "Epoch   4/5 Batch 2700/5926 - Loss:  0.835, Acc:  0.641, Seconds: 19.29\n",
      "Epoch   4/5 Batch 2720/5926 - Loss:  0.847, Acc:  0.594, Seconds: 13.98\n",
      "Epoch   4/5 Batch 2740/5926 - Loss:  0.862, Acc:  0.672, Seconds: 10.76\n",
      "Epoch   4/5 Batch 2760/5926 - Loss:  0.846, Acc:  0.672, Seconds: 10.69\n",
      "Average loss for this update: 0.844\n",
      "New Record!\n",
      "Epoch   4/5 Batch 2780/5926 - Loss:  0.867, Acc:  0.578, Seconds: 8.49\n",
      "Epoch   4/5 Batch 2800/5926 - Loss:  0.866, Acc:  0.578, Seconds: 7.73\n",
      "Epoch   4/5 Batch 2820/5926 - Loss:  0.865, Acc:  0.531, Seconds: 19.39\n",
      "Epoch   4/5 Batch 2840/5926 - Loss:  0.834, Acc:  0.734, Seconds: 18.59\n",
      "Epoch   4/5 Batch 2860/5926 - Loss:  0.845, Acc:  0.672, Seconds: 21.35\n",
      "Epoch   4/5 Batch 2880/5926 - Loss:  0.866, Acc:  0.578, Seconds: 12.28\n",
      "Epoch   4/5 Batch 2900/5926 - Loss:  0.813, Acc:  0.641, Seconds: 10.33\n",
      "Epoch   4/5 Batch 2920/5926 - Loss:  0.846, Acc:  0.531, Seconds: 7.41\n",
      "Epoch   4/5 Batch 2940/5926 - Loss:  0.832, Acc:  0.656, Seconds: 7.81\n",
      "Epoch   4/5 Batch 2960/5926 - Loss:  0.833, Acc:  0.641, Seconds: 12.84\n",
      "Epoch   4/5 Batch 2980/5926 - Loss:  0.868, Acc:  0.641, Seconds: 13.54\n",
      "Epoch   4/5 Batch 3000/5926 - Loss:  0.883, Acc:  0.531, Seconds: 7.59\n",
      "Epoch   4/5 Batch 3020/5926 - Loss:  0.823, Acc:  0.688, Seconds: 8.03\n",
      "Epoch   4/5 Batch 3040/5926 - Loss:  0.834, Acc:  0.609, Seconds: 14.02\n",
      "Epoch   4/5 Batch 3060/5926 - Loss:  0.809, Acc:  0.719, Seconds: 9.61\n",
      "Epoch   4/5 Batch 3080/5926 - Loss:  0.817, Acc:  0.609, Seconds: 10.16\n",
      "Epoch   4/5 Batch 3100/5926 - Loss:  0.847, Acc:  0.734, Seconds: 13.80\n",
      "Epoch   4/5 Batch 3120/5926 - Loss:  0.859, Acc:  0.688, Seconds: 9.88\n",
      "Epoch   4/5 Batch 3140/5926 - Loss:  0.840, Acc:  0.609, Seconds: 13.38\n",
      "Epoch   4/5 Batch 3160/5926 - Loss:  0.842, Acc:  0.672, Seconds: 17.43\n",
      "Epoch   4/5 Batch 3180/5926 - Loss:  0.820, Acc:  0.719, Seconds: 12.72\n",
      "Epoch   4/5 Batch 3200/5926 - Loss:  0.802, Acc:  0.609, Seconds: 9.83\n",
      "Epoch   4/5 Batch 3220/5926 - Loss:  0.856, Acc:  0.703, Seconds: 9.27\n",
      "Epoch   4/5 Batch 3240/5926 - Loss:  0.819, Acc:  0.766, Seconds: 15.52\n",
      "Epoch   4/5 Batch 3260/5926 - Loss:  0.838, Acc:  0.609, Seconds: 10.56\n",
      "Epoch   4/5 Batch 3280/5926 - Loss:  0.851, Acc:  0.656, Seconds: 8.83\n",
      "Epoch   4/5 Batch 3300/5926 - Loss:  0.883, Acc:  0.641, Seconds: 19.99\n",
      "Epoch   4/5 Batch 3320/5926 - Loss:  0.834, Acc:  0.562, Seconds: 12.16\n",
      "Epoch   4/5 Batch 3340/5926 - Loss:  0.811, Acc:  0.672, Seconds: 14.14\n",
      "Epoch   4/5 Batch 3360/5926 - Loss:  0.814, Acc:  0.641, Seconds: 10.65\n",
      "Epoch   4/5 Batch 3380/5926 - Loss:  0.836, Acc:  0.625, Seconds: 15.50\n",
      "Epoch   4/5 Batch 3400/5926 - Loss:  0.838, Acc:  0.625, Seconds: 10.60\n",
      "Epoch   4/5 Batch 3420/5926 - Loss:  0.851, Acc:  0.703, Seconds: 16.10\n",
      "Epoch   4/5 Batch 3440/5926 - Loss:  0.857, Acc:  0.703, Seconds: 12.36\n",
      "Epoch   4/5 Batch 3460/5926 - Loss:  0.834, Acc:  0.625, Seconds: 12.22\n",
      "Epoch   4/5 Batch 3480/5926 - Loss:  0.794, Acc:  0.625, Seconds: 18.51\n",
      "Epoch   4/5 Batch 3500/5926 - Loss:  0.835, Acc:  0.609, Seconds: 13.22\n",
      "Epoch   4/5 Batch 3520/5926 - Loss:  0.822, Acc:  0.766, Seconds: 15.12\n",
      "Epoch   4/5 Batch 3540/5926 - Loss:  0.880, Acc:  0.578, Seconds: 11.76\n",
      "Epoch   4/5 Batch 3560/5926 - Loss:  0.832, Acc:  0.656, Seconds: 14.66\n",
      "Epoch   4/5 Batch 3580/5926 - Loss:  0.830, Acc:  0.609, Seconds: 8.97\n",
      "Epoch   4/5 Batch 3600/5926 - Loss:  0.860, Acc:  0.594, Seconds: 13.74\n",
      "Epoch   4/5 Batch 3620/5926 - Loss:  0.836, Acc:  0.656, Seconds: 7.17\n",
      "Epoch   4/5 Batch 3640/5926 - Loss:  0.830, Acc:  0.656, Seconds: 13.26\n",
      "Epoch   4/5 Batch 3660/5926 - Loss:  0.825, Acc:  0.672, Seconds: 12.78\n",
      "Epoch   4/5 Batch 3680/5926 - Loss:  0.818, Acc:  0.688, Seconds: 11.96\n",
      "Epoch   4/5 Batch 3700/5926 - Loss:  0.837, Acc:  0.625, Seconds: 15.34\n",
      "Average loss for this update: 0.839\n",
      "New Record!\n",
      "Epoch   4/5 Batch 3720/5926 - Loss:  0.836, Acc:  0.688, Seconds: 17.15\n",
      "Epoch   4/5 Batch 3740/5926 - Loss:  0.831, Acc:  0.625, Seconds: 6.21\n",
      "Epoch   4/5 Batch 3760/5926 - Loss:  0.831, Acc:  0.562, Seconds: 14.75\n",
      "Epoch   4/5 Batch 3780/5926 - Loss:  0.867, Acc:  0.562, Seconds: 20.33\n",
      "Epoch   4/5 Batch 3800/5926 - Loss:  0.844, Acc:  0.641, Seconds: 15.26\n",
      "Epoch   4/5 Batch 3820/5926 - Loss:  0.836, Acc:  0.703, Seconds: 16.02\n",
      "Epoch   4/5 Batch 3840/5926 - Loss:  0.828, Acc:  0.625, Seconds: 10.84\n",
      "Epoch   4/5 Batch 3860/5926 - Loss:  0.879, Acc:  0.625, Seconds: 13.86\n",
      "Epoch   4/5 Batch 3880/5926 - Loss:  0.841, Acc:  0.688, Seconds: 16.38\n",
      "Epoch   4/5 Batch 3900/5926 - Loss:  0.843, Acc:  0.719, Seconds: 10.26\n",
      "Epoch   4/5 Batch 3920/5926 - Loss:  0.871, Acc:  0.594, Seconds: 18.77\n",
      "Epoch   4/5 Batch 3940/5926 - Loss:  0.896, Acc:  0.562, Seconds: 10.88\n",
      "Epoch   4/5 Batch 3960/5926 - Loss:  0.871, Acc:  0.594, Seconds: 13.00\n",
      "Epoch   4/5 Batch 3980/5926 - Loss:  0.818, Acc:  0.672, Seconds: 11.54\n",
      "Epoch   4/5 Batch 4000/5926 - Loss:  0.858, Acc:  0.578, Seconds: 12.76\n",
      "Epoch   4/5 Batch 4020/5926 - Loss:  0.833, Acc:  0.547, Seconds: 16.28\n",
      "Epoch   4/5 Batch 4040/5926 - Loss:  0.837, Acc:  0.688, Seconds: 9.85\n",
      "Epoch   4/5 Batch 4060/5926 - Loss:  0.866, Acc:  0.719, Seconds: 9.35\n",
      "Epoch   4/5 Batch 4080/5926 - Loss:  0.831, Acc:  0.625, Seconds: 9.21\n",
      "Epoch   4/5 Batch 4100/5926 - Loss:  0.834, Acc:  0.516, Seconds: 15.10\n",
      "Epoch   4/5 Batch 4120/5926 - Loss:  0.838, Acc:  0.625, Seconds: 11.62\n",
      "Epoch   4/5 Batch 4140/5926 - Loss:  0.851, Acc:  0.672, Seconds: 15.20\n",
      "Epoch   4/5 Batch 4160/5926 - Loss:  0.863, Acc:  0.578, Seconds: 8.77\n",
      "Epoch   4/5 Batch 4180/5926 - Loss:  0.852, Acc:  0.562, Seconds: 7.99\n",
      "Epoch   4/5 Batch 4200/5926 - Loss:  0.831, Acc:  0.562, Seconds: 9.19\n",
      "Epoch   4/5 Batch 4220/5926 - Loss:  0.839, Acc:  0.656, Seconds: 21.71\n",
      "Epoch   4/5 Batch 4240/5926 - Loss:  0.812, Acc:  0.672, Seconds: 9.73\n",
      "Epoch   4/5 Batch 4260/5926 - Loss:  0.840, Acc:  0.703, Seconds: 21.49\n",
      "Epoch   4/5 Batch 4280/5926 - Loss:  0.834, Acc:  0.656, Seconds: 11.62\n",
      "Epoch   4/5 Batch 4300/5926 - Loss:  0.864, Acc:  0.656, Seconds: 10.90\n",
      "Epoch   4/5 Batch 4320/5926 - Loss:  0.841, Acc:  0.641, Seconds: 9.55\n",
      "Epoch   4/5 Batch 4340/5926 - Loss:  0.831, Acc:  0.609, Seconds: 11.62\n",
      "Epoch   4/5 Batch 4360/5926 - Loss:  0.827, Acc:  0.625, Seconds: 9.59\n",
      "Epoch   4/5 Batch 4380/5926 - Loss:  0.839, Acc:  0.578, Seconds: 9.17\n",
      "Epoch   4/5 Batch 4400/5926 - Loss:  0.825, Acc:  0.609, Seconds: 19.51\n",
      "Epoch   4/5 Batch 4420/5926 - Loss:  0.851, Acc:  0.594, Seconds: 11.22\n",
      "Epoch   4/5 Batch 4440/5926 - Loss:  0.818, Acc:  0.641, Seconds: 7.65\n",
      "Epoch   4/5 Batch 4460/5926 - Loss:  0.838, Acc:  0.594, Seconds: 14.10\n",
      "Epoch   4/5 Batch 4480/5926 - Loss:  0.816, Acc:  0.672, Seconds: 7.51\n",
      "Epoch   4/5 Batch 4500/5926 - Loss:  0.835, Acc:  0.734, Seconds: 16.02\n",
      "Epoch   4/5 Batch 4520/5926 - Loss:  0.793, Acc:  0.625, Seconds: 9.05\n",
      "Epoch   4/5 Batch 4540/5926 - Loss:  0.847, Acc:  0.625, Seconds: 11.14\n",
      "Epoch   4/5 Batch 4560/5926 - Loss:  0.903, Acc:  0.594, Seconds: 19.59\n",
      "Epoch   4/5 Batch 4580/5926 - Loss:  0.841, Acc:  0.703, Seconds: 15.00\n",
      "Epoch   4/5 Batch 4600/5926 - Loss:  0.821, Acc:  0.656, Seconds: 10.02\n",
      "Epoch   4/5 Batch 4620/5926 - Loss:  0.806, Acc:  0.781, Seconds: 10.88\n",
      "Average loss for this update: 0.842\n",
      "No Improvement.\n",
      "Epoch   4/5 Batch 4640/5926 - Loss:  0.873, Acc:  0.594, Seconds: 11.16\n",
      "Epoch   4/5 Batch 4660/5926 - Loss:  0.813, Acc:  0.719, Seconds: 7.81\n",
      "Epoch   4/5 Batch 4680/5926 - Loss:  0.855, Acc:  0.594, Seconds: 17.33\n",
      "Epoch   4/5 Batch 4700/5926 - Loss:  0.852, Acc:  0.672, Seconds: 11.22\n",
      "Epoch   4/5 Batch 4720/5926 - Loss:  0.873, Acc:  0.578, Seconds: 13.96\n",
      "Epoch   4/5 Batch 4740/5926 - Loss:  0.813, Acc:  0.672, Seconds: 14.28\n",
      "Epoch   4/5 Batch 4760/5926 - Loss:  0.868, Acc:  0.594, Seconds: 12.76\n",
      "Epoch   4/5 Batch 4780/5926 - Loss:  0.866, Acc:  0.656, Seconds: 16.12\n",
      "Epoch   4/5 Batch 4800/5926 - Loss:  0.858, Acc:  0.672, Seconds: 20.79\n",
      "Epoch   4/5 Batch 4820/5926 - Loss:  0.889, Acc:  0.609, Seconds: 13.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4/5 Batch 4840/5926 - Loss:  0.841, Acc:  0.609, Seconds: 10.68\n",
      "Epoch   4/5 Batch 4860/5926 - Loss:  0.853, Acc:  0.562, Seconds: 9.91\n",
      "Epoch   4/5 Batch 4880/5926 - Loss:  0.830, Acc:  0.609, Seconds: 10.38\n",
      "Epoch   4/5 Batch 4900/5926 - Loss:  0.834, Acc:  0.656, Seconds: 14.52\n",
      "Epoch   4/5 Batch 4920/5926 - Loss:  0.857, Acc:  0.594, Seconds: 24.06\n",
      "Epoch   4/5 Batch 4940/5926 - Loss:  0.849, Acc:  0.703, Seconds: 9.73\n",
      "Epoch   4/5 Batch 4960/5926 - Loss:  0.872, Acc:  0.594, Seconds: 11.64\n",
      "Epoch   4/5 Batch 4980/5926 - Loss:  0.814, Acc:  0.766, Seconds: 13.84\n",
      "Epoch   4/5 Batch 5000/5926 - Loss:  0.825, Acc:  0.547, Seconds: 11.70\n",
      "Epoch   4/5 Batch 5020/5926 - Loss:  0.827, Acc:  0.578, Seconds: 10.81\n",
      "Epoch   4/5 Batch 5040/5926 - Loss:  0.835, Acc:  0.562, Seconds: 10.11\n",
      "Epoch   4/5 Batch 5060/5926 - Loss:  0.807, Acc:  0.609, Seconds: 16.40\n",
      "Epoch   4/5 Batch 5080/5926 - Loss:  0.868, Acc:  0.516, Seconds: 14.16\n",
      "Epoch   4/5 Batch 5100/5926 - Loss:  0.822, Acc:  0.766, Seconds: 12.08\n",
      "Epoch   4/5 Batch 5120/5926 - Loss:  0.860, Acc:  0.625, Seconds: 11.21\n",
      "Epoch   4/5 Batch 5140/5926 - Loss:  0.825, Acc:  0.578, Seconds: 8.75\n",
      "Epoch   4/5 Batch 5160/5926 - Loss:  0.868, Acc:  0.812, Seconds: 9.61\n",
      "Epoch   4/5 Batch 5180/5926 - Loss:  0.792, Acc:  0.672, Seconds: 14.74\n",
      "Epoch   4/5 Batch 5200/5926 - Loss:  0.816, Acc:  0.656, Seconds: 14.76\n",
      "Epoch   4/5 Batch 5220/5926 - Loss:  0.799, Acc:  0.594, Seconds: 14.58\n",
      "Epoch   4/5 Batch 5240/5926 - Loss:  0.835, Acc:  0.797, Seconds: 8.51\n",
      "Epoch   4/5 Batch 5260/5926 - Loss:  0.862, Acc:  0.531, Seconds: 11.20\n",
      "Epoch   4/5 Batch 5280/5926 - Loss:  0.867, Acc:  0.594, Seconds: 9.77\n",
      "Epoch   4/5 Batch 5300/5926 - Loss:  0.838, Acc:  0.625, Seconds: 11.44\n",
      "Epoch   4/5 Batch 5320/5926 - Loss:  0.828, Acc:  0.688, Seconds: 11.06\n",
      "Epoch   4/5 Batch 5340/5926 - Loss:  0.817, Acc:  0.734, Seconds: 14.80\n",
      "Epoch   4/5 Batch 5360/5926 - Loss:  0.810, Acc:  0.547, Seconds: 13.72\n",
      "Epoch   4/5 Batch 5380/5926 - Loss:  0.844, Acc:  0.594, Seconds: 15.60\n",
      "Epoch   4/5 Batch 5400/5926 - Loss:  0.865, Acc:  0.516, Seconds: 20.93\n",
      "Epoch   4/5 Batch 5420/5926 - Loss:  0.846, Acc:  0.625, Seconds: 9.75\n",
      "Epoch   4/5 Batch 5440/5926 - Loss:  0.843, Acc:  0.625, Seconds: 6.49\n",
      "Epoch   4/5 Batch 5460/5926 - Loss:  0.841, Acc:  0.578, Seconds: 19.91\n",
      "Epoch   4/5 Batch 5480/5926 - Loss:  0.846, Acc:  0.688, Seconds: 22.17\n",
      "Epoch   4/5 Batch 5500/5926 - Loss:  0.857, Acc:  0.641, Seconds: 13.70\n",
      "Epoch   4/5 Batch 5520/5926 - Loss:  0.809, Acc:  0.562, Seconds: 15.85\n",
      "Epoch   4/5 Batch 5540/5926 - Loss:  0.820, Acc:  0.656, Seconds: 13.60\n",
      "Average loss for this update: 0.84\n",
      "No Improvement.\n",
      "Epoch   4/5 Batch 5560/5926 - Loss:  0.856, Acc:  0.562, Seconds: 16.40\n",
      "Epoch   4/5 Batch 5580/5926 - Loss:  0.820, Acc:  0.641, Seconds: 12.14\n",
      "Epoch   4/5 Batch 5600/5926 - Loss:  0.834, Acc:  0.547, Seconds: 11.48\n",
      "Epoch   4/5 Batch 5620/5926 - Loss:  0.845, Acc:  0.531, Seconds: 7.15\n",
      "Epoch   4/5 Batch 5640/5926 - Loss:  0.868, Acc:  0.672, Seconds: 8.71\n",
      "Epoch   4/5 Batch 5660/5926 - Loss:  0.818, Acc:  0.625, Seconds: 12.16\n",
      "Epoch   4/5 Batch 5680/5926 - Loss:  0.861, Acc:  0.547, Seconds: 10.32\n",
      "Epoch   4/5 Batch 5700/5926 - Loss:  0.874, Acc:  0.547, Seconds: 21.61\n",
      "Epoch   4/5 Batch 5720/5926 - Loss:  0.866, Acc:  0.609, Seconds: 6.79\n",
      "Epoch   4/5 Batch 5740/5926 - Loss:  0.881, Acc:  0.625, Seconds: 16.60\n",
      "Epoch   4/5 Batch 5760/5926 - Loss:  0.848, Acc:  0.578, Seconds: 15.50\n",
      "Epoch   4/5 Batch 5780/5926 - Loss:  0.829, Acc:  0.641, Seconds: 14.24\n",
      "Epoch   4/5 Batch 5800/5926 - Loss:  0.831, Acc:  0.625, Seconds: 8.63\n",
      "Epoch   4/5 Batch 5820/5926 - Loss:  0.870, Acc:  0.547, Seconds: 12.04\n",
      "Epoch   4/5 Batch 5840/5926 - Loss:  0.846, Acc:  0.594, Seconds: 20.03\n",
      "Epoch   4/5 Batch 5860/5926 - Loss:  0.825, Acc:  0.578, Seconds: 12.08\n",
      "Epoch   4/5 Batch 5880/5926 - Loss:  0.861, Acc:  0.625, Seconds: 15.04\n",
      "Epoch   4/5 Batch 5900/5926 - Loss:  0.846, Acc:  0.719, Seconds: 11.82\n",
      "Epoch   4/5 Batch 5920/5926 - Loss:  0.845, Acc:  0.547, Seconds: 20.75\n",
      "Starting\n",
      "Finished first\n",
      "Epoch   5/5 Batch   20/5926 - Loss:  0.904, Acc:  0.641, Seconds: 11.47\n",
      "Epoch   5/5 Batch   40/5926 - Loss:  0.853, Acc:  0.562, Seconds: 8.75\n",
      "Epoch   5/5 Batch   60/5926 - Loss:  0.843, Acc:  0.656, Seconds: 8.05\n",
      "Epoch   5/5 Batch   80/5926 - Loss:  0.832, Acc:  0.656, Seconds: 9.98\n",
      "Epoch   5/5 Batch  100/5926 - Loss:  0.803, Acc:  0.625, Seconds: 14.62\n",
      "Epoch   5/5 Batch  120/5926 - Loss:  0.800, Acc:  0.609, Seconds: 22.29\n",
      "Epoch   5/5 Batch  140/5926 - Loss:  0.845, Acc:  0.750, Seconds: 15.43\n",
      "Epoch   5/5 Batch  160/5926 - Loss:  0.848, Acc:  0.609, Seconds: 15.06\n",
      "Epoch   5/5 Batch  180/5926 - Loss:  0.831, Acc:  0.578, Seconds: 17.67\n",
      "Epoch   5/5 Batch  200/5926 - Loss:  0.822, Acc:  0.641, Seconds: 13.26\n",
      "Epoch   5/5 Batch  220/5926 - Loss:  0.874, Acc:  0.625, Seconds: 12.38\n",
      "Epoch   5/5 Batch  240/5926 - Loss:  0.842, Acc:  0.594, Seconds: 7.37\n",
      "Epoch   5/5 Batch  260/5926 - Loss:  0.848, Acc:  0.578, Seconds: 19.19\n",
      "Epoch   5/5 Batch  280/5926 - Loss:  0.827, Acc:  0.641, Seconds: 7.37\n",
      "Epoch   5/5 Batch  300/5926 - Loss:  0.832, Acc:  0.594, Seconds: 16.14\n",
      "Epoch   5/5 Batch  320/5926 - Loss:  0.819, Acc:  0.578, Seconds: 14.16\n",
      "Epoch   5/5 Batch  340/5926 - Loss:  0.852, Acc:  0.656, Seconds: 12.34\n",
      "Epoch   5/5 Batch  360/5926 - Loss:  0.865, Acc:  0.656, Seconds: 12.50\n",
      "Epoch   5/5 Batch  380/5926 - Loss:  0.862, Acc:  0.703, Seconds: 17.01\n",
      "Epoch   5/5 Batch  400/5926 - Loss:  0.840, Acc:  0.656, Seconds: 15.34\n",
      "Epoch   5/5 Batch  420/5926 - Loss:  0.831, Acc:  0.625, Seconds: 9.33\n",
      "Epoch   5/5 Batch  440/5926 - Loss:  0.834, Acc:  0.609, Seconds: 11.28\n",
      "Epoch   5/5 Batch  460/5926 - Loss:  0.854, Acc:  0.641, Seconds: 7.51\n",
      "Epoch   5/5 Batch  480/5926 - Loss:  0.845, Acc:  0.625, Seconds: 8.91\n",
      "Epoch   5/5 Batch  500/5926 - Loss:  0.837, Acc:  0.625, Seconds: 19.21\n",
      "Epoch   5/5 Batch  520/5926 - Loss:  0.865, Acc:  0.609, Seconds: 16.93\n",
      "Epoch   5/5 Batch  540/5926 - Loss:  0.859, Acc:  0.547, Seconds: 9.25\n",
      "Epoch   5/5 Batch  560/5926 - Loss:  0.797, Acc:  0.594, Seconds: 17.47\n",
      "Epoch   5/5 Batch  580/5926 - Loss:  0.832, Acc:  0.609, Seconds: 11.34\n",
      "Epoch   5/5 Batch  600/5926 - Loss:  0.813, Acc:  0.594, Seconds: 11.78\n",
      "Epoch   5/5 Batch  620/5926 - Loss:  0.835, Acc:  0.672, Seconds: 8.73\n",
      "Epoch   5/5 Batch  640/5926 - Loss:  0.823, Acc:  0.594, Seconds: 7.55\n",
      "Epoch   5/5 Batch  660/5926 - Loss:  0.822, Acc:  0.531, Seconds: 11.88\n",
      "Epoch   5/5 Batch  680/5926 - Loss:  0.846, Acc:  0.594, Seconds: 10.46\n",
      "Epoch   5/5 Batch  700/5926 - Loss:  0.828, Acc:  0.531, Seconds: 12.46\n",
      "Epoch   5/5 Batch  720/5926 - Loss:  0.771, Acc:  0.672, Seconds: 8.41\n",
      "Epoch   5/5 Batch  740/5926 - Loss:  0.844, Acc:  0.625, Seconds: 14.16\n",
      "Epoch   5/5 Batch  760/5926 - Loss:  0.821, Acc:  0.672, Seconds: 20.77\n",
      "Epoch   5/5 Batch  780/5926 - Loss:  0.817, Acc:  0.688, Seconds: 10.58\n",
      "Epoch   5/5 Batch  800/5926 - Loss:  0.858, Acc:  0.578, Seconds: 10.20\n",
      "Epoch   5/5 Batch  820/5926 - Loss:  0.844, Acc:  0.578, Seconds: 14.02\n",
      "Epoch   5/5 Batch  840/5926 - Loss:  0.890, Acc:  0.531, Seconds: 6.15\n",
      "Epoch   5/5 Batch  860/5926 - Loss:  0.883, Acc:  0.625, Seconds: 10.98\n",
      "Epoch   5/5 Batch  880/5926 - Loss:  0.860, Acc:  0.641, Seconds: 16.30\n",
      "Epoch   5/5 Batch  900/5926 - Loss:  0.870, Acc:  0.625, Seconds: 10.66\n",
      "Epoch   5/5 Batch  920/5926 - Loss:  0.826, Acc:  0.672, Seconds: 18.81\n",
      "Average loss for this update: 0.84\n",
      "No Improvement.\n",
      "Stopping Training.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"./saves/best_model.ckpt\" \n",
    "if load:\n",
    "    loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "if IS_TRAINING:\n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        if load:\n",
    "            loader.restore(sess, checkpoint)\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('./summaries' + '/train', sess.graph)\n",
    "\n",
    "        for epoch_i in range(1, epochs+1):\n",
    "            state = sess.run(graph.initial_state)\n",
    "\n",
    "            update_loss = 0\n",
    "            batch_loss = 0\n",
    "\n",
    "            for batch_i, (x, y) in enumerate(get_batches(X_train, y_train, batch_size)):\n",
    "                if batch_i == 1:\n",
    "                    print(\"Starting\")\n",
    "                feed = {graph.input_data: x,\n",
    "                        graph.labels: y,\n",
    "                        graph.keep_prob: keep_probability,\n",
    "                        graph.initial_state: state, \n",
    "                        graph.lr: learning_rate}\n",
    "                start_time = time.time()\n",
    "                summary, loss, acc, state, _ = sess.run([graph.merged, \n",
    "                                                         graph.cost, \n",
    "                                                         graph.accuracy, \n",
    "                                                         graph.final_state, \n",
    "                                                         graph.optimizer], \n",
    "                                                        feed_dict=feed)\n",
    "                if batch_i == 1:\n",
    "                    print(\"Finished first\")\n",
    "\n",
    "                train_writer.add_summary(summary, epoch_i*batch_i + batch_i)\n",
    "\n",
    "                batch_loss += loss\n",
    "                update_loss += loss\n",
    "                end_time = time.time()\n",
    "                batch_time = end_time - start_time\n",
    "\n",
    "                if batch_i % display_step == 0 and batch_i > 0:\n",
    "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Acc: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(epoch_i,\n",
    "                                  epochs, \n",
    "                                  batch_i, \n",
    "                                  len(X_train) // batch_size, \n",
    "                                  batch_loss / display_step,\n",
    "                                  acc,\n",
    "                                  batch_time*display_step))\n",
    "                    batch_loss = 0\n",
    "\n",
    "                if batch_i % update_check == 0 and batch_i > 0:\n",
    "                    print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                    summary_update_loss.append(update_loss)\n",
    "\n",
    "                    # If the update loss is at a new minimum, save the model\n",
    "                    if update_loss <= min(summary_update_loss):\n",
    "                        print('New Record!') \n",
    "                        stop_early = 0\n",
    "                        saver = tf.train.Saver() \n",
    "                        saver.save(sess, checkpoint)\n",
    "\n",
    "                    else:\n",
    "                        print(\"No Improvement.\")\n",
    "                        stop_early += 1\n",
    "                        if stop_early == stop:\n",
    "                            break\n",
    "                    update_loss = 0\n",
    "\n",
    "\n",
    "            # Reduce learning rate, but not below its minimum value\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "            # Set shuffle to True if you want to shuffle data between epochs\n",
    "            # This can add some randomness and potentially learn new patterns in data\n",
    "#             if shuffle:\n",
    "#                 X_train, y_train = shuffle_data(X_train, y_train)\n",
    "            if stop_early == stop:\n",
    "                print(\"Stopping Training.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Data\n",
    "This part of the code is allocated to testing the data. On top of recording accuracy results, I also generate a confusion matrix. Since reviews are subjective and aren't concretely one rating or another, a confusion matrix helps visualize your results a lot better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_TESTING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saves/best_model.ckpt\n",
      "Total Batches: 1481\n",
      "20 batches\n",
      "40 batches\n",
      "60 batches\n",
      "80 batches\n",
      "100 batches\n",
      "120 batches\n",
      "140 batches\n",
      "160 batches\n",
      "180 batches\n",
      "200 batches\n",
      "220 batches\n",
      "240 batches\n",
      "260 batches\n",
      "280 batches\n",
      "300 batches\n",
      "320 batches\n",
      "340 batches\n",
      "360 batches\n",
      "380 batches\n",
      "400 batches\n",
      "420 batches\n",
      "440 batches\n",
      "460 batches\n",
      "480 batches\n",
      "500 batches\n",
      "520 batches\n",
      "540 batches\n",
      "560 batches\n",
      "580 batches\n",
      "600 batches\n",
      "620 batches\n",
      "640 batches\n",
      "660 batches\n",
      "680 batches\n",
      "700 batches\n",
      "720 batches\n",
      "740 batches\n",
      "760 batches\n",
      "780 batches\n",
      "800 batches\n",
      "820 batches\n",
      "840 batches\n",
      "860 batches\n",
      "880 batches\n",
      "900 batches\n",
      "920 batches\n",
      "940 batches\n",
      "960 batches\n",
      "980 batches\n",
      "1000 batches\n",
      "1020 batches\n",
      "1040 batches\n",
      "1060 batches\n",
      "1080 batches\n",
      "1100 batches\n",
      "1120 batches\n",
      "1140 batches\n",
      "1160 batches\n",
      "1180 batches\n",
      "1200 batches\n",
      "1220 batches\n",
      "1240 batches\n",
      "1260 batches\n",
      "1280 batches\n",
      "1300 batches\n",
      "1320 batches\n",
      "1340 batches\n",
      "1360 batches\n",
      "1380 batches\n",
      "1400 batches\n",
      "1420 batches\n",
      "1440 batches\n",
      "1460 batches\n",
      "1480 batches\n"
     ]
    }
   ],
   "source": [
    "if IS_TESTING:\n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        checkpoint = \"./saves/best_model.ckpt\"  \n",
    "\n",
    "        all_preds = []\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            # Load the model\n",
    "            saver.restore(sess, checkpoint)\n",
    "            test_state = sess.run(graph.initial_state)\n",
    "            print(\"Total Batches: %d\"%(len(X_test)//batch_size))\n",
    "            for ii, x in enumerate(get_test_batches(X_test, batch_size), 1):\n",
    "                if ii%20==0:\n",
    "                    print(\"%d batches\"%(ii))\n",
    "                feed = {graph.input_data: x,\n",
    "                        graph.keep_prob: keep_probability,\n",
    "                        graph.initial_state: state}\n",
    "\n",
    "                predictions = sess.run(graph.predictions, feed_dict=feed)\n",
    "                for i in range(len(predictions)):\n",
    "                    all_preds.append(predictions[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94784, 6)\n",
      "0.619281735313977\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x219482f5668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21949ce3f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_preds = np.array(all_preds)\n",
    "print(all_preds.shape)\n",
    "y_predictions = all_preds.argmax(axis=1)\n",
    "y_true = y_test.argmax(axis=1)\n",
    "y_true = y_true[:len(y_predictions)]\n",
    "\n",
    "cm = ConfusionMatrix(y_true, y_predictions)\n",
    "cm.plot(backend='seaborn', normalized=True)\n",
    "plt.title('Confusion Matrix Stars prediction')\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "test_correctPred = np.equal(y_predictions, y_true)\n",
    "test_accuracy = np.mean(test_correctPred.astype(float))\n",
    "\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out the network yourself!\n",
    "User can enter a review here and see how the network does in predicting his or her review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a review in englishI love this place! Will definitely come back!\n",
      "INFO:tensorflow:Restoring parameters from ./saves/best_model.ckpt\n",
      "\n",
      "You rated the restaurant: 5 stars!\n"
     ]
    }
   ],
   "source": [
    "load_files = True\n",
    "if load_files == True:\n",
    "    word_embedding_matrix = loadfiles(\"./data/pickles/word_embedding_matrix.p\")\n",
    "    word2int = loadfiles('./data/pickles/word2int.p')\n",
    "\n",
    "pred_text = input(\"Please enter a review in english\")\n",
    "contractions = get_contractions()\n",
    "pred_text = clean_text(pred_text)\n",
    "pred_seq = convert_to_ints(pred_text, pred=True)\n",
    "pred_seq = np.tile(pred_seq, (batch_size, 1))\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    checkpoint = \"./saves/best_model.ckpt\"  \n",
    "    all_preds = []\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        # Load the model\n",
    "        saver.restore(sess, checkpoint)\n",
    "        test_state = sess.run(graph.initial_state)\n",
    "        feed = {graph.input_data: pred_seq,\n",
    "                graph.keep_prob: keep_probability,\n",
    "                graph.initial_state: state}\n",
    "\n",
    "        predictions = sess.run(graph.predictions, feed_dict=feed)\n",
    "        for i in range(len(predictions)):\n",
    "            all_preds.append(predictions[i,:])\n",
    "all_preds = np.array(all_preds)\n",
    "y_predictions = all_preds.argmax(axis=1)\n",
    "counts = np.bincount(y_predictions)\n",
    "print(\"\\nYou rated the restaurant: \" + str(np.argmax(counts)) + \" stars!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf15",
   "language": "python",
   "name": "tf15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
