{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from langdetect import detect\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import random\n",
    "import nltk, re, time\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from collections import namedtuple\n",
    "\n",
    "from contractions import get_contractions\n",
    "import operator\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "alreadyPickled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "num_layers = 2\n",
    "num_classes = 6\n",
    "epochs = 100\n",
    "batch_size = 5\n",
    "rnn_size = 128\n",
    "num_layers = 2\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "keep_probability = 0.75\n",
    "max_sequence_length = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    assert isinstance(text, str)\n",
    "    text = text.lower()    \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text,  \n",
    "                  flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done getting Business IDs\n",
      "There are 65028 restaurants\n"
     ]
    }
   ],
   "source": [
    "restId = []\n",
    "for line in open('./data/dataset/business.json', 'r'):\n",
    "    data = json.loads(line)\n",
    "    if 'Restaurants' in data['categories'] or 'Food' in data['categories']:\n",
    "        restId.append(data['business_id'])\n",
    "print(\"Done getting Business IDs\")\n",
    "print(\"There are %d restaurants\" % (len(restId)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "contractions = get_contractions()\n",
    "\n",
    "revs_list = [[]]\n",
    "stars_list = [[]]\n",
    "num = 5000 # Number of review read\n",
    "k = 0 # Count\n",
    "for line in open('./data/dataset/review.json', 'r'):\n",
    "    if k >= num:\n",
    "        break\n",
    "    data = json.loads(line)\n",
    "    text = data['text']\n",
    "    star = data['stars']\n",
    "    ID = data['business_id']\n",
    "    # Check language\n",
    "    if detect(text) != 'en':\n",
    "        continue  \n",
    "    # Check whether it is a restaurant\n",
    "    if ID not in restId:\n",
    "        continue\n",
    "    revs_list.append(clean_text(text))\n",
    "    stars_list.append(star)\n",
    "    k += 1\n",
    "    # Notify for every 500 reviews\n",
    "    if len(revs_list) % 500 == 0:\n",
    "        print(len(revs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "place horrible excited try since got gift card birthday went ordered whole meal except gift card system unacceptable would helpful would known prior\n",
      "5001 5001\n"
     ]
    }
   ],
   "source": [
    "print(revs_list[1])\n",
    "print(len(revs_list), len(stars_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Ziiped Revs is [5001, 2]\n"
     ]
    }
   ],
   "source": [
    "zipped_revs = zip(revs_list, stars_list)\n",
    "print(\"Shape of Ziiped Revs is [%d, %d]\" % (len(zipped_revs), len(zipped_revs[0])))\n",
    "categories = ['text', 'stars']\n",
    "df_reviews_processing = pd.DataFrame(zipped_revs, columns=categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Dropping Nones: Shape 5000,2\n",
      "After Dropping Nones: Shape 5000,2\n"
     ]
    }
   ],
   "source": [
    "df_reviews_processing.shape\n",
    "df_reviews_processing.head()\n",
    "df_reviews_processing[['stars']] = df_reviews_processing[['stars']].apply(pd.to_numeric)\n",
    "\n",
    "df_reviews_processing = df_reviews_processing[np.isfinite(df_reviews_processing['stars'])]\n",
    "\n",
    "print(\"Before Dropping Nones: Shape %d,%d\" % (df_reviews_processing.shape[0], df_reviews_processing.shape[1]))\n",
    "df_reviews = df_reviews_processing[np.isfinite(df_reviews_processing['stars'])]\n",
    "print(\"After Dropping Nones: Shape %d,%d\" % (df_reviews_processing.shape[0], df_reviews_processing.shape[1]))\n",
    "\n",
    "df_reviews_processing.to_csv(\"reviews_df_processed.csv\", encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "If the data is already pickled, then can skip embedding and data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picklefiles(filename, stuff):\n",
    "    save_stuff = open(filename, \"wb\")\n",
    "    pickle.dump(stuff, save_stuff)\n",
    "    save_stuff.close()\n",
    "def loadfiles(filename):\n",
    "    saved_stuff = open(filename,\"rb\")\n",
    "    stuff = pickle.load(saved_stuff)\n",
    "    saved_stuff.close()\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "Using ConceptNet Numberbatch instead of GLoVE (supposedly outperforms GLoVE embeddings)  \n",
    "  \n",
    "  \n",
    "On top of the embeddings, we also keep track of commonly used words in the reviews that Embeddings don't cover. This way we could have higher test accuracy when words we come across words like these. This is specified by a threshold value. Currently, threshold is set to 20 occuraces.  \n",
    "  \n",
    "  \n",
    "We also process the reviews a bit more, sorting them into comparable lengths. This way, there is less padding necessary and (possibly) faster computation time when training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 15259\n"
     ]
    }
   ],
   "source": [
    "word_counts = {}\n",
    "count_words(word_counts, df_reviews.text)            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_path='./embeddings/numberbatch-en-17.02.txt'\n",
    "def load_embeddings(path='./embeddings/numberbatch-en-17.02.txt'):\n",
    "    embeddings_index = {}\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            embedding = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = embedding\n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = load_embeddings(embed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance the Dataset\n",
    "Want to balance the dataset, such that we have an equal number of reviews for each different category.  \n",
    "For example, if our distribution of reviews is [200,500,100,300,400], for [1,2,3,4,5] stars, respectively, then we will only take 100 of each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataframe(df, category=['stars'], downsample_by=0.9):\n",
    "    \"\"\"\n",
    "    :param df: pandas.DataFrame\n",
    "    :param categorical_columns: iterable of categorical columns names contained in {df}\n",
    "    :return: balanced pandas.DataFrame\n",
    "    \"\"\"\n",
    "    if category is None or not all([col in df.columns for col in category]):\n",
    "        raise ValueError('Please provide one or more columns containing categorical variables')\n",
    "\n",
    "    lowest_count = df.groupby(category).apply(lambda x: x.shape[0]).min()\n",
    "    # Remove 10 percent of the reviews\n",
    "    lowest_count = int(lowest_count * downsample_by)\n",
    "    \n",
    "    df = df.groupby(category).apply( \n",
    "        lambda x: x.sample(lowest_count)).drop(category, axis=1).reset_index().set_index('level_1')\n",
    "\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pre) 1 star ratings: 469\n",
      "(pre) 2 star ratings: 545\n",
      "(pre) 3 star ratings: 759\n",
      "(pre) 4 star ratings: 1353\n",
      "(pre) 5 star ratings: 1874\n",
      "(post) 1 star ratings: 422\n",
      "(post) 2 star ratings: 422\n",
      "(post) 3 star ratings: 422\n",
      "(post) 4 star ratings: 422\n",
      "(post) 5 star ratings: 422\n"
     ]
    }
   ],
   "source": [
    "# df_reviews = pd.read_csv('reviewsText.csv')\n",
    "df_reviews = pd.read_csv(\"reviews_df_processed.csv\")\n",
    "df_reviews['len'] = df_reviews.text.str.len()\n",
    "df_reviews = df_reviews[df_reviews['len'].between(10, 4000)]\n",
    "\n",
    "\n",
    "print(\"(pre) 1 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 1])))\n",
    "print(\"(pre) 2 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 2])))\n",
    "print(\"(pre) 3 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 3])))\n",
    "print(\"(pre) 4 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 4])))\n",
    "print(\"(pre) 5 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 5])))\n",
    "\n",
    "# Set downsample_by to 0.9, since not much training data to begin with\n",
    "df_balanced = balance_dataframe(df_reviews, \n",
    "                                category=['stars'], \n",
    "                                downsample_by=0.9)\n",
    "\n",
    "df_balanced.to_csv('balanced_reviews1000.csv', encoding='utf-8')\n",
    "print(\"(post) 1 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 1])))\n",
    "print(\"(post) 2 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 2])))\n",
    "print(\"(post) 3 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 3])))\n",
    "print(\"(post) 4 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 4])))\n",
    "print(\"(post) 5 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 5])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(1 in df_reviews['stars'])\n",
    "print(2 in df_reviews['stars'])\n",
    "print(3 in df_reviews['stars'])\n",
    "print(4 in df_reviews['stars'])\n",
    "print(5 in df_reviews['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words is 23\n",
      "embedding matrix size: is [23,300]\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "joblib.dump(tokenizer, 'tokenizer.p')\n",
    "\n",
    "WORD_INDEX_SORTED = sorted(tokenizer.word_index.items(), key=operator.itemgetter(1))\n",
    "print(\"number of words is %d\" % (len(WORD_INDEX_SORTED)))\n",
    "\n",
    "vocab_size = len(WORD_INDEX_SORTED)+1\n",
    "\n",
    "NB_WORDS = min(len(WORD_INDEX_SORTED), MAX_NB_WORDS)\n",
    "\n",
    "word_embedding_matrix = np.zeros((NB_WORDS, embedding_dim), dtype=np.float32)\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i-1] = embedding_vector\n",
    "print(\"embedding matrix size: is [%d,%d]\" % (word_embedding_matrix.shape[0], \n",
    "                                            word_embedding_matrix.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = tokenizer.texts_to_sequences(df_balanced.text.values)\n",
    "\n",
    "padReviews = pad_sequences(seqs, maxlen=max_sequence_length)\n",
    "\n",
    "reviewsLength = max_sequence_length\n",
    "\n",
    "ratings = df_balanced.stars.values.astype(int)\n",
    "ratings_cat = to_categorical(ratings)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padReviews, ratings_cat, test_size=0.2, random_state=9)\n",
    "with pd.HDFStore('x_y_test_train.h5') as h:\n",
    "    h['X_train'] = pd.DataFrame(X_train)\n",
    "    h['X_test'] = pd.DataFrame(X_test)\n",
    "    h['y_train'] = pd.DataFrame(y_train)\n",
    "    h['y_test'] = pd.DataFrame(y_test)\n",
    "\n",
    "assert padReviews.shape[0] == ratings.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(2110, 6)\n",
      "(1688, 1000)\n",
      "(1688, 6)\n"
     ]
    }
   ],
   "source": [
    "print(type(ratings_cat))\n",
    "print(ratings_cat.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "if alreadyPickled == False:\n",
    "    picklefiles(\"./data/good_pickles/balanced_reviews.p\",df_balanced)\n",
    "    picklefiles(\"./data/good_pickles/category_ratings.p\",ratings_cat)\n",
    "    picklefiles(\"./data/good_pickles/word_embedding_matrix.p\",word_embedding_matrix)\n",
    "    \n",
    "if alreadyPickled == True:\n",
    "\n",
    "    word_embedding_matrix = loadfiles(\"./data/word_embedding_matrix.p\")\n",
    "    df_rev_balanced = pd.read_csv('balanced_reviews.csv')\n",
    "    \n",
    "    ratings_cat = loadfiles(\"./data/good_pickles/category_ratings.p\")\n",
    "    balanced_reviews = loadfiles(\"./data/good_pickles/balanced_reviews.p\")\n",
    "    \n",
    "    tokenizer = joblib.load('tokenizer.pickle')\n",
    "    with pd.HDFStore('x_y_test_train.h5') as h:\n",
    "        X_train = h['X_train'].values\n",
    "        X_test = h['X_test'].values\n",
    "        y_train = h['y_train'].values\n",
    "        y_test = h['y_test'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gettin Batches\n",
    "Gets batches as well as pads them to have similar length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(ratings, reviews, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(reviews)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        rating_batch = ratings[start_i:start_i + batch_size, :]\n",
    "        reviews_batch = reviews[start_i:start_i + batch_size, :]\n",
    "\n",
    "        yield rating_batch, reviews_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1000)\n",
      "(5, 6)\n",
      "(5, 1000)\n",
      "(5, 6)\n",
      "(5, 1000)\n",
      "(5, 6)\n",
      "(5, 1000)\n",
      "(5, 6)\n",
      "(5, 1000)\n",
      "(5, 6)\n",
      "(5, 1000)\n",
      "(5, 6)\n"
     ]
    }
   ],
   "source": [
    "for batch_i, (trainX_batch, trainY_batch) in enumerate(get_batches(X_train, y_train, batch_size)):\n",
    "    print(trainX_batch.shape)\n",
    "    print(trainY_batch.shape)\n",
    "    if batch_i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "\n",
    "    # Should be [batch_size x review length]\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    # Should be [batch_size x num_classes]\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    \n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    return input_data, labels, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input Data\n",
      "Tensor(\"Shape:0\", shape=(2,), dtype=int32)\n",
      "Graph is built.\n",
      "./graph\n"
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    input_data, labels, lr, keep_prob = model_inputs()\n",
    "    \n",
    "    print(\"Shape of Input Data\")\n",
    "    print(tf.shape(input_data))\n",
    "    embeddings = word_embedding_matrix\n",
    "    embs = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "\n",
    "    W2 = tf.Variable(np.random.rand(rnn_size, num_classes),dtype=tf.float32)\n",
    "    b2 = tf.Variable(np.zeros((1, num_classes)), dtype=tf.float32)\n",
    "\n",
    "\n",
    "    with tf.name_scope(\"RNN_Layers\"):\n",
    "        stacked_rnn = []\n",
    "        for layer in range(num_layers):\n",
    "            cell_fw = tf.contrib.rnn.GRUCell(rnn_size)\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw,\n",
    "                                                    output_keep_prob=keep_prob)\n",
    "            stacked_rnn.append(cell_fw)\n",
    "        multilayer_cell = tf.contrib.rnn.MultiRNNCell(stacked_rnn, state_is_tuple=True)\n",
    "\n",
    "    with tf.name_scope(\"init_state\"):\n",
    "        initial_state = multilayer_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    with tf.name_scope(\"Forward_Pass\"):\n",
    "        output, states_series = tf.nn.dynamic_rnn(multilayer_cell,\n",
    "                                           embs,\n",
    "                                           dtype=tf.float32)\n",
    "\n",
    "    states_s = tf.reshape(states_series, [-1, rnn_size])\n",
    "\n",
    "    logits = tf.matmul(states_s, W2) + b2 #Broadcasted addition\n",
    "    # labels = tf.reshape(labels, [-1])\n",
    "\n",
    "    logits_series = tf.unstack(tf.reshape(logits, [batch_size, num_classes]), axis=1)\n",
    "    predictions_series = [tf.nn.softmax(logit) for logit in logits_series]\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "#         dense_labels = tf.sparse_to_dense(labels)\n",
    "        losses = tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=labels)\n",
    "\n",
    "        total_loss = tf.reduce_mean(losses)\n",
    "        tf.summary.scalar(\"cost\", total_loss)\n",
    "        \n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Optimizer\n",
    "        train_step = tf.train.RMSPropOptimizer(learning_rate).minimize(total_loss)\n",
    "#         gradients = optimizer.compute_gradients(total_loss)\n",
    "#         capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "#         train_op = optimizer.apply_gradients(capped_gradients)\n",
    "\n",
    "\n",
    "print(\"Graph is built.\")\n",
    "graph_location = \"./graph\"\n",
    "print(graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(train_graph)\n",
    "    # with tf.name_scope(\"FC_Layer\"):\n",
    "    #     dense = tf.contrib.layers.fully_connected(output[:,-1],\n",
    "    #                                               num_outputs = 64,\n",
    "    #                                               activation_fn = tf.sigmoid,\n",
    "    #                                               weights_initializer = W2,\n",
    "    #                                               biases_initializer = b2)\n",
    "    #     dense = tf.contrib.layers.dropout(dense, keep_prob)\n",
    "\n",
    "    # with tf.name_scope(\"Predictions\"):\n",
    "    #     predictions = tf.contrib.layers.fully_connected(dense,\n",
    "    #                                                     num_outputs = num_classes, \n",
    "    #                                                     activation_fn=tf.nn.relu,\n",
    "    #                                                     weights_initializer = W2,\n",
    "    #                                                     biases_initializer = b2)\n",
    "\n",
    "    # states_series_reshaped = tf.reshape(states_series, [-1, rnn_size])\n",
    "\n",
    "    # logits = tf.matmul(states_series_reshaped, W2) + b2 #Broadcasted addition\n",
    "    # labels_logits = tf.reshape(labels, [-1])\n",
    "\n",
    "\n",
    "    # logits_series = tf.unstack(tf.reshape(logits, [batch_size, num_classes]), axis=1)\n",
    "    # predictions_series = [tf.nn.softmax(logit) for logit in logits_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape_13:0\", shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.shape(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape_14:0\", shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.shape(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(X_train)//batch_size//per_epoch)-1\n",
    "keep_probability = 0.75\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  0  0 23]\n",
      " [ 0  0  0 ...,  0  0  0]]\n",
      "[[ 0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[3,999] = 23 is not in [0, 23)\n\t [[Node: embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@embedding_lookup/params_0\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_lookup/params_0, _arg_input_0_0)]]\n\nCaused by op u'embedding_lookup', defined at:\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-277-b5407318cc2d>\", line 9, in <module>\n    embs = tf.nn.embedding_lookup(embeddings, input_data)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/embedding_ops.py\", line 328, in embedding_lookup\n    transform_fn=None)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/embedding_ops.py\", line 150, in _embedding_lookup_and_transform\n    result = _clip(_gather(params[0], ids, name=name), ids, max_norm)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/embedding_ops.py\", line 54, in _gather\n    return array_ops.gather(params, ids, name=name)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 2486, in gather\n    params, indices, validate_indices=validate_indices, name=name)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1834, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): indices[3,999] = 23 is not in [0, 23)\n\t [[Node: embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@embedding_lookup/params_0\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_lookup/params_0, _arg_input_0_0)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-278-e6e6c0408695>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                                   \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrainY_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                   \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                                   keep_prob: keep_probability})\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[3,999] = 23 is not in [0, 23)\n\t [[Node: embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@embedding_lookup/params_0\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_lookup/params_0, _arg_input_0_0)]]\n\nCaused by op u'embedding_lookup', defined at:\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-277-b5407318cc2d>\", line 9, in <module>\n    embs = tf.nn.embedding_lookup(embeddings, input_data)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/embedding_ops.py\", line 328, in embedding_lookup\n    transform_fn=None)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/embedding_ops.py\", line 150, in _embedding_lookup_and_transform\n    result = _clip(_gather(params[0], ids, name=name), ids, max_norm)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/embedding_ops.py\", line 54, in _gather\n    return array_ops.gather(params, ids, name=name)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 2486, in gather\n    params, indices, validate_indices=validate_indices, name=name)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1834, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/Users/thomasan/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): indices[3,999] = 23 is not in [0, 23)\n\t [[Node: embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@embedding_lookup/params_0\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_lookup/params_0, _arg_input_0_0)]]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"./saves/best_model.ckpt\" \n",
    "logs_path = \"./logs/\"\n",
    "summary_ops = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ###\n",
    "    # If we want to continue training a previous session\n",
    "    ###\n",
    "#     loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "#     loader.restore(sess, checkpoint)\n",
    "\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (trainX_batch, trainY_batch) in enumerate(get_batches(X_train, y_train, batch_size)):\n",
    "            if batch_i < 10:\n",
    "                print(trainX_batch)\n",
    "                print(trainY_batch)\n",
    "            start_time = time.time()\n",
    "            _, loss= sess.run([train_step, total_loss],\n",
    "                              feed_dict={\n",
    "                                  input_data: trainX_batch,\n",
    "                                  labels: trainY_batch,\n",
    "                                  lr: learning_rate,\n",
    "                                  keep_prob: keep_probability})\n",
    "            \n",
    "            \n",
    "#             writer.add_summary(summary, epoch_i * batch_size + batch_i)\n",
    "            \n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(X_train) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test.argmax(axis=1)\n",
    "preds = model.predict_classes(X_test)\n",
    "cm = ConfusionMatrix(y_true, preds)\n",
    "cm.plot(backend='seaborn', normalized=True)\n",
    "plt.title('Confusion Matrix Stars prediction')\n",
    "plt.figure(figsize=(12, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
