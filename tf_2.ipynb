{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Review Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "from langdetect import detect\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_ml import ConfusionMatrix\n",
    "import seaborn\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import random\n",
    "import nltk, re, time\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from collections import namedtuple\n",
    "\n",
    "from contractions import get_contractions\n",
    "import operator\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "alreadyPickled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "num_layers = 2\n",
    "num_classes = 6\n",
    "epochs = 4\n",
    "batch_size = 128\n",
    "rnn_size = 128\n",
    "num_layers = 2\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "keep_probability = 0.8\n",
    "max_sequence_length = 1000\n",
    "\n",
    "IS_TRAINING = True\n",
    "IS_TESTING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    assert isinstance(text, str)\n",
    "    text = text.lower()    \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text,  \n",
    "                  flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done getting Business IDs\n",
      "There are 65028 restaurants\n"
     ]
    }
   ],
   "source": [
    "restId = []\n",
    "for line in open('./data/dataset/business.json', 'r'):\n",
    "    data = json.loads(line)\n",
    "    if 'Restaurants' in data['categories'] or 'Food' in data['categories']:\n",
    "        restId.append(data['business_id'])\n",
    "print(\"Done getting Business IDs\")\n",
    "print(\"There are %d restaurants\" % (len(restId)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "contractions = get_contractions()\n",
    "\n",
    "revs_list = [[]]\n",
    "stars_list = [[]]\n",
    "num = 5000 # Number of review read\n",
    "k = 0 # Count\n",
    "for line in open('./data/dataset/review.json', 'r'):\n",
    "    if k >= num:\n",
    "        break\n",
    "    data = json.loads(line)\n",
    "    text = data['text']\n",
    "    star = data['stars']\n",
    "    ID = data['business_id']\n",
    "    # Check language\n",
    "    if detect(text) != 'en':\n",
    "        continue  \n",
    "    # Check whether it is a restaurant\n",
    "    if ID not in restId:\n",
    "        continue\n",
    "    revs_list.append(clean_text(text))\n",
    "    stars_list.append(star)\n",
    "    k += 1\n",
    "    # Notify for every 500 reviews\n",
    "    if len(revs_list) % 500 == 0:\n",
    "        print(len(revs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "place horrible excited try since got gift card birthday went ordered whole meal except gift card system unacceptable would helpful would known prior\n",
      "5001 5001\n"
     ]
    }
   ],
   "source": [
    "print(revs_list[1])\n",
    "print(len(revs_list), len(stars_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Ziiped Revs is [5001, 2]\n"
     ]
    }
   ],
   "source": [
    "zipped_revs = zip(revs_list, stars_list)\n",
    "print(\"Shape of Ziiped Revs is [%d, %d]\" % (len(zipped_revs), len(zipped_revs[0])))\n",
    "categories = ['text', 'stars']\n",
    "df_reviews_processing = pd.DataFrame(zipped_revs, columns=categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Dropping Nones: Shape 5000,2\n",
      "After Dropping Nones: Shape 5000,2\n"
     ]
    }
   ],
   "source": [
    "df_reviews_processing.shape\n",
    "df_reviews_processing.head()\n",
    "df_reviews_processing[['stars']] = df_reviews_processing[['stars']].apply(pd.to_numeric)\n",
    "\n",
    "df_reviews_processing = df_reviews_processing[np.isfinite(df_reviews_processing['stars'])]\n",
    "\n",
    "print(\"Before Dropping Nones: Shape %d,%d\" % (df_reviews_processing.shape[0], df_reviews_processing.shape[1]))\n",
    "df_reviews = df_reviews_processing[np.isfinite(df_reviews_processing['stars'])]\n",
    "print(\"After Dropping Nones: Shape %d,%d\" % (df_reviews_processing.shape[0], df_reviews_processing.shape[1]))\n",
    "\n",
    "df_reviews_processing.to_csv(\"reviews_df_processed.csv\", encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "If the data is already pickled, then can skip embedding and data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picklefiles(filename, stuff):\n",
    "    save_stuff = open(filename, \"wb\")\n",
    "    pickle.dump(stuff, save_stuff)\n",
    "    save_stuff.close()\n",
    "def loadfiles(filename):\n",
    "    saved_stuff = open(filename,\"rb\")\n",
    "    stuff = pickle.load(saved_stuff)\n",
    "    saved_stuff.close()\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "Using ConceptNet Numberbatch instead of GLoVE (supposedly outperforms GLoVE embeddings)  \n",
    "https://github.com/commonsense/conceptnet-numberbatch\n",
    "  \n",
    "On top of the embeddings, we also keep track of commonly used words in the reviews that Embeddings don't cover. This way we could have higher test accuracy when words we come across words like these. This is specified by a threshold value. Currently, threshold is set to 20 occuraces.  \n",
    "  \n",
    "  \n",
    "We also process the reviews a bit more, sorting them into comparable lengths. This way, there is less padding necessary and (possibly) faster computation time when training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 15259\n"
     ]
    }
   ],
   "source": [
    "word_counts = {}\n",
    "count_words(word_counts, df_reviews.text)            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_path='./embeddings/numberbatch-en-17.02.txt'\n",
    "def load_embeddings(path='./embeddings/numberbatch-en-17.02.txt'):\n",
    "    embeddings_index = {}\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            embedding = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = embedding\n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = load_embeddings(embed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance the Dataset\n",
    "Want to balance the dataset, such that we have an equal number of reviews for each different category.  \n",
    "For example, if our distribution of reviews is [200,500,100,300,400], for [1,2,3,4,5] stars, respectively, then I will only take 100 of each review  \n",
    "I do this so we have an equal representation of all labels when he train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataframe(df, category=['stars'], downsample_by=0.9):\n",
    "    \"\"\"\n",
    "    :param df: pandas.DataFrame\n",
    "    :param categorical_columns: iterable of categorical columns names contained in {df}\n",
    "    :return: balanced pandas.DataFrame\n",
    "    \"\"\"\n",
    "    if category is None or not all([col in df.columns for col in category]):\n",
    "        raise ValueError('Please provide one or more columns containing categorical variables')\n",
    "\n",
    "    lowest_count = df.groupby(category).apply(lambda x: x.shape[0]).min()\n",
    "    # Remove 10 percent of the reviews\n",
    "    lowest_count = int(lowest_count * downsample_by)\n",
    "    \n",
    "    df = df.groupby(category).apply( \n",
    "        lambda x: x.sample(lowest_count)).drop(category, axis=1).reset_index().set_index('level_1')\n",
    "\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pre) 1 star ratings: 5617\n",
      "(pre) 2 star ratings: 4934\n",
      "(pre) 3 star ratings: 7189\n",
      "(pre) 4 star ratings: 13757\n",
      "(pre) 5 star ratings: 18502\n",
      "(post) 1 star ratings: 4440\n",
      "(post) 2 star ratings: 4440\n",
      "(post) 3 star ratings: 4440\n",
      "(post) 4 star ratings: 4440\n",
      "(post) 5 star ratings: 4440\n"
     ]
    }
   ],
   "source": [
    "# df_reviews = pd.read_csv('reviewsText.csv')\n",
    "df_reviews = pd.read_csv(\"./data/reviews_df_processed50000.csv\")\n",
    "df_reviews['len'] = df_reviews.text.str.len()\n",
    "\n",
    "df_reviews = df_reviews[df_reviews['len'].between(10, 4000)]\n",
    "df_reviews[['stars']] = df_reviews[['stars']].apply(pd.to_numeric)\n",
    "\n",
    "print(\"(pre) 1 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 1])))\n",
    "print(\"(pre) 2 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 2])))\n",
    "print(\"(pre) 3 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 3])))\n",
    "print(\"(pre) 4 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 4])))\n",
    "print(\"(pre) 5 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 5])))\n",
    "\n",
    "# Set downsample_by to 0.9, since not much training data to begin with\n",
    "df_balanced = balance_dataframe(df_reviews, \n",
    "                                category=['stars'], \n",
    "                                downsample_by=0.9)\n",
    "\n",
    "df_balanced.to_csv('balanced_reviews1000.csv', encoding='utf-8')\n",
    "print(\"(post) 1 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 1])))\n",
    "print(\"(post) 2 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 2])))\n",
    "print(\"(post) 3 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 3])))\n",
    "print(\"(post) 4 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 4])))\n",
    "print(\"(post) 5 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 5])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words is 34118\n",
      "embedding matrix size: is [20000,300]\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(df_balanced.text.tolist())\n",
    "\n",
    "joblib.dump(tokenizer, 'tokenizer.p')\n",
    "\n",
    "WORD_INDEX_SORTED = sorted(tokenizer.word_index.items(), key=operator.itemgetter(1))\n",
    "print(\"number of words is %d\" % (len(WORD_INDEX_SORTED)))\n",
    "\n",
    "vocab_size = len(WORD_INDEX_SORTED)+1\n",
    "\n",
    "NB_WORDS = min(len(WORD_INDEX_SORTED), MAX_NB_WORDS)\n",
    "\n",
    "word_embedding_matrix = np.zeros((NB_WORDS, embedding_dim), dtype=np.float32)\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i-1] = embedding_vector\n",
    "print(\"embedding matrix size: is [%d,%d]\" % (word_embedding_matrix.shape[0], \n",
    "                                            word_embedding_matrix.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(df_balanced.text.values)\n",
    "\n",
    "padReviews = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "reviewsLength = max_sequence_length\n",
    "\n",
    "ratings = df_balanced.stars.values.astype(int)\n",
    "ratings_cat = to_categorical(ratings)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padReviews, ratings_cat, test_size=0.2, random_state=9)\n",
    "with pd.HDFStore('x_y_test_train.h5') as h:\n",
    "    h['X_train'] = pd.DataFrame(X_train)\n",
    "    h['X_test'] = pd.DataFrame(X_test)\n",
    "    h['y_train'] = pd.DataFrame(y_train)\n",
    "    h['y_test'] = pd.DataFrame(y_test)\n",
    "\n",
    "assert padReviews.shape[0] == ratings.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "(22200, 6)\n",
      "(17760, 1000)\n",
      "(17760, 6)\n"
     ]
    }
   ],
   "source": [
    "print(type(ratings_cat))\n",
    "print(type(X_train))\n",
    "print(ratings_cat.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if alreadyPickled == False:\n",
    "    picklefiles(\"./data/good_pickles/balanced_reviews.p\",df_balanced)\n",
    "    picklefiles(\"./data/good_pickles/category_ratings.p\",ratings_cat)\n",
    "    picklefiles(\"./data/good_pickles/word_embedding_matrix.p\",word_embedding_matrix)\n",
    "    picklefiles(\"./data/good_pickles/tokenizer.p\", tokenizer)\n",
    "    \n",
    "if alreadyPickled == True:\n",
    "\n",
    "    word_embedding_matrix = loadfiles(\"./data/word_embedding_matrix.p\")\n",
    "    df_rev_balanced = pd.read_csv('balanced_reviews.csv')\n",
    "    \n",
    "    ratings_cat = loadfiles(\"./data/good_pickles/category_ratings.p\")\n",
    "    balanced_reviews = loadfiles(\"./data/good_pickles/balanced_reviews.p\")\n",
    "    \n",
    "    tokenizer = joblib.load('tokenizer.pickle')\n",
    "    with pd.HDFStore('x_y_test_train.h5') as h:\n",
    "        X_train = h['X_train'].values\n",
    "        X_test = h['X_test'].values\n",
    "        y_train = h['y_train'].values\n",
    "        y_test = h['y_test'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Batches\n",
    "Gets batches. These will be called later to then fill our X and y placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size):\n",
    "    assert isinstance(batch_size, int)\n",
    "    assert isinstance(x, np.ndarray) and isinstance(y, np.ndarray)\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size,:], y[ii:ii+batch_size]\n",
    "        \n",
    "def get_test_batches(x, batch_size):\n",
    "    \n",
    "    assert isinstance(batch_size, int)\n",
    "    assert isinstance(x, np.ndarray)\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x = x[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1000)\n",
      "(128, 6)\n"
     ]
    }
   ],
   "source": [
    "# Display some data vectors here and their type\n",
    "for batch_i, (trainX_batch, trainY_batch) in enumerate(get_batches(X_train, y_train, batch_size)):\n",
    "    print(trainX_batch.shape)\n",
    "    print(trainY_batch.shape)\n",
    "    if batch_i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Graph\n",
    "Here we start building our computational graph. We use a 2 layer GRU Recurrent Neural Network. We define placeholders for learning rate and dropout since these are variables that we could potentially want to vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    # Should be [batch_size x review length]\n",
    "    with tf.name_scope(\"input_data\"):\n",
    "        input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "        tf.summary_scalar(\"input_data\", input_data)\n",
    "        \n",
    "    # Should be [batch_size x num_classes]\n",
    "    with tf.name_scope(\"labels\"):\n",
    "        labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "        tf.summary_scalar(\"labels\", labels)\n",
    "        \n",
    "    with tf.name_scope(\"learning_rate\"):\n",
    "        lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "        tf.summary_scalar(\"learning_rate\", lr)\n",
    "        \n",
    "    with tf.name_scope(\"keep_prob\")\n",
    "        keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "        tf.summary_scalar(\"dropout_keep_prob\", keep_prob)\n",
    "\n",
    "    return input_data, labels, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input Data\n",
      "Tensor(\"Shape:0\", shape=(2,), dtype=int32)\n",
      "Graph is built.\n",
      "./graph\n"
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        input_data, labels, lr, keep_prob = model_inputs()\n",
    "    \n",
    "    print(\"Shape of Input Data\")\n",
    "    print(tf.shape(input_data))\n",
    "    embeddings = word_embedding_matrix\n",
    "    embs = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "\n",
    "    with tf.name_scope(\"RNN_Layers\"):\n",
    "        \n",
    "        stacked_rnn = []\n",
    "        for layer in range(num_layers):\n",
    "            cell_fw = tf.contrib.rnn.GRUCell(rnn_size)\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw,\n",
    "                                                    output_keep_prob=keep_prob)\n",
    "            stacked_rnn.append(cell_fw)\n",
    "        multilayer_cell = tf.contrib.rnn.MultiRNNCell(stacked_rnn, state_is_tuple=True)\n",
    "\n",
    "        \n",
    "    with tf.name_scope(\"init_state\"):\n",
    "        initial_state = multilayer_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    with tf.name_scope(\"Forward_Pass\"):\n",
    "        output, final_state = tf.nn.dynamic_rnn(multilayer_cell,\n",
    "                                           embs,\n",
    "                                           dtype=tf.float32)\n",
    "\n",
    "\n",
    "    with tf.name_scope(\"FC_Layer\"):\n",
    "        dense = tf.contrib.layers.fully_connected(output[:,-1],\n",
    "                                                  num_outputs = 64,\n",
    "                                                  activation_fn = tf.sigmoid,\n",
    "                                                  weights_initializer = tf.random_normal_initializer())\n",
    "        \n",
    "        dense = tf.contrib.layers.dropout(dense, keep_prob)\n",
    "\n",
    "    with tf.name_scope(\"Predictions\"):\n",
    "        predictions = tf.contrib.layers.fully_connected(dense,\n",
    "                                                        num_outputs = num_classes, \n",
    "                                                        activation_fn=tf.nn.relu,\n",
    "                                                        weights_initializer = tf.random_normal_initializer())\n",
    "        tf.summary.histogram('predictions', predictions)\n",
    "        \n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.losses.mean_squared_error(labels, predictions)\n",
    "        tf.summary.scalar('cost', cost)\n",
    "        \n",
    "    with tf.name_scope('train'):    \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "        \n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    export_nodes = ['input_data', 'labels', 'lr', 'keep_prob', 'initial_state', 'final_state',\n",
    "                    'accuracy', 'predictions', 'cost', 'optimizer', 'merged']\n",
    "    \n",
    "print(\"Graph is built.\")\n",
    "graph_location = \"./graph\"\n",
    "\n",
    "Graph = namedtuple('train_graph', export_nodes)\n",
    "local_dict = locals()\n",
    "graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "print(graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(train_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape_1:0\", shape=(3,), dtype=int32)\n",
      "Tensor(\"Shape_2:0\", shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.shape(output))\n",
    "print(tf.shape(embs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(X_train)//batch_size//per_epoch)-1\n",
    "keep_probability = 0.75\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "I've been keeping track of the tensorboard summaries so it'll allow me to visualize the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] Epoch 1 , Batch 10\n",
      "[PROGRESS] Epoch 1 , Batch 20\n",
      "Epoch   1/4 Batch   20/138 - Loss:  1.825, Seconds: 168.55\n",
      "[PROGRESS] Epoch 1 , Batch 30\n",
      "[PROGRESS] Epoch 1 , Batch 40\n",
      "Epoch   1/4 Batch   40/138 - Loss:  0.249, Seconds: 156.71\n",
      "Average loss for this update: 0.946\n",
      "No Improvement.\n",
      "[PROGRESS] Epoch 1 , Batch 50\n",
      "[PROGRESS] Epoch 1 , Batch 60\n",
      "Epoch   1/4 Batch   60/138 - Loss:  0.200, Seconds: 157.27\n",
      "[PROGRESS] Epoch 1 , Batch 70\n",
      "[PROGRESS] Epoch 1 , Batch 80\n",
      "Epoch   1/4 Batch   80/138 - Loss:  0.188, Seconds: 156.30\n",
      "[PROGRESS] Epoch 1 , Batch 90\n",
      "Average loss for this update: 0.187\n",
      "No Improvement.\n",
      "[PROGRESS] Epoch 1 , Batch 100\n",
      "Epoch   1/4 Batch  100/138 - Loss:  0.176, Seconds: 156.01\n",
      "[PROGRESS] Epoch 1 , Batch 110\n",
      "[PROGRESS] Epoch 1 , Batch 120\n",
      "Epoch   1/4 Batch  120/138 - Loss:  0.175, Seconds: 192.81\n",
      "[PROGRESS] Epoch 1 , Batch 130\n",
      "Average loss for this update: 0.178\n",
      "No Improvement.\n",
      "[PROGRESS] Epoch 2 , Batch 10\n",
      "[PROGRESS] Epoch 2 , Batch 20\n",
      "Epoch   2/4 Batch   20/138 - Loss:  0.172, Seconds: 265.54\n",
      "[PROGRESS] Epoch 2 , Batch 30\n",
      "[PROGRESS] Epoch 2 , Batch 40\n",
      "Epoch   2/4 Batch   40/138 - Loss:  0.168, Seconds: 176.25\n",
      "Average loss for this update: 0.17\n",
      "No Improvement.\n",
      "[PROGRESS] Epoch 2 , Batch 50\n",
      "[PROGRESS] Epoch 2 , Batch 60\n",
      "Epoch   2/4 Batch   60/138 - Loss:  0.168, Seconds: 161.20\n",
      "[PROGRESS] Epoch 2 , Batch 70\n",
      "[PROGRESS] Epoch 2 , Batch 80\n",
      "Epoch   2/4 Batch   80/138 - Loss:  0.168, Seconds: 191.92\n",
      "[PROGRESS] Epoch 2 , Batch 90\n",
      "Average loss for this update: 0.167\n",
      "No Improvement.\n",
      "[PROGRESS] Epoch 2 , Batch 100\n",
      "Epoch   2/4 Batch  100/138 - Loss:  0.167, Seconds: 178.11\n",
      "[PROGRESS] Epoch 2 , Batch 110\n",
      "[PROGRESS] Epoch 2 , Batch 120\n",
      "Epoch   2/4 Batch  120/138 - Loss:  0.167, Seconds: 186.37\n",
      "[PROGRESS] Epoch 2 , Batch 130\n",
      "Average loss for this update: 0.167\n",
      "No Improvement.\n",
      "[PROGRESS] Epoch 3 , Batch 10\n",
      "[PROGRESS] Epoch 3 , Batch 20\n",
      "Epoch   3/4 Batch   20/138 - Loss:  0.168, Seconds: 215.44\n",
      "[PROGRESS] Epoch 3 , Batch 30\n",
      "[PROGRESS] Epoch 3 , Batch 40\n",
      "Epoch   3/4 Batch   40/138 - Loss:  0.167, Seconds: 192.41\n",
      "Average loss for this update: 0.167\n",
      "No Improvement.\n",
      "[PROGRESS] Epoch 3 , Batch 50\n",
      "[PROGRESS] Epoch 3 , Batch 60\n",
      "Epoch   3/4 Batch   60/138 - Loss:  0.167, Seconds: 182.97\n",
      "[PROGRESS] Epoch 3 , Batch 70\n",
      "[PROGRESS] Epoch 3 , Batch 80\n",
      "Epoch   3/4 Batch   80/138 - Loss:  2.863, Seconds: 192.63\n",
      "[PROGRESS] Epoch 3 , Batch 90\n",
      "Average loss for this update: 1.366\n",
      "No Improvement.\n",
      "[PROGRESS] Epoch 3 , Batch 100\n",
      "Epoch   3/4 Batch  100/138 - Loss:  0.168, Seconds: 180.12\n",
      "[PROGRESS] Epoch 3 , Batch 110\n",
      "[PROGRESS] Epoch 3 , Batch 120\n",
      "Epoch   3/4 Batch  120/138 - Loss:  0.167, Seconds: 177.91\n",
      "[PROGRESS] Epoch 3 , Batch 130\n",
      "Average loss for this update: 0.167\n",
      "No Improvement.\n",
      "[PROGRESS] Epoch 4 , Batch 10\n",
      "[PROGRESS] Epoch 4 , Batch 20\n",
      "Epoch   4/4 Batch   20/138 - Loss:  0.167, Seconds: 160.80\n",
      "[PROGRESS] Epoch 4 , Batch 30\n",
      "[PROGRESS] Epoch 4 , Batch 40\n",
      "Epoch   4/4 Batch   40/138 - Loss:  0.167, Seconds: 156.87\n",
      "Average loss for this update: 0.167\n",
      "New Record!\n",
      "[PROGRESS] Epoch 4 , Batch 50\n",
      "[PROGRESS] Epoch 4 , Batch 60\n",
      "Epoch   4/4 Batch   60/138 - Loss:  0.167, Seconds: 157.31\n",
      "[PROGRESS] Epoch 4 , Batch 70\n",
      "[PROGRESS] Epoch 4 , Batch 80\n",
      "Epoch   4/4 Batch   80/138 - Loss:  0.167, Seconds: 157.59\n",
      "[PROGRESS] Epoch 4 , Batch 90\n",
      "Average loss for this update: 0.167\n",
      "New Record!\n",
      "[PROGRESS] Epoch 4 , Batch 100\n",
      "Epoch   4/4 Batch  100/138 - Loss:  0.167, Seconds: 157.90\n",
      "[PROGRESS] Epoch 4 , Batch 110\n",
      "[PROGRESS] Epoch 4 , Batch 120\n",
      "Epoch   4/4 Batch  120/138 - Loss:  0.167, Seconds: 157.73\n",
      "[PROGRESS] Epoch 4 , Batch 130\n",
      "Average loss for this update: 0.167\n",
      "New Record!\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"./saves/best_model_50k.ckpt\" \n",
    "logs_path = \"./logs/\"\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "if IS_TRAINING:\n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #     loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "    #     loader.restore(sess, checkpoint)\n",
    "\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('./summaries' + '/train', sess.graph)\n",
    "\n",
    "        for epoch_i in range(1, epochs+1):\n",
    "            state = sess.run(graph.initial_state)\n",
    "\n",
    "            update_loss = 0\n",
    "            batch_loss = 0\n",
    "\n",
    "            for batch_i, (x, y) in enumerate(get_batches(X_train, y_train, batch_size), 1):\n",
    "                if batch_i % 10 == 0:\n",
    "                    print(\"[PROGRESS] Epoch %d , Batch %d\" % (epoch_i,batch_i))\n",
    "                feed = {graph.input_data: x,\n",
    "                        graph.labels: y,\n",
    "                        graph.keep_prob: keep_probability,\n",
    "                        graph.initial_state: state}\n",
    "                start_time = time.time()\n",
    "                summary, loss, acc, state, _ = sess.run([graph.merged, \n",
    "                                                         graph.cost, \n",
    "                                                         graph.accuracy, \n",
    "                                                         graph.final_state, \n",
    "                                                         graph.optimizer], \n",
    "                                                        feed_dict=feed)\n",
    "\n",
    "                train_writer.add_summary(summary, epoch_i*batch_i + batch_i)\n",
    "\n",
    "                batch_loss += loss\n",
    "                update_loss += loss\n",
    "                end_time = time.time()\n",
    "                batch_time = end_time - start_time\n",
    "\n",
    "                if batch_i % display_step == 0 and batch_i > 0:\n",
    "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(epoch_i,\n",
    "                                  epochs, \n",
    "                                  batch_i, \n",
    "                                  len(X_train) // batch_size, \n",
    "                                  batch_loss / display_step, \n",
    "                                  batch_time*display_step))\n",
    "                    batch_loss = 0\n",
    "\n",
    "                if batch_i % update_check == 0 and batch_i > 0:\n",
    "                    print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                    summary_update_loss.append(update_loss)\n",
    "\n",
    "                    # If the update loss is at a new minimum, save the model\n",
    "                    if update_loss <= min(summary_update_loss):\n",
    "                        print('New Record!') \n",
    "                        stop_early = 0\n",
    "                        saver = tf.train.Saver() \n",
    "                        saver.save(sess, checkpoint)\n",
    "\n",
    "                    else:\n",
    "                        print(\"No Improvement.\")\n",
    "                        stop_early += 1\n",
    "                        if stop_early == stop:\n",
    "                            break\n",
    "                    update_loss = 0\n",
    "\n",
    "\n",
    "            # Reduce learning rate, but not below its minimum value\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "\n",
    "            if stop_early == stop:\n",
    "                print(\"Stopping Training.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Data\n",
    "This part of the code is allocated to testing the data. On top of recording accuracy results, I also generate a confusion matrix. Since reviews are subjective and aren't concretely one rating or another, a confusion matrix helps visualize your results a lot better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if IS_TESTING:\n",
    "    \n",
    "    y_true = y_test.argmax(axis=1)\n",
    "    checkpoint = \"./saves/best_model_50k.ckpt\"  \n",
    "    \n",
    "    all_preds = []\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        # Load the model\n",
    "        saver.restore(sess, checkpoint)\n",
    "        test_state = sess.run(graph.initial_state)\n",
    "\n",
    "        for _, x in enumerate(get_test_batches(x_test, batch_size), 1):\n",
    "            feed = {graph.input_data: x,\n",
    "                    graph.keep_prob: keep_probability,\n",
    "                    graph.initial_state: state}\n",
    "\n",
    "            predictions = sess.run(graph.predictions, feed_dict=feed)\n",
    "            for pred in predictions:\n",
    "                all_preds.append(float(pred))\n",
    "                \n",
    "    y_predictions = preds.argmax(axis=1)\n",
    "    cm = ConfusionMatrix(y_true, y_predictions)\n",
    "    cm.plot(backend='seaborn', normalized=True)\n",
    "    plt.title('Confusion Matrix Stars prediction')\n",
    "    plt.figure(figsize=(12, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
