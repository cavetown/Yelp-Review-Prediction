{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import itertools\n",
    "\n",
    "import utils\n",
    "import util\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import nltk, re, time\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tf.keras.preprocessing.text import Tokenizer\n",
    "from tf.keras.preprocessing.sequence import pad_sequences\n",
    "from tf.keras.utils import to_categorical\n",
    "from collections import namedtuple\n",
    "\n",
    "from contractions import get_contractions\n",
    "\n",
    "alreadyPickled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "num_layers = 2\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.01\n",
    "keep_probability = 0.75\n",
    "max_sequence_length = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "If the data is already pickled, then can skip embedding and data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if alreadyPickled:\n",
    "    clean_reviews = loadfiles(\"./data/clean_reviews.p\")\n",
    "\n",
    "    sorted_reviews = loadfiles(\"./data/sorted_reviews.p\")\n",
    "    word_embedding_matrix = loadfiles(\"./data/word_embedding_matrix.p\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to grab contractions. Located in contractions.py\n",
    "contractions = get_contractions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    text = text.lower()    \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text,  \n",
    "                  flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picklefiles(filename, stuff):\n",
    "    save_stuff = open(filename, \"wb\")\n",
    "    pickle.dump(stuff, save_stuff)\n",
    "    save_stuff.close()\n",
    "def loadfiles(filename):\n",
    "    saved_stuff = open(filename,\"rb\")\n",
    "    stuff = pickle.load(saved_stuff)\n",
    "    saved_stuff.close()\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_reviews = []\n",
    "for text in reviews.Text:\n",
    "    clean_reviews.append(clean_text(text))\n",
    "print(\"Texts are complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "Using ConceptNet Numberbatch instead of GLoVE (supposedly outperforms GLoVE embeddings)  \n",
    "  \n",
    "  \n",
    "On top of the embeddings, we also keep track of commonly used words in the reviews that Embeddings don't cover. This way we could have higher test accuracy when words we come across words like these. This is specified by a threshold value. Currently, threshold is set to 20 occuraces.  \n",
    "  \n",
    "  \n",
    "We also process the reviews a bit more, sorting them into comparable lengths. This way, there is less padding necessary and (possibly) faster computation time when training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "count_words(word_counts, clean_reviews)            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_path='./embeddings/numberbatch-en-17.02.txt'\n",
    "def load_embeddings(path='./embeddings/numberbatch-en-17.02.txt'):\n",
    "    embeddings_index = {}\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            embedding = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = embedding\n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = load_embeddings(embed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_for_layer(X, trained_model, layer_number, batches=256):\n",
    "    \"\"\"\n",
    "    :param X: Batch with dimensions according to the models first layer input-shape\n",
    "    :param trained_model: Model to extract data from\n",
    "    :param layer_number: Index of the layer we want to extract features from.\n",
    "    :param batches: If set it will call the function in batches to save (gpu)memory\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    get_features = K.function([trained_model.layers[0].input, K.learning_phase()],\n",
    "                              [trained_model.layers[layer_number].output])\n",
    "    \n",
    "    if batches:\n",
    "        g = array_batch_yield(X, batches)\n",
    "        features = []\n",
    "        for batch in g:\n",
    "            feature_batch = get_features([batch, 0])\n",
    "            features.append(feature_batch)\n",
    "            \n",
    "        features = np.concatenate(features, axis=1)[0]\n",
    "        \n",
    "    else:\n",
    "        features = get_features([X, 0])\n",
    "\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance the Dataset\n",
    "Want to balance the dataset, such that we have an equal number of reviews for each different category.  \n",
    "For example, if our distribution of reviews is [200,500,100,300,400], for [1,2,3,4,5] stars, respectively, then we will only take 100 of each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minority_balance_dataframe_by_multiple_categorical_variables(df, categorical_columns=None, downsample_by=0.1):\n",
    "    \"\"\"\n",
    "    :param df: pandas.DataFrame\n",
    "    :param categorical_columns: iterable of categorical columns names contained in {df}\n",
    "    :return: balanced pandas.DataFrame\n",
    "    \"\"\"\n",
    "    if categorical_columns is None or not all([c in df.columns for c in categorical_columns]):\n",
    "        raise ValueError('Please provide one or more columns containing categorical variables')\n",
    "\n",
    "    minority_class_combination_count = df.groupby(categorical_columns).apply(lambda x: x.shape[0]).min()\n",
    "    \n",
    "    minority_class_combination_count = int(minority_class_combination_count * downsample_by)\n",
    "    \n",
    "    df = df.groupby(categorical_columns).apply(\n",
    "        lambda x: x.sample(minority_class_combination_count)\n",
    "    ).drop(categorical_columns, axis=1).reset_index().set_index('level_1')\n",
    "\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_csv('reviews.csv')#, encoding='utf-8')\n",
    "df_reviews['len'] = df_reviews.text.str.len()\n",
    "df_reviews = df_reviews[df_reviews['len'].between(10, 4000)]\n",
    "\n",
    "df_balanced = minority_balance_dataframe_by_multiple_categorical_variables(\n",
    "    df_reviews, \n",
    "    categorical_columns=['stars'], \n",
    "    downsample_by=0.1)\n",
    "\n",
    "df_balanced.to_csv('balanced_reviews.csv', encoding='utf-8')\n",
    "    \n",
    "tokenizer = Tokenizer(nb_words=NB_WORDS)\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "joblib.dump(tokenizer, 'tokenizer.pickle')\n",
    "\n",
    "WORD_INDEX_SORTED = sorted(tokenizer.word_index.items(), key=operator.itemgetter(1))\n",
    "seqs = tokenizer.texts_to_sequences(df_balanced.text.values)\n",
    "\n",
    "padReviews = pad_sequences(seqs, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "reviewsLength = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "ratings = df_balanced.stars.values.astype(int)\n",
    "ratings_cat = to_categorical(ratings)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padReviews, ratings_cat, test_size=0.2, random_state=9)\n",
    "with pd.HDFStore('x_y_test_train.h5') as store:\n",
    "    store['X_train'] = pd.DataFrame(X_train)\n",
    "    store['X_test'] = pd.DataFrame(X_test)\n",
    "    store['y_train'] = pd.DataFrame(y_train)\n",
    "    store['y_test'] = pd.DataFrame(y_test)\n",
    "\n",
    "assert padReviews.shape[0] == ratings.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "NB_WORDS = min(len(WORD_INDEX_SORTED), MAX_NB_WORDS)\n",
    "\n",
    "word_embedding_matrix = np.zeros((NB_WORDS, embedding_dim),\n",
    "                                 dtype=np.float32)\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_texts = create_lengths(int_texts)\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the length of texts\n",
    "print(np.percentile(lengths_texts.counts, 80))\n",
    "print(np.percentile(lengths_texts.counts, 85))\n",
    "print(np.percentile(lengths_texts.counts, 90))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if alreadyPickled == False:\n",
    "    picklefiles(\"./data/good_pickles/clean_reviews.p\",clean_reviews)\n",
    "    picklefiles(\"./data/good_pickles/sorted_reviews.p\",sorted_reviews)\n",
    "    picklefiles(\"./data/good_pickles/word_embedding_matrix.p\",word_embedding_matrix)\n",
    "    picklefiles(\"./data/good_pickles/int_to_vocab.p\",int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    # Should be [batch_size x review length]\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    # Should be [batch_size x num_classes]\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    \n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    return input_data, labels, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_lstm(rnn_inputs, rnn_size, num_layers, keep_prob):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('GRU_lstm'):\n",
    "            cell_fw = tf.contrib.rnn.GRUCell(rnn_size,\n",
    "                                             initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw,\n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            output, states = tf.nn.dynamic_rnn(cell_fw,\n",
    "                                               rnn_inputs,\n",
    "                                               dtype=tf.float32)\n",
    "            \n",
    "#             output, states = tf.nn.dynamic_rnn(cell_fw,\n",
    "#                                                rnn_inputs,\n",
    "#                                                sequence_length,\n",
    "#                                                dtype=tf.float32)\n",
    "    return output, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gettin Batches\n",
    "Gets batches as well as pads them to have similar length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(ratings, reviews, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(reviews)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        rating_batch = ratings[start_i:start_i + batch_size, :]\n",
    "        reviews_batch = reviews[start_i:start_i + batch_size, :]\n",
    "\n",
    "        yield rating_batch, pad_texts_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(WORD_INDEX_SORTED)+1\n",
    "def prediction_model(input_data, target_data, keep_prob, num_classes \n",
    "                     vocab_size, rnn_size, num_layers, batch_size):\n",
    "    \n",
    "    W2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\n",
    "    b2 = tf.Variable(np.zeros((1, num_classes)), dtype=tf.float32)\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    embs = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    output, state = gru_lstm(input_data, rnn_size, num_layers, rnn_inputs, keep_prob)\n",
    "    states_series = tf.reshape(state, [-1, rnn_size])\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, labels, lr, keep_prob = model_inputs()\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits[0].rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits[0].sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss, the sould be all True across since each batch is padded\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.losses.softmax_cross_entropy(one_hot_labels,\n",
    "                                               logits)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.RMSOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "        \n",
    "print(\"Graph is built.\")\n",
    "graph_location = \"./graph\"\n",
    "print(graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(train_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find missing embedding words\n",
    "# missing_words = 0\n",
    "# threshold = 20\n",
    "\n",
    "# for word, count in word_counts.items():\n",
    "#     if count > threshold:\n",
    "#         if word not in embeddings_index:\n",
    "#             missing_words += 1\n",
    "            \n",
    "# missing_ratio = round(missing_words/len(word_counts),4)*100    \n",
    "\n",
    "# print(\"Number of words missing from CN:\", missing_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "# word_count = 0\n",
    "# unk_count = 0x\n",
    "\n",
    "# int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
    "\n",
    "# unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "# print(\"Total number of words in headlines:\", word_count)\n",
    "# print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "# print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pad_sentence_batch(sentence_batch):\n",
    "#     \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "#     max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "#     return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sort the summaries and texts by the length of the texts, shortest to longest\n",
    "# # Limit the length of summaries and texts based on the min and max ranges.\n",
    "# # Remove reviews that include too many UNKs\n",
    "# sorted_reviews = []\n",
    "# max_text_length = 200\n",
    "# min_length = 2\n",
    "# unk_text_limit = 1\n",
    "# unk_summary_limit = 0\n",
    "\n",
    "# for length in range(min(lengths_texts.counts), max_text_length): \n",
    "#     for count, words in enumerate(int_summaries):\n",
    "#         if (len(int_texts[count]) >= min_length and\n",
    "#             length == len(int_texts[count])\n",
    "#            ):\n",
    "#             sorted_texts.append(int_texts[count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_to_int = {} \n",
    "# value = 0\n",
    "# for word, count in word_counts.items():\n",
    "#     if count >= threshold or word in embeddings_index:\n",
    "#         vocab_to_int[word] = value\n",
    "#         value += 1\n",
    "\n",
    "# # Special tokens that will be added to our vocab\n",
    "# codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# # Add codes to vocab\n",
    "# for code in codes:\n",
    "#     vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# # Dictionary to convert integers to words\n",
    "# int_to_vocab = {}\n",
    "# for word, value in vocab_to_int.items():\n",
    "#     int_to_vocab[value] = word\n",
    "\n",
    "# usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "# print(\"Total number of unique words:\", len(word_counts))\n",
    "# print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "# print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unk_counter(sentence):\n",
    "#     '''Counts the number of time UNK appears in a sentence.'''\n",
    "#     unk_count = 0\n",
    "#     for word in sentence:\n",
    "#         if word == vocab_to_int[\"<UNK>\"]:\n",
    "#             unk_count += 1\n",
    "#     return unk_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
