{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Review Prediction\n",
    "Objective: Construct and train a Neural Network that would be able to predict the number of star ratings from a Yelp review.    \n",
    "Dataset used: https://www.yelp.com/dataset/challenge  \n",
    "\n",
    "Steps:  \n",
    "1) Data Preprocessing  \n",
    "2) Deep Learning Preprocessing  \n",
    "3) Network Training  \n",
    "4) Network Testing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "from langdetect import detect\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_ml import ConfusionMatrix\n",
    "import seaborn\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import nltk, re, time\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from collections import namedtuple\n",
    "\n",
    "from contractions import get_contractions\n",
    "import operator\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "alreadyPickled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "num_layers = 2\n",
    "num_classes = 6\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "rnn_size = 64\n",
    "num_layers = 2\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "keep_probability = 0.8\n",
    "max_sequence_length = 700\n",
    "\n",
    "IS_TRAINING = True\n",
    "IS_TESTING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "The following few cells preprocess the data for me. Clean test defines a function where it removes stop_words (found here https://gist.github.com/sebleier/554280). These words typically have no beneficial meaning to any reviews and are thus wasted features. I also strip punctuation and turn everything lower case. These procedures can be considered pretty standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    \n",
    "    assert isinstance(text, unicode)\n",
    "    \n",
    "    text = text.lower()    \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 65028 restaurants\n"
     ]
    }
   ],
   "source": [
    "restId = []\n",
    "for line in open('./data/dataset/business.json', 'r'):\n",
    "    data = json.loads(line)\n",
    "    if 'Restaurants' in data['categories'] or 'Food' in data['categories']:\n",
    "        restId.append(data['business_id'])\n",
    "print(\"There are %d restaurants\" % (len(restId)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing the reviews and star ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-c194fef6feb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'business_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Check language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Check whether it is a restaurant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thomasan/tensorflow/lib/python2.7/site-packages/langdetect/detector_factory.pyc\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0minit_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_factory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thomasan/tensorflow/lib/python2.7/site-packages/langdetect/detector_factory.pyc\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, alpha)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;34m'''Construct Detector instance with smoothing parameter.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thomasan/tensorflow/lib/python2.7/site-packages/langdetect/detector_factory.pyc\u001b[0m in \u001b[0;36m_create_detector\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanglist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLangDetectException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrorCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNeedLoadProfileError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Need to load profiles.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thomasan/tensorflow/lib/python2.7/site-packages/langdetect/detector.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, factory)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanglist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanglist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlangprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/random.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgauss_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/random.pyc\u001b[0m in \u001b[0;36mseed\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;31m# Seed with enough bytes to span the 19937 bit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;31m# state space for the Mersenne Twister\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_hexlify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_urandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "contractions = get_contractions()\n",
    "\n",
    "revs_list = [[]]\n",
    "stars_list = [[]]\n",
    "num = 5000 # Number of review read\n",
    "k = 0 # Count\n",
    "for line in open('./data/dataset/review.json', 'r'):\n",
    "    if k >= num:\n",
    "        break\n",
    "    data = json.loads(line)\n",
    "    text = data['text']\n",
    "    star = data['stars']\n",
    "    ID = data['business_id']\n",
    "    # Check language\n",
    "    if detect(text) != 'en':\n",
    "        continue  \n",
    "    # Check whether it is a restaurant\n",
    "    if ID not in restId:\n",
    "        continue\n",
    "\n",
    "    revs_list.append(clean_text(text))\n",
    "    stars_list.append(star)\n",
    "    k += 1\n",
    "    # Notify for every 500 reviews\n",
    "    if len(revs_list) % 500 == 0:\n",
    "        print(len(revs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "place horrible excited try since got gift card birthday went ordered whole meal except gift card system unacceptable would helpful would known prior\n",
      "1045 1045\n"
     ]
    }
   ],
   "source": [
    "print(revs_list[1])\n",
    "print(len(revs_list), len(stars_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Ziped Revs is [1045, 2]\n"
     ]
    }
   ],
   "source": [
    "zipped_revs = zip(revs_list, stars_list)\n",
    "print(\"Shape of Ziped Revs is [%d, %d]\" % (len(zipped_revs), len(zipped_revs[0])))\n",
    "categories = ['text', 'stars']\n",
    "\n",
    "df_reviews_processing = pd.DataFrame(zipped_revs, columns=categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Dropping Nones: Shape 1044,2\n",
      "After Dropping Nones: Shape 1044,2\n"
     ]
    }
   ],
   "source": [
    "df_reviews_processing.shape\n",
    "df_reviews_processing.head()\n",
    "df_reviews_processing[['stars']] = df_reviews_processing[['stars']].apply(pd.to_numeric)\n",
    "\n",
    "# Grabbing only numbers that are of numerical value (get rid of None, NaN, etc)\n",
    "df_reviews_processing = df_reviews_processing[np.isfinite(df_reviews_processing['stars'])]\n",
    "\n",
    "print(\"Before Dropping Nones: Shape %d,%d\" % (df_reviews_processing.shape[0], df_reviews_processing.shape[1]))\n",
    "df_reviews = df_reviews_processing[np.isfinite(df_reviews_processing['stars'])]\n",
    "print(\"After Dropping Nones: Shape %d,%d\" % (df_reviews.shape[0], df_reviews.shape[1]))\n",
    "\n",
    "df_reviews.to_csv(\"reviews_df_processed.csv\", encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "If the data is already pickled, then can skip embedding and data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picklefiles(filename, stuff):\n",
    "    save_stuff = open(filename, \"wb\")\n",
    "    pickle.dump(stuff, save_stuff)\n",
    "    save_stuff.close()\n",
    "def loadfiles(filename):\n",
    "    saved_stuff = open(filename,\"rb\")\n",
    "    stuff = pickle.load(saved_stuff)\n",
    "    saved_stuff.close()\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "Using ConceptNet Numberbatch instead of GLoVE (supposedly outperforms GLoVE embeddings)  \n",
    "https://github.com/commonsense/conceptnet-numberbatch\n",
    "  \n",
    "On top of the embeddings, we also keep track of commonly used words in the reviews that Embeddings don't cover. This way we could have higher test accuracy when words we come across words like these. This is specified by a threshold value. Currently, threshold is set to 20 occuraces.  \n",
    "  \n",
    "  \n",
    "We also process the reviews a bit more, sorting them into comparable lengths. This way, there is less padding necessary and (possibly) faster computation time when training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 7260\n"
     ]
    }
   ],
   "source": [
    "word_counts = {}\n",
    "count_words(word_counts, df_reviews.text)            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_path='./embeddings/numberbatch-en-17.02.txt'\n",
    "def load_embeddings(path='./embeddings/numberbatch-en-17.02.txt'):\n",
    "    embeddings_index = {}\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            embedding = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = embedding\n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = load_embeddings(embed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance the Dataset\n",
    "Want to balance the dataset, such that we have an equal number of reviews for each different category.  \n",
    "For example, if our distribution of reviews is [200,500,100,300,400], for [1,2,3,4,5] stars, respectively, then I will only take 100 of each review  \n",
    "I do this so we have an equal representation of all labels when he train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataframe(df, category=['stars'], downsample_by=0.9):\n",
    "    \"\"\"\n",
    "    :param df: pandas.DataFrame\n",
    "    :param categorical_columns: iterable of categorical columns names contained in {df}\n",
    "    :return: balanced pandas.DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(df, pd.DataFrame)\n",
    "    if category is None or not all([col in df.columns for col in category]):\n",
    "        raise ValueError('Please provide one or more columns containing categorical variables')\n",
    "\n",
    "    lowest_count = df.groupby(category).apply(lambda x: x.shape[0]).min()\n",
    "    # Remove 10 percent of the reviews\n",
    "    lowest_count = int(lowest_count * downsample_by)\n",
    "    \n",
    "    df = df.groupby(category).apply( \n",
    "        lambda x: x.sample(lowest_count)).drop(category, axis=1).reset_index().set_index('level_1')\n",
    "\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pre) 1 star ratings: 11399\n",
      "(pre) 2 star ratings: 10085\n",
      "(pre) 3 star ratings: 14640\n",
      "(pre) 4 star ratings: 27767\n",
      "(pre) 5 star ratings: 36104\n",
      "(post) 1 star ratings: 9076\n",
      "(post) 2 star ratings: 9076\n",
      "(post) 3 star ratings: 9076\n",
      "(post) 4 star ratings: 9076\n",
      "(post) 5 star ratings: 9076\n"
     ]
    }
   ],
   "source": [
    "df_reviews = pd.read_csv(\"./data/reviews_df_processed100k.csv\")\n",
    "df_reviews['len'] = df_reviews.text.str.len()\n",
    "\n",
    "df_reviews = df_reviews[df_reviews['len'].between(10, 4000)]\n",
    "df_reviews[['stars']] = df_reviews[['stars']].apply(pd.to_numeric)\n",
    "\n",
    "print(\"(pre) 1 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 1])))\n",
    "print(\"(pre) 2 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 2])))\n",
    "print(\"(pre) 3 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 3])))\n",
    "print(\"(pre) 4 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 4])))\n",
    "print(\"(pre) 5 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 5])))\n",
    "\n",
    "# Set downsample_by to 0.9, since not much training data to begin with\n",
    "df_balanced = balance_dataframe(df_reviews, \n",
    "                                category=['stars'], \n",
    "                                downsample_by=0.9)\n",
    "\n",
    "df_balanced.to_csv('balanced_reviews1000.csv', encoding='utf-8')\n",
    "print(\"(post) 1 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 1])))\n",
    "print(\"(post) 2 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 2])))\n",
    "print(\"(post) 3 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 3])))\n",
    "print(\"(post) 4 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 4])))\n",
    "print(\"(post) 5 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 5])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592.0\n",
      "690.0\n",
      "753.0\n",
      "1089.0\n"
     ]
    }
   ],
   "source": [
    "df_balanced.head()\n",
    "print(np.percentile(df_balanced.len, 80))\n",
    "print(np.percentile(df_balanced.len, 85))\n",
    "print(np.percentile(df_balanced.len, 87.5))\n",
    "print(np.percentile(df_balanced.len, 95))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words is 47647\n",
      "embedding matrix size: is [15000,300]\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300 # Matches CN embedding file\n",
    "MAX_NB_WORDS = 15000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(df_balanced.text.tolist())\n",
    "\n",
    "WORD_INDEX_SORTED = sorted(tokenizer.word_index.items(), key=operator.itemgetter(1))\n",
    "print(\"number of words is %d\" % (len(WORD_INDEX_SORTED)))\n",
    "\n",
    "vocab_size = len(WORD_INDEX_SORTED)+1\n",
    "\n",
    "NB_WORDS = min(len(WORD_INDEX_SORTED), MAX_NB_WORDS)\n",
    "\n",
    "word_embedding_matrix = np.zeros((NB_WORDS, embedding_dim), dtype=np.float32)\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i-1] = embedding_vector\n",
    "print(\"embedding matrix size: is [%d,%d]\" % (word_embedding_matrix.shape[0], \n",
    "                                            word_embedding_matrix.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(df_balanced.text.values)\n",
    "# Pad the reviews so that they are all of the same length\n",
    "padReviews = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "reviewsLength = max_sequence_length\n",
    "\n",
    "# Split the ratings into OneHot representation\n",
    "ratings = df_balanced.stars.values.astype(int)\n",
    "ratings_cat = to_categorical(ratings)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padReviews, ratings_cat, test_size=0.2, random_state=9)\n",
    "with pd.HDFStore('x_y_test_train.h5') as h:\n",
    "    h['X_train'] = pd.DataFrame(X_train)\n",
    "    h['X_test'] = pd.DataFrame(X_test)\n",
    "    h['y_train'] = pd.DataFrame(y_train)\n",
    "    h['y_test'] = pd.DataFrame(y_test)\n",
    "\n",
    "assert padReviews.shape[0] == ratings.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "(45380, 6)\n",
      "(36304, 700)\n",
      "(36304, 6)\n"
     ]
    }
   ],
   "source": [
    "print(type(ratings_cat))\n",
    "print(type(X_train))\n",
    "print(ratings_cat.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "if alreadyPickled == False:\n",
    "    \n",
    "    picklefiles(\"./data/good_pickles/balanced_reviews100k.p\",df_balanced)\n",
    "    picklefiles(\"./data/good_pickles/category_ratings100k.p\",ratings_cat)\n",
    "    picklefiles(\"./data/good_pickles/word_embedding_matrix100k.p\",word_embedding_matrix)\n",
    "    picklefiles(\"./data/good_pickles/tokenizer100k.p\", tokenizer)\n",
    "    \n",
    "if alreadyPickled == True:\n",
    "    \n",
    "    word_embedding_matrix = loadfiles(\"./data/word_embedding_matrix.p\")\n",
    "    df_rev_balanced = pd.read_csv('balanced_reviews.csv')\n",
    "    ratings_cat = loadfiles(\"./data/good_pickles/category_ratings.p\")\n",
    "    balanced_reviews = loadfiles(\"./data/good_pickles/balanced_reviews.p\")\n",
    "    tokenizer = loadfiles('tokenizer.pickle')\n",
    "    with pd.HDFStore('x_y_test_train.h5') as h:\n",
    "        X_train = h['X_train'].values\n",
    "        X_test = h['X_test'].values\n",
    "        y_train = h['y_train'].values\n",
    "        y_test = h['y_test'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Batches\n",
    "Gets batches. These will be called later to then fill our X and y placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size):\n",
    "    assert isinstance(batch_size, int)\n",
    "    assert isinstance(x, np.ndarray) and isinstance(y, np.ndarray)\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size,:], y[ii:ii+batch_size]\n",
    "        \n",
    "def get_test_batches(x, batch_size):\n",
    "    \n",
    "    assert isinstance(batch_size, int)\n",
    "    assert isinstance(x, np.ndarray)\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x = x[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 850)\n",
      "(128, 6)\n"
     ]
    }
   ],
   "source": [
    "# Display some data vectors here and their type\n",
    "for batch_i, (trainX_batch, trainY_batch) in enumerate(get_batches(X_train, y_train, batch_size)):\n",
    "    print(trainX_batch.shape)\n",
    "    print(trainY_batch.shape)\n",
    "    if batch_i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Graph\n",
    "Here we start building our computational graph. We use a 2 layer GRU Recurrent Neural Network. We define placeholders for learning rate and dropout since these are variables that we could potentially want to vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    # Should be [batch_size x review length]\n",
    "#     with tf.name_scope(\"input_data\"):\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "#         tf.summary.scalar('input_data', input_data)\n",
    "        \n",
    "    # Should be [batch_size x num_classes]\n",
    "#     with tf.name_scope(\"labels\"):\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "#         tf.summary.scalar('labels', labels)\n",
    "        \n",
    "#     with tf.name_scope(\"lr\"):    \n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "#         tf.summary.scalar(\"lr\", labels)\n",
    "    \n",
    "#     with tf.name_scope(\"keep_prob\"):\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "#         tf.summary.scalar(\"keep_prob\", keep_prob)\n",
    "\n",
    "    return input_data, labels, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_axis_1(data, ind):\n",
    "    \"\"\"\n",
    "    Get specified elements along the first axis of tensor.\n",
    "    :param data: Tensorflow tensor that will be subsetted.\n",
    "    :param ind: Indices to take (one for each element along axis 0 of data).\n",
    "    :return: Subsetted tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_range = tf.range(tf.shape(data)[0])\n",
    "    indices = tf.stack([batch_range, ind], axis=1)\n",
    "    res = tf.gather_nd(data, indices)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input Data\n",
      "Tensor(\"Shape:0\", shape=(2,), dtype=int32)\n",
      "Graph is built.\n",
      "./graph\n"
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        input_data, labels, lr, keep_prob = model_inputs()\n",
    "    \n",
    "    print(\"Shape of Input Data\")\n",
    "    print(tf.shape(input_data))\n",
    "    embeddings = word_embedding_matrix\n",
    "    embs = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "\n",
    "    with tf.name_scope(\"RNN_Layers\"):\n",
    "        \n",
    "        stacked_rnn = []\n",
    "        for layer in range(num_layers):\n",
    "            cell_fw = tf.contrib.rnn.GRUCell(rnn_size)\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw,\n",
    "                                                    output_keep_prob=keep_prob)\n",
    "            stacked_rnn.append(cell_fw)\n",
    "        multilayer_cell = tf.contrib.rnn.MultiRNNCell(stacked_rnn, state_is_tuple=True)\n",
    "\n",
    "        \n",
    "    with tf.name_scope(\"init_state\"):\n",
    "        initial_state = multilayer_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    with tf.name_scope(\"Forward_Pass\"):\n",
    "        output, final_state = tf.nn.dynamic_rnn(multilayer_cell,\n",
    "                                           embs,\n",
    "                                           dtype=tf.float32)\n",
    "#     with tf.name_scope(\"FC_Layer\"):\n",
    "#         dense = tf.contrib.layers.fully_connected(output[:,-1],\n",
    "#                                                   num_outputs = 64,\n",
    "#                                                   activation_fn = tf.sigmoid,\n",
    "#                                                   weights_initializer = tf.random_normal_initializer())\n",
    "        \n",
    "#         dense = tf.contrib.layers.dropout(dense, keep_prob)\n",
    "    with tf.name_scope(\"Predictions\"):\n",
    "        weight = tf.Variable(tf.truncated_normal([rnn_size, num_classes]))\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "#         value = tf.transpose(output, [1, 0, 2])\n",
    "#         last = extract_axis_1(output, rnn_size - 1)\n",
    "        last = output[:, -1, :]\n",
    "#         last = final_state.output #tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "        \n",
    "        predictions = (tf.matmul(last, weight) + bias)\n",
    "        \n",
    "        \n",
    "#         predictions = tf.contrib.layers.fully_connected(output[:,-1],\n",
    "#                                                         num_outputs = num_classes, \n",
    "#                                                         activation_fn=tf.nn.sigmoid,\n",
    "#                                                         weights_initializer = tf.random_normal_initializer())\n",
    "#         predictions = tf.layers.dense(dense,\n",
    "#                                      units = num_classes,\n",
    "#                                      activation=tf.nn.sigmoid,\n",
    "#                                      trainable=True)\n",
    "#         predictions = tf.one_hot(tf.argmax(predictions, axis=1), depth=num_classes)\n",
    "        tf.summary.histogram('predictions', predictions)\n",
    "        \n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=labels))        \n",
    "#         cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=labels))\n",
    "        tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    # Optimizer \n",
    "    with tf.name_scope('train'):    \n",
    "        optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    # Predictions comes out as 6 output layer, so need to \"change\" to one hot\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correctPred = tf.equal(tf.argmax(predictions,1), tf.argmax(labels,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "        \n",
    "#         accuracy = tf.metrics.accuracy(tf.argmax(labels,1), tf.argmax(predictions,1))\n",
    "#         correct = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))\n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        \n",
    "#         correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels)\n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    export_nodes = ['input_data', 'labels', 'keep_prob', 'lr', 'initial_state', 'final_state',\n",
    "                    'accuracy', 'predictions', 'cost', 'optimizer', 'merged']\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "print(\"Graph is built.\")\n",
    "graph_location = \"./graph\"\n",
    "\n",
    "Graph = namedtuple('train_graph', export_nodes)\n",
    "local_dict = locals()\n",
    "graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "print(graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(train_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape_1:0\", shape=(3,), dtype=int32)\n",
      "Tensor(\"Shape_2:0\", shape=(3,), dtype=int32)\n",
      "Tensor(\"Shape_3:0\", shape=(2,), dtype=int32)\n",
      "[[ 0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.  0.  0.]]\n",
      "<dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(tf.shape(output))\n",
    "print(tf.shape(embs))\n",
    "print(tf.shape(predictions))\n",
    "print(y_train[0:10])\n",
    "print(predictions.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 10 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 5 # Make 3 update checks per epoch\n",
    "update_check = (len(X_train)//batch_size//per_epoch)-1\n",
    "keep_probability = 0.75\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "I've been keeping track of the tensorboard summaries so it'll allow me to visualize the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "Finished first\n",
      "Epoch   1/2 Batch   10/567 - Loss:  1.703, Acc:  0.203, Seconds: 17.46\n",
      "Epoch   1/2 Batch   20/567 - Loss:  1.654, Acc:  0.219, Seconds: 16.31\n",
      "Epoch   1/2 Batch   30/567 - Loss:  1.656, Acc:  0.188, Seconds: 15.64\n",
      "Epoch   1/2 Batch   40/567 - Loss:  1.614, Acc:  0.250, Seconds: 18.95\n",
      "Epoch   1/2 Batch   50/567 - Loss:  1.613, Acc:  0.156, Seconds: 15.84\n",
      "Epoch   1/2 Batch   60/567 - Loss:  1.592, Acc:  0.250, Seconds: 18.26\n",
      "Epoch   1/2 Batch   70/567 - Loss:  1.605, Acc:  0.188, Seconds: 20.38\n",
      "Epoch   1/2 Batch   80/567 - Loss:  1.596, Acc:  0.188, Seconds: 22.25\n",
      "Epoch   1/2 Batch   90/567 - Loss:  1.543, Acc:  0.328, Seconds: 16.40\n",
      "Epoch   1/2 Batch  100/567 - Loss:  1.503, Acc:  0.281, Seconds: 16.03\n",
      "Epoch   1/2 Batch  110/567 - Loss:  1.476, Acc:  0.406, Seconds: 23.00\n",
      "Average loss for this update: 1.594\n",
      "New Record!\n",
      "Epoch   1/2 Batch  120/567 - Loss:  1.505, Acc:  0.312, Seconds: 16.86\n",
      "Epoch   1/2 Batch  130/567 - Loss:  1.532, Acc:  0.438, Seconds: 15.74\n",
      "Epoch   1/2 Batch  140/567 - Loss:  1.510, Acc:  0.344, Seconds: 27.04\n",
      "Epoch   1/2 Batch  150/567 - Loss:  1.450, Acc:  0.297, Seconds: 18.86\n",
      "Epoch   1/2 Batch  160/567 - Loss:  1.454, Acc:  0.328, Seconds: 16.24\n",
      "Epoch   1/2 Batch  170/567 - Loss:  1.394, Acc:  0.469, Seconds: 18.48\n",
      "Epoch   1/2 Batch  180/567 - Loss:  1.465, Acc:  0.344, Seconds: 15.54\n",
      "Epoch   1/2 Batch  190/567 - Loss:  1.448, Acc:  0.312, Seconds: 15.51\n",
      "Epoch   1/2 Batch  200/567 - Loss:  1.431, Acc:  0.312, Seconds: 15.80\n",
      "Epoch   1/2 Batch  210/567 - Loss:  1.367, Acc:  0.438, Seconds: 15.52\n",
      "Epoch   1/2 Batch  220/567 - Loss:  1.372, Acc:  0.438, Seconds: 15.70\n",
      "Average loss for this update: 1.445\n",
      "New Record!\n",
      "Epoch   1/2 Batch  230/567 - Loss:  1.384, Acc:  0.344, Seconds: 15.49\n",
      "Epoch   1/2 Batch  240/567 - Loss:  1.382, Acc:  0.500, Seconds: 15.61\n",
      "Epoch   1/2 Batch  250/567 - Loss:  1.382, Acc:  0.391, Seconds: 15.51\n",
      "Epoch   1/2 Batch  260/567 - Loss:  1.406, Acc:  0.312, Seconds: 16.05\n",
      "Epoch   1/2 Batch  270/567 - Loss:  1.351, Acc:  0.359, Seconds: 15.51\n",
      "Epoch   1/2 Batch  280/567 - Loss:  1.373, Acc:  0.422, Seconds: 15.63\n",
      "Epoch   1/2 Batch  290/567 - Loss:  1.346, Acc:  0.328, Seconds: 18.80\n",
      "Epoch   1/2 Batch  300/567 - Loss:  1.371, Acc:  0.406, Seconds: 17.47\n",
      "Epoch   1/2 Batch  310/567 - Loss:  1.366, Acc:  0.438, Seconds: 16.39\n",
      "Epoch   1/2 Batch  320/567 - Loss:  1.337, Acc:  0.422, Seconds: 15.89\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"./saves/best_model_100k.ckpt\" \n",
    "if IS_TRAINING:\n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #     loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "    #     loader.restore(sess, checkpoint)\n",
    "\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('./summaries' + '/train' + '/100k/run7/', sess.graph)\n",
    "\n",
    "        for epoch_i in range(1, epochs+1):\n",
    "            state = sess.run(graph.initial_state)\n",
    "\n",
    "            update_loss = 0\n",
    "            batch_loss = 0\n",
    "\n",
    "            for batch_i, (x, y) in enumerate(get_batches(X_train, y_train, batch_size), 1):\n",
    "                if batch_i == 1:\n",
    "                    print(\"Starting\")\n",
    "                feed = {graph.input_data: x,\n",
    "                        graph.labels: y,\n",
    "                        graph.keep_prob: keep_probability,\n",
    "                        graph.initial_state: state}\n",
    "                start_time = time.time()\n",
    "                summary, loss, acc, state, _ = sess.run([graph.merged, \n",
    "                                                         graph.cost, \n",
    "                                                         graph.accuracy, \n",
    "                                                         graph.final_state, \n",
    "                                                         graph.optimizer], \n",
    "                                                        feed_dict=feed)\n",
    "                if batch_i == 1:\n",
    "                    print(\"Finished first\")\n",
    "\n",
    "                train_writer.add_summary(summary, epoch_i*batch_i + batch_i)\n",
    "\n",
    "                batch_loss += loss\n",
    "                update_loss += loss\n",
    "                end_time = time.time()\n",
    "                batch_time = end_time - start_time\n",
    "\n",
    "                if batch_i % display_step == 0 and batch_i > 0:\n",
    "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Acc: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(epoch_i,\n",
    "                                  epochs, \n",
    "                                  batch_i, \n",
    "                                  len(X_train) // batch_size, \n",
    "                                  batch_loss / display_step,\n",
    "                                  acc,\n",
    "                                  batch_time*display_step))\n",
    "                    batch_loss = 0\n",
    "\n",
    "                if batch_i % update_check == 0 and batch_i > 0:\n",
    "                    print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                    summary_update_loss.append(update_loss)\n",
    "\n",
    "                    # If the update loss is at a new minimum, save the model\n",
    "                    if update_loss <= min(summary_update_loss):\n",
    "                        print('New Record!') \n",
    "                        stop_early = 0\n",
    "                        saver = tf.train.Saver() \n",
    "                        saver.save(sess, checkpoint)\n",
    "\n",
    "                    else:\n",
    "                        print(\"No Improvement.\")\n",
    "                        stop_early += 1\n",
    "                        if stop_early == stop:\n",
    "                            break\n",
    "                    update_loss = 0\n",
    "\n",
    "\n",
    "            # Reduce learning rate, but not below its minimum value\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "\n",
    "            if stop_early == stop:\n",
    "                print(\"Stopping Training.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Data\n",
    "This part of the code is allocated to testing the data. On top of recording accuracy results, I also generate a confusion matrix. Since reviews are subjective and aren't concretely one rating or another, a confusion matrix helps visualize your results a lot better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if IS_TESTING:\n",
    "    \n",
    "    y_true = y_test.argmax(axis=1)\n",
    "    checkpoint = \"./saves/best_model_50k.ckpt\"  \n",
    "    \n",
    "    all_preds = []\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        # Load the model\n",
    "        saver.restore(sess, checkpoint)\n",
    "        test_state = sess.run(graph.initial_state)\n",
    "\n",
    "        for _, x in enumerate(get_test_batches(x_test, batch_size), 1):\n",
    "            feed = {graph.input_data: x,\n",
    "                    graph.keep_prob: keep_probability,\n",
    "                    graph.initial_state: state}\n",
    "\n",
    "            predictions = sess.run(graph.predictions, feed_dict=feed)\n",
    "            for pred in predictions:\n",
    "                all_preds.append(float(pred))\n",
    "                \n",
    "    y_predictions = preds.argmax(axis=1)\n",
    "    cm = ConfusionMatrix(y_true, y_predictions)\n",
    "    cm.plot(backend='seaborn', normalized=True)\n",
    "    plt.title('Confusion Matrix Stars prediction')\n",
    "    plt.figure(figsize=(12, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
