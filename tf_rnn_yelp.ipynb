{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Review Prediction\n",
    "Objective: Construct and train a Neural Network that would be able to predict the number of star ratings from a Yelp review.    \n",
    "Dataset used: https://www.yelp.com/dataset/challenge  \n",
    "\n",
    "Steps:  \n",
    "1) Data Preprocessing  \n",
    "2) Deep Learning Preprocessing  \n",
    "3) Network Training  \n",
    "4) Network Testing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "from langdetect import detect\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_ml import ConfusionMatrix\n",
    "import seaborn\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import nltk, re, time\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import namedtuple\n",
    "from contractions import get_contractions\n",
    "\n",
    "import operator\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "alreadyPickled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "num_layers = 2\n",
    "num_classes = 6\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "rnn_size = 64\n",
    "num_layers = 2\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "keep_probability = 0.8\n",
    "max_sequence_length = 750\n",
    "\n",
    "IS_TRAINING = True\n",
    "IS_TESTING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "If the data is already pickled, then can skip embedding and data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picklefiles(filename, stuff):\n",
    "    save_stuff = open(filename, \"wb\")\n",
    "    pickle.dump(stuff, save_stuff)\n",
    "    save_stuff.close()\n",
    "def loadfiles(filename):\n",
    "    saved_stuff = open(filename,\"rb\")\n",
    "    stuff = pickle.load(saved_stuff)\n",
    "    saved_stuff.close()\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "The following few cells preprocess the data for me. Clean test defines a function where it removes stop_words (found here https://gist.github.com/sebleier/554280). These words typically have no beneficial meaning to any reviews and are thus wasted features. I also strip punctuation and turn everything lower case. These procedures can be considered pretty standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, contractions):\n",
    "    \n",
    "    text = text.lower()    \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 69047 restaurants\n"
     ]
    }
   ],
   "source": [
    "restId = []\n",
    "for line in open('./data/dataset/business.json', 'r'):\n",
    "    data = json.loads(line)\n",
    "    if 'Restaurants' in data['categories'] or 'Food' in data['categories']:\n",
    "        restId.append(data['business_id'])\n",
    "print(\"There are %d restaurants\" % (len(restId)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing the reviews and star ratings\n",
    "Here, I narrow down my reviews to english only reviews. I do this since restaurants make up around 60% of the yelp reviews. This way review types and wording may stay relatively similar for more accurate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 499\n",
      "1000 999\n",
      "1500 1499\n",
      "2000 1999\n",
      "2500 2499\n",
      "3000 2999\n",
      "3500 3499\n",
      "4000 3999\n",
      "4500 4499\n",
      "5000 4999\n",
      "5500 5499\n",
      "6000 5999\n",
      "6500 6499\n",
      "7000 6999\n",
      "7500 7499\n",
      "8000 7999\n",
      "8500 8499\n",
      "9000 8999\n",
      "9500 9499\n",
      "10000 9999\n",
      "10500 10499\n",
      "11000 10999\n",
      "11500 11499\n",
      "12000 11999\n",
      "12500 12499\n",
      "13000 12999\n",
      "13500 13499\n",
      "14000 13999\n",
      "14500 14499\n",
      "15000 14999\n",
      "15500 15499\n",
      "16000 15999\n",
      "16500 16499\n",
      "17000 16999\n",
      "17500 17499\n",
      "18000 17999\n",
      "18500 18499\n",
      "19000 18999\n",
      "19500 19499\n",
      "20000 19999\n",
      "20500 20499\n",
      "21000 20999\n",
      "21500 21499\n",
      "22000 21999\n",
      "22500 22499\n",
      "23000 22999\n",
      "23500 23499\n",
      "24000 23999\n",
      "24500 24499\n",
      "25000 24999\n",
      "25500 25499\n",
      "26000 25999\n",
      "26500 26499\n",
      "27000 26999\n",
      "27500 27499\n",
      "28000 27999\n",
      "28500 28499\n",
      "29000 28999\n",
      "29500 29499\n",
      "30000 29999\n",
      "30500 30499\n",
      "31000 30999\n",
      "31500 31499\n",
      "32000 31999\n",
      "32500 32499\n",
      "33000 32999\n",
      "33500 33499\n",
      "34000 33999\n",
      "34500 34499\n",
      "35000 34999\n",
      "35500 35499\n",
      "36000 35999\n",
      "36500 36499\n",
      "37000 36999\n",
      "37500 37499\n",
      "38000 37999\n",
      "38500 38499\n",
      "39000 38999\n",
      "39500 39499\n",
      "40000 39999\n",
      "40500 40499\n",
      "41000 40999\n",
      "41500 41499\n",
      "42000 41999\n",
      "42500 42499\n",
      "43000 42999\n",
      "43500 43499\n",
      "44000 43999\n",
      "44500 44499\n",
      "45000 44999\n",
      "45500 45499\n",
      "46000 45999\n",
      "46500 46499\n",
      "47000 46999\n",
      "47500 47499\n",
      "48000 47999\n",
      "48500 48499\n",
      "49000 48999\n",
      "49500 49499\n",
      "50000 49999\n",
      "50500 50499\n",
      "51000 50999\n",
      "51500 51499\n",
      "52000 51999\n",
      "52500 52499\n",
      "53000 52999\n",
      "53500 53499\n",
      "54000 53999\n",
      "54500 54499\n",
      "55000 54999\n",
      "55500 55499\n",
      "56000 55999\n",
      "56500 56499\n",
      "57000 56999\n",
      "57500 57499\n",
      "58000 57999\n",
      "58500 58499\n",
      "59000 58999\n",
      "59500 59499\n",
      "60000 59999\n",
      "60500 60499\n",
      "61000 60999\n",
      "61500 61499\n",
      "62000 61999\n",
      "62500 62499\n",
      "63000 62999\n",
      "63500 63499\n",
      "64000 63999\n",
      "64500 64499\n",
      "65000 64999\n",
      "65500 65499\n",
      "66000 65999\n",
      "66500 66499\n",
      "67000 66999\n",
      "67500 67499\n",
      "68000 67999\n",
      "68500 68499\n",
      "69000 68999\n",
      "69500 69499\n",
      "70000 69999\n",
      "70500 70499\n",
      "71000 70999\n",
      "71500 71499\n",
      "72000 71999\n",
      "72500 72499\n",
      "73000 72999\n",
      "73500 73499\n",
      "74000 73999\n",
      "74500 74499\n",
      "75000 74999\n",
      "75500 75499\n",
      "76000 75999\n",
      "76500 76499\n",
      "77000 76999\n",
      "77500 77499\n",
      "78000 77999\n",
      "78500 78499\n",
      "79000 78999\n",
      "79500 79499\n",
      "80000 79999\n",
      "80500 80499\n",
      "81000 80999\n",
      "81500 81499\n",
      "82000 81999\n",
      "82500 82499\n",
      "83000 82999\n",
      "83500 83499\n",
      "84000 83999\n",
      "84500 84499\n",
      "85000 84999\n",
      "85500 85499\n",
      "86000 85999\n",
      "86500 86499\n",
      "87000 86999\n",
      "87500 87499\n",
      "88000 87999\n",
      "88500 88499\n",
      "89000 88999\n",
      "89500 89499\n",
      "90000 89999\n",
      "90500 90499\n",
      "91000 90999\n",
      "91500 91499\n",
      "92000 91999\n",
      "92500 92499\n",
      "93000 92999\n",
      "93500 93499\n",
      "94000 93999\n",
      "94500 94499\n",
      "95000 94999\n",
      "95500 95499\n",
      "96000 95999\n",
      "96500 96499\n",
      "97000 96999\n",
      "97500 97499\n",
      "98000 97999\n",
      "98500 98499\n",
      "99000 98999\n",
      "99500 99499\n",
      "100000 99999\n",
      "100500 100499\n",
      "101000 100999\n",
      "101500 101499\n",
      "102000 101999\n",
      "102500 102499\n",
      "103000 102999\n",
      "103500 103499\n",
      "104000 103999\n",
      "104500 104499\n",
      "105000 104999\n",
      "105500 105499\n",
      "106000 105999\n",
      "106500 106499\n",
      "107000 106999\n",
      "107500 107499\n",
      "108000 107999\n",
      "108500 108499\n",
      "109000 108999\n",
      "109500 109499\n",
      "110000 109999\n",
      "110500 110499\n",
      "111000 110999\n",
      "111500 111499\n",
      "112000 111999\n",
      "112500 112499\n",
      "113000 112999\n",
      "113500 113499\n",
      "114000 113999\n",
      "114500 114499\n",
      "115000 114999\n",
      "115500 115499\n",
      "116000 115999\n",
      "116500 116499\n",
      "117000 116999\n",
      "117500 117499\n",
      "118000 117999\n",
      "118500 118499\n",
      "119000 118999\n",
      "119500 119499\n",
      "120000 119999\n",
      "120500 120499\n",
      "121000 120999\n",
      "121500 121499\n",
      "122000 121999\n",
      "122500 122499\n",
      "123000 122999\n",
      "123500 123499\n",
      "124000 123999\n",
      "124500 124499\n",
      "125000 124999\n",
      "125500 125499\n",
      "126000 125999\n",
      "126500 126499\n",
      "127000 126999\n",
      "127500 127499\n",
      "128000 127999\n",
      "128500 128499\n",
      "129000 128999\n",
      "129500 129499\n",
      "130000 129999\n",
      "130500 130499\n",
      "131000 130999\n",
      "131500 131499\n",
      "132000 131999\n",
      "132500 132499\n",
      "133000 132999\n",
      "133500 133499\n",
      "134000 133999\n",
      "134500 134499\n",
      "135000 134999\n",
      "135500 135499\n",
      "136000 135999\n",
      "136500 136499\n",
      "137000 136999\n",
      "137500 137499\n",
      "138000 137999\n",
      "138500 138499\n",
      "139000 138999\n",
      "139500 139499\n",
      "140000 139999\n",
      "140500 140499\n",
      "141000 140999\n",
      "141500 141499\n",
      "142000 141999\n",
      "142500 142499\n",
      "143000 142999\n",
      "143500 143499\n",
      "144000 143999\n",
      "144500 144499\n",
      "145000 144999\n",
      "145500 145499\n",
      "146000 145999\n",
      "146500 146499\n",
      "147000 146999\n",
      "147500 147499\n",
      "148000 147999\n",
      "148500 148499\n",
      "149000 148999\n",
      "149500 149499\n",
      "150000 149999\n",
      "150500 150499\n",
      "151000 150999\n",
      "151500 151499\n",
      "152000 151999\n",
      "152500 152499\n",
      "153000 152999\n",
      "153500 153499\n",
      "154000 153999\n",
      "154500 154499\n",
      "155000 154999\n",
      "155500 155499\n",
      "156000 155999\n",
      "156500 156499\n",
      "157000 156999\n",
      "157500 157499\n",
      "158000 157999\n",
      "158500 158499\n",
      "159000 158999\n",
      "159500 159499\n",
      "160000 159999\n",
      "160500 160499\n",
      "161000 160999\n",
      "161500 161499\n",
      "162000 161999\n",
      "162500 162499\n",
      "163000 162999\n",
      "163500 163499\n",
      "164000 163999\n",
      "164500 164499\n",
      "165000 164999\n",
      "165500 165499\n",
      "166000 165999\n",
      "166500 166499\n",
      "167000 166999\n",
      "167500 167499\n",
      "168000 167999\n",
      "168500 168499\n",
      "169000 168999\n",
      "169500 169499\n",
      "170000 169999\n",
      "170500 170499\n",
      "171000 170999\n",
      "171500 171499\n",
      "172000 171999\n",
      "172500 172499\n",
      "173000 172999\n",
      "173500 173499\n",
      "174000 173999\n",
      "174500 174499\n",
      "175000 174999\n",
      "175500 175499\n",
      "176000 175999\n",
      "176500 176499\n",
      "177000 176999\n",
      "177500 177499\n",
      "178000 177999\n",
      "178500 178499\n",
      "179000 178999\n",
      "179500 179499\n",
      "180000 179999\n",
      "180500 180499\n",
      "181000 180999\n",
      "181500 181499\n",
      "182000 181999\n",
      "182500 182499\n",
      "183000 182999\n",
      "183500 183499\n",
      "184000 183999\n",
      "184500 184499\n",
      "185000 184999\n",
      "185500 185499\n",
      "186000 185999\n",
      "186500 186499\n",
      "187000 186999\n",
      "187500 187499\n",
      "188000 187999\n",
      "188500 188499\n",
      "189000 188999\n",
      "189500 189499\n",
      "190000 189999\n",
      "190500 190499\n",
      "191000 190999\n",
      "191500 191499\n",
      "192000 191999\n",
      "192500 192499\n",
      "193000 192999\n",
      "193500 193499\n",
      "194000 193999\n",
      "194500 194499\n",
      "195000 194999\n",
      "195500 195499\n",
      "196000 195999\n",
      "196500 196499\n",
      "197000 196999\n",
      "197500 197499\n",
      "198000 197999\n",
      "198500 198499\n",
      "199000 198999\n",
      "199500 199499\n",
      "200000 199999\n",
      "200500 200499\n",
      "201000 200999\n",
      "201500 201499\n",
      "202000 201999\n",
      "202500 202499\n",
      "203000 202999\n",
      "203500 203499\n",
      "204000 203999\n",
      "204500 204499\n",
      "205000 204999\n",
      "205500 205499\n",
      "206000 205999\n",
      "206500 206499\n",
      "207000 206999\n",
      "207500 207499\n",
      "208000 207999\n",
      "208500 208499\n",
      "209000 208999\n",
      "209500 209499\n",
      "210000 209999\n",
      "210500 210499\n",
      "211000 210999\n",
      "211500 211499\n",
      "212000 211999\n",
      "212500 212499\n",
      "213000 212999\n",
      "213500 213499\n",
      "214000 213999\n",
      "214500 214499\n",
      "215000 214999\n",
      "215500 215499\n",
      "216000 215999\n",
      "216500 216499\n",
      "217000 216999\n",
      "217500 217499\n",
      "218000 217999\n",
      "218500 218499\n",
      "219000 218999\n",
      "219500 219499\n",
      "220000 219999\n",
      "220500 220499\n",
      "221000 220999\n",
      "221500 221499\n",
      "222000 221999\n",
      "222500 222499\n",
      "223000 222999\n",
      "223500 223499\n",
      "224000 223999\n",
      "224500 224499\n",
      "225000 224999\n",
      "225500 225499\n",
      "226000 225999\n",
      "226500 226499\n",
      "227000 226999\n",
      "227500 227499\n",
      "228000 227999\n",
      "228500 228499\n",
      "229000 228999\n",
      "229500 229499\n",
      "230000 229999\n",
      "230500 230499\n",
      "231000 230999\n",
      "231500 231499\n",
      "232000 231999\n",
      "232500 232499\n",
      "233000 232999\n",
      "233500 233499\n",
      "234000 233999\n",
      "234500 234499\n",
      "235000 234999\n",
      "235500 235499\n",
      "236000 235999\n",
      "236500 236499\n",
      "237000 236999\n",
      "237500 237499\n",
      "238000 237999\n",
      "238500 238499\n",
      "239000 238999\n",
      "239500 239499\n",
      "240000 239999\n",
      "240500 240499\n",
      "241000 240999\n",
      "241500 241499\n",
      "242000 241999\n",
      "242500 242499\n",
      "243000 242999\n",
      "243500 243499\n",
      "244000 243999\n",
      "244500 244499\n",
      "245000 244999\n",
      "245500 245499\n",
      "246000 245999\n",
      "246500 246499\n",
      "247000 246999\n",
      "247500 247499\n",
      "248000 247999\n",
      "248500 248499\n",
      "249000 248999\n",
      "249500 249499\n",
      "250000 249999\n",
      "250500 250499\n",
      "251000 250999\n",
      "251500 251499\n",
      "252000 251999\n",
      "252500 252499\n",
      "253000 252999\n",
      "253500 253499\n",
      "254000 253999\n",
      "254500 254499\n",
      "255000 254999\n",
      "255500 255499\n",
      "256000 255999\n",
      "256500 256499\n",
      "257000 256999\n",
      "257500 257499\n",
      "258000 257999\n",
      "258500 258499\n",
      "259000 258999\n",
      "259500 259499\n",
      "260000 259999\n",
      "260500 260499\n",
      "261000 260999\n",
      "261500 261499\n",
      "262000 261999\n",
      "262500 262499\n",
      "263000 262999\n",
      "263500 263499\n",
      "264000 263999\n",
      "264500 264499\n",
      "265000 264999\n",
      "265500 265499\n",
      "266000 265999\n",
      "266500 266499\n",
      "267000 266999\n",
      "267500 267499\n",
      "268000 267999\n",
      "268500 268499\n",
      "269000 268999\n",
      "269500 269499\n",
      "270000 269999\n",
      "270500 270499\n",
      "271000 270999\n",
      "271500 271499\n",
      "272000 271999\n",
      "272500 272499\n",
      "273000 272999\n",
      "273500 273499\n",
      "274000 273999\n",
      "274500 274499\n",
      "275000 274999\n",
      "275500 275499\n",
      "276000 275999\n",
      "276500 276499\n",
      "277000 276999\n",
      "277500 277499\n",
      "278000 277999\n",
      "278500 278499\n",
      "279000 278999\n",
      "279500 279499\n",
      "280000 279999\n",
      "280500 280499\n",
      "281000 280999\n",
      "281500 281499\n",
      "282000 281999\n",
      "282500 282499\n",
      "283000 282999\n",
      "283500 283499\n",
      "284000 283999\n",
      "284500 284499\n",
      "285000 284999\n",
      "285500 285499\n",
      "286000 285999\n",
      "286500 286499\n",
      "287000 286999\n",
      "287500 287499\n",
      "288000 287999\n",
      "288500 288499\n",
      "289000 288999\n",
      "289500 289499\n",
      "290000 289999\n",
      "290500 290499\n",
      "291000 290999\n",
      "291500 291499\n",
      "292000 291999\n",
      "292500 292499\n",
      "293000 292999\n",
      "293500 293499\n",
      "294000 293999\n",
      "294500 294499\n",
      "295000 294999\n",
      "295500 295499\n",
      "296000 295999\n",
      "296500 296499\n",
      "297000 296999\n",
      "297500 297499\n",
      "298000 297999\n",
      "298500 298499\n",
      "299000 298999\n",
      "299500 299499\n",
      "300000 299999\n",
      "300500 300499\n",
      "301000 300999\n",
      "301500 301499\n",
      "302000 301999\n",
      "302500 302499\n",
      "303000 302999\n",
      "303500 303499\n",
      "304000 303999\n",
      "304500 304499\n",
      "305000 304999\n",
      "305500 305499\n",
      "306000 305999\n",
      "306500 306499\n",
      "307000 306999\n",
      "307500 307499\n",
      "308000 307999\n",
      "308500 308499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309000 308999\n",
      "309500 309499\n",
      "310000 309999\n",
      "310500 310499\n",
      "311000 310999\n",
      "311500 311499\n",
      "312000 311999\n",
      "312500 312499\n",
      "313000 312999\n",
      "313500 313499\n",
      "314000 313999\n",
      "314500 314499\n",
      "315000 314999\n",
      "315500 315499\n",
      "316000 315999\n",
      "316500 316499\n",
      "317000 316999\n",
      "317500 317499\n",
      "318000 317999\n",
      "318500 318499\n",
      "319000 318999\n",
      "319500 319499\n",
      "320000 319999\n",
      "320500 320499\n",
      "321000 320999\n",
      "321500 321499\n",
      "322000 321999\n",
      "322500 322499\n",
      "323000 322999\n",
      "323500 323499\n",
      "324000 323999\n",
      "324500 324499\n",
      "325000 324999\n",
      "325500 325499\n",
      "326000 325999\n",
      "326500 326499\n",
      "327000 326999\n",
      "327500 327499\n",
      "328000 327999\n",
      "328500 328499\n",
      "329000 328999\n",
      "329500 329499\n",
      "330000 329999\n",
      "330500 330499\n",
      "331000 330999\n",
      "331500 331499\n",
      "332000 331999\n",
      "332500 332499\n",
      "333000 332999\n",
      "333500 333499\n",
      "334000 333999\n",
      "334500 334499\n",
      "335000 334999\n",
      "335500 335499\n",
      "336000 335999\n",
      "336500 336499\n",
      "337000 336999\n",
      "337500 337499\n",
      "338000 337999\n",
      "338500 338499\n",
      "339000 338999\n",
      "339500 339499\n",
      "340000 339999\n",
      "340500 340499\n",
      "341000 340999\n",
      "341500 341499\n",
      "342000 341999\n",
      "342500 342499\n",
      "343000 342999\n",
      "343500 343499\n",
      "344000 343999\n",
      "344500 344499\n",
      "345000 344999\n",
      "345500 345499\n",
      "346000 345999\n",
      "346500 346499\n",
      "347000 346999\n",
      "347500 347499\n",
      "348000 347999\n",
      "348500 348499\n",
      "349000 348999\n",
      "349500 349499\n",
      "350000 349999\n",
      "350500 350499\n",
      "351000 350999\n",
      "351500 351499\n",
      "352000 351999\n",
      "352500 352499\n",
      "353000 352999\n",
      "353500 353499\n",
      "354000 353999\n",
      "354500 354499\n",
      "355000 354999\n",
      "355500 355499\n",
      "356000 355999\n",
      "356500 356499\n",
      "357000 356999\n",
      "357500 357499\n",
      "358000 357999\n",
      "358500 358499\n",
      "359000 358999\n",
      "359500 359499\n",
      "360000 359999\n",
      "360500 360499\n",
      "361000 360999\n",
      "361500 361499\n",
      "362000 361999\n",
      "362500 362499\n",
      "363000 362999\n",
      "363500 363499\n",
      "364000 363999\n",
      "364500 364499\n",
      "365000 364999\n",
      "365500 365499\n",
      "366000 365999\n",
      "366500 366499\n",
      "367000 366999\n",
      "367500 367499\n",
      "368000 367999\n",
      "368500 368499\n",
      "369000 368999\n",
      "369500 369499\n",
      "370000 369999\n",
      "370500 370499\n",
      "371000 370999\n",
      "371500 371499\n",
      "372000 371999\n",
      "372500 372499\n",
      "373000 372999\n",
      "373500 373499\n",
      "374000 373999\n",
      "374500 374499\n",
      "375000 374999\n",
      "375500 375499\n",
      "376000 375999\n",
      "376500 376499\n",
      "377000 376999\n",
      "377500 377499\n",
      "378000 377999\n",
      "378500 378499\n",
      "379000 378999\n",
      "379500 379499\n",
      "380000 379999\n",
      "380500 380499\n",
      "381000 380999\n",
      "381500 381499\n",
      "382000 381999\n",
      "382500 382499\n",
      "383000 382999\n",
      "383500 383499\n",
      "384000 383999\n",
      "384500 384499\n",
      "385000 384999\n",
      "385500 385499\n",
      "386000 385999\n",
      "386500 386499\n",
      "387000 386999\n",
      "387500 387499\n",
      "388000 387999\n",
      "388500 388499\n",
      "389000 388999\n",
      "389500 389499\n",
      "390000 389999\n",
      "390500 390499\n",
      "391000 390999\n",
      "391500 391499\n",
      "392000 391999\n",
      "392500 392499\n",
      "393000 392999\n",
      "393500 393499\n",
      "394000 393999\n",
      "394500 394499\n",
      "395000 394999\n",
      "395500 395499\n",
      "396000 395999\n",
      "396500 396499\n",
      "397000 396999\n",
      "397500 397499\n",
      "398000 397999\n",
      "398500 398499\n",
      "399000 398999\n",
      "399500 399499\n",
      "400000 399999\n",
      "400500 400499\n",
      "401000 400999\n",
      "401500 401499\n",
      "402000 401999\n",
      "402500 402499\n",
      "403000 402999\n",
      "403500 403499\n",
      "404000 403999\n",
      "404500 404499\n",
      "405000 404999\n",
      "405500 405499\n",
      "406000 405999\n",
      "406500 406499\n",
      "407000 406999\n",
      "407500 407499\n",
      "408000 407999\n",
      "408500 408499\n",
      "409000 408999\n",
      "409500 409499\n",
      "410000 409999\n",
      "410500 410499\n",
      "411000 410999\n",
      "411500 411499\n",
      "412000 411999\n",
      "412500 412499\n",
      "413000 412999\n",
      "413500 413499\n",
      "414000 413999\n",
      "414500 414499\n",
      "415000 414999\n",
      "415500 415499\n",
      "416000 415999\n",
      "416500 416499\n",
      "417000 416999\n",
      "417500 417499\n",
      "418000 417999\n",
      "418500 418499\n",
      "419000 418999\n",
      "419500 419499\n",
      "420000 419999\n",
      "420500 420499\n",
      "421000 420999\n",
      "421500 421499\n",
      "422000 421999\n",
      "422500 422499\n",
      "423000 422999\n",
      "423500 423499\n",
      "424000 423999\n",
      "424500 424499\n",
      "425000 424999\n",
      "425500 425499\n",
      "426000 425999\n",
      "426500 426499\n",
      "427000 426999\n",
      "427500 427499\n",
      "428000 427999\n",
      "428500 428499\n",
      "429000 428999\n",
      "429500 429499\n",
      "430000 429999\n",
      "430500 430499\n",
      "431000 430999\n",
      "431500 431499\n",
      "432000 431999\n",
      "432500 432499\n",
      "433000 432999\n",
      "433500 433499\n",
      "434000 433999\n",
      "434500 434499\n",
      "435000 434999\n",
      "435500 435499\n",
      "436000 435999\n",
      "436500 436499\n",
      "437000 436999\n",
      "437500 437499\n",
      "438000 437999\n",
      "438500 438499\n",
      "439000 438999\n",
      "439500 439499\n",
      "440000 439999\n",
      "440500 440499\n",
      "441000 440999\n",
      "441500 441499\n",
      "442000 441999\n",
      "442500 442499\n",
      "443000 442999\n",
      "443500 443499\n",
      "444000 443999\n",
      "444500 444499\n",
      "445000 444999\n",
      "445500 445499\n",
      "446000 445999\n",
      "446500 446499\n",
      "447000 446999\n",
      "447500 447499\n",
      "448000 447999\n",
      "448500 448499\n",
      "449000 448999\n",
      "449500 449499\n",
      "450000 449999\n",
      "450500 450499\n",
      "451000 450999\n",
      "451500 451499\n",
      "452000 451999\n",
      "452500 452499\n",
      "453000 452999\n",
      "453500 453499\n",
      "454000 453999\n",
      "454500 454499\n",
      "455000 454999\n",
      "455500 455499\n",
      "456000 455999\n",
      "456500 456499\n",
      "457000 456999\n",
      "457500 457499\n",
      "458000 457999\n",
      "458500 458499\n",
      "459000 458999\n",
      "459500 459499\n",
      "460000 459999\n",
      "460500 460499\n",
      "461000 460999\n",
      "461500 461499\n",
      "462000 461999\n",
      "462500 462499\n",
      "463000 462999\n",
      "463500 463499\n",
      "464000 463999\n",
      "464500 464499\n",
      "465000 464999\n",
      "465500 465499\n",
      "466000 465999\n",
      "466500 466499\n",
      "467000 466999\n",
      "467500 467499\n",
      "468000 467999\n",
      "468500 468499\n",
      "469000 468999\n",
      "469500 469499\n",
      "470000 469999\n",
      "470500 470499\n",
      "471000 470999\n",
      "471500 471499\n",
      "472000 471999\n",
      "472500 472499\n",
      "473000 472999\n",
      "473500 473499\n",
      "474000 473999\n",
      "474500 474499\n",
      "475000 474999\n",
      "475500 475499\n",
      "476000 475999\n",
      "476500 476499\n",
      "477000 476999\n",
      "477500 477499\n",
      "478000 477999\n",
      "478500 478499\n",
      "479000 478999\n",
      "479500 479499\n",
      "480000 479999\n",
      "480500 480499\n",
      "481000 480999\n",
      "481500 481499\n",
      "482000 481999\n",
      "482500 482499\n",
      "483000 482999\n",
      "483500 483499\n",
      "484000 483999\n",
      "484500 484499\n",
      "485000 484999\n",
      "485500 485499\n",
      "486000 485999\n",
      "486500 486499\n",
      "487000 486999\n",
      "487500 487499\n",
      "488000 487999\n",
      "488500 488499\n",
      "489000 488999\n",
      "489500 489499\n",
      "490000 489999\n",
      "490500 490499\n",
      "491000 490999\n",
      "491500 491499\n",
      "492000 491999\n",
      "492500 492499\n",
      "493000 492999\n",
      "493500 493499\n",
      "494000 493999\n",
      "494500 494499\n",
      "495000 494999\n",
      "495500 495499\n",
      "496000 495999\n",
      "496500 496499\n",
      "497000 496999\n",
      "497500 497499\n",
      "498000 497999\n",
      "498500 498499\n",
      "499000 498999\n",
      "499500 499499\n",
      "500000 499999\n"
     ]
    }
   ],
   "source": [
    "contractions = get_contractions()\n",
    "\n",
    "revs_list = [[]]\n",
    "stars_list = [[]]\n",
    "num = 1500000 # Number of review read\n",
    "k = 0 # Count\n",
    "nolang = [[]]\n",
    "for line in open('./data/dataset/review.json', 'r', encoding='utf8'):\n",
    "    if k >= num:\n",
    "        break\n",
    "    data = json.loads(line)\n",
    "    text = data['text']\n",
    "    star = data['stars']\n",
    "    ID = data['business_id']\n",
    "    # Check language\n",
    "    if text == None:\n",
    "        continue\n",
    "    if star == None:\n",
    "        continue\n",
    "    if ID not in restId:\n",
    "        continue\n",
    "    try:\n",
    "        if detect(text) == 'en':\n",
    "            revs_list.append(clean_text(text))\n",
    "            stars_list.append(star)\n",
    "            k += 1\n",
    "            # Notify for every 500 reviews\n",
    "            if len(revs_list) % 500 == 0:\n",
    "                print(len(revs_list), k)\n",
    "    except:\n",
    "        nolang.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love staff love meat love place prepare long line around lunch dinner hours ask want meat lean something maybe cannot remember say want fatty get half sour pickle hot pepper hand cut french fries\n",
      "500001 500001\n"
     ]
    }
   ],
   "source": [
    "print(revs_list[1])\n",
    "print(len(revs_list), len(stars_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_revs = np.asarray([revs_list]).T\n",
    "np_stars = np.asarray([stars_list]).T\n",
    "stacked_revs = np.hstack((np_revs, np_stars))\n",
    "print(np_revs.shape, np_stars.shape, stacked_revs.shape)\n",
    "df_reviews_processing = pd.DataFrame(stacked_revs, columns=categories)\n",
    "print(df_reviews_processing.shape)\n",
    "print(df_reviews_processing.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Dropping Nones: Shape 500000,2\n",
      "After Dropping Nones: Shape 500000,2\n"
     ]
    }
   ],
   "source": [
    "df_reviews_processing[['stars']] = df_reviews_processing[['stars']].apply(pd.to_numeric)\n",
    "\n",
    "# Grabbing only numbers that are of numerical value (get rid of None, NaN, etc)\n",
    "print(\"Before Dropping Nones: Shape %d,%d\" % (df_reviews_processing.shape[0], df_reviews_processing.shape[1]))\n",
    "\n",
    "df_reviews_processing = df_reviews_processing[np.isfinite(df_reviews_processing['stars'])]\n",
    "df_reviews = df_reviews_processing[np.isfinite(df_reviews_processing['stars'])]\n",
    "df_reviews = df_reviews.dropna()\n",
    "df_reviews = df_reviews.reset_index(drop=True)\n",
    "\n",
    "print(\"After Dropping Nones: Shape %d,%d\" % (df_reviews.shape[0], df_reviews.shape[1]))\n",
    "\n",
    "df_reviews.to_csv(\"./csvs/reviews_df_processed.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance the Dataset\n",
    "Want to balance the dataset, such that we have an equal number of reviews for each different category.  \n",
    "For example, if our distribution of reviews is [200,500,100,300,400], for [1,2,3,4,5] stars, respectively, then I will only take 100 of each review  \n",
    "I do this so we have an equal representation of all labels when he train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataframe(df, category=['stars']):\n",
    "    \"\"\"\n",
    "    :param df: pandas.DataFrame\n",
    "    :param categorical_columns: iterable of categorical columns names contained in {df}\n",
    "    :return: balanced pandas.DataFrame\n",
    "    \"\"\"    \n",
    "    if category is None or not all([col in df.columns for col in category]):\n",
    "        raise ValueError('Please provide one or more columns containing categorical variables')\n",
    "\n",
    "    lowest_count = df.groupby(category).apply(lambda x: x.shape[0]).min()\n",
    "    df = df.groupby(category).apply( \n",
    "        lambda x: x.sample(lowest_count)).drop(category, axis=1).reset_index().set_index('level_1')\n",
    "\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pre) 1 star ratings: 56790\n",
      "(pre) 2 star ratings: 47624\n",
      "(pre) 3 star ratings: 67513\n",
      "(pre) 4 star ratings: 134880\n",
      "(pre) 5 star ratings: 193172\n",
      "(post) 1 star ratings: 42861\n",
      "(post) 2 star ratings: 42861\n",
      "(post) 3 star ratings: 42861\n",
      "(post) 4 star ratings: 42861\n",
      "(post) 5 star ratings: 42861\n"
     ]
    }
   ],
   "source": [
    "df_reviews = pd.read_csv(\"./reviews_df_processed.csv\")\n",
    "df_reviews['len'] = df_reviews.text.str.len()\n",
    "\n",
    "df_reviews = df_reviews[df_reviews['len'].between(10, 4000)]\n",
    "df_reviews[['stars']] = df_reviews[['stars']].apply(pd.to_numeric)\n",
    "\n",
    "print(\"(pre) 1 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 1])))\n",
    "print(\"(pre) 2 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 2])))\n",
    "print(\"(pre) 3 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 3])))\n",
    "print(\"(pre) 4 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 4])))\n",
    "print(\"(pre) 5 star ratings: %d\" % (len(df_reviews[df_reviews.stars == 5])))\n",
    "\n",
    "df_balanced = balance_dataframe(df_reviews, \n",
    "                                category=['stars'])\n",
    "\n",
    "df_balanced.to_csv('balanced_reviews1000.csv', encoding='utf-8')\n",
    "print(\"(post) 1 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 1])))\n",
    "print(\"(post) 2 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 2])))\n",
    "print(\"(post) 3 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 3])))\n",
    "print(\"(post) 4 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 4])))\n",
    "print(\"(post) 5 star ratings: %d\" % (len(df_balanced[df_balanced.stars == 5])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         stars  Unnamed: 0                                               text  \\\n",
      "level_1                                                                         \n",
      "0          5.0           0  love staff love meat love place prepare long l...   \n",
      "1          5.0           1  super simple place amazing nonetheless around ...   \n",
      "2          5.0           2  small unassuming place changes menu every ofte...   \n",
      "3          5.0           3  lester located beautiful neighborhood since 19...   \n",
      "4          4.0           4  love coming yes place always needs floor swept...   \n",
      "\n",
      "           len  \n",
      "level_1         \n",
      "0        195.0  \n",
      "1        135.0  \n",
      "2        333.0  \n",
      "3        205.0  \n",
      "4        314.0  \n",
      "571.0\n",
      "665.0\n",
      "728.0\n",
      "1057.0\n"
     ]
    }
   ],
   "source": [
    "print(df_balanced.head())\n",
    "print(np.percentile(df_balanced.len, 80))\n",
    "print(np.percentile(df_balanced.len, 85))\n",
    "print(np.percentile(df_balanced.len, 87.5))\n",
    "print(np.percentile(df_balanced.len, 95))\n",
    "max_sequence_length = 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "Using ConceptNet Numberbatch instead of GLoVE (supposedly outperforms GLoVE embeddings)  \n",
    "https://github.com/commonsense/conceptnet-numberbatch\n",
    "  \n",
    "On top of the embeddings, we also keep track of commonly used words in the reviews that Embeddings don't cover. This way we could have higher test accuracy when words we come across words like these. This is specified by a threshold value. Currently, threshold is set to 20 occuraces.  \n",
    "  \n",
    "  \n",
    "We also process the reviews a bit more, sorting them into comparable lengths. This way, there is less padding necessary and (possibly) faster computation time when training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 97597\n"
     ]
    }
   ],
   "source": [
    "word_counts = {}\n",
    "count_words(word_counts, df_balanced.text)            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_path='./embeddings/numberbatch-en.txt'\n",
    "def load_embeddings(path='./embeddings/numberbatch-en.txt'):\n",
    "    embeddings_index = {}\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            embedding = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = embedding\n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = load_embeddings(embed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 97597\n",
      "Number of words we will use: 54459\n",
      "Percent of words we will use: 55.800000000000004%\n"
     ]
    }
   ],
   "source": [
    "#dictionary to tokenizer words\n",
    "word2int = {} \n",
    "threshold = 20\n",
    "token_index = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        word2int[word] = token_index\n",
    "        token_indez += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "special_characters = [\"<unk>\",\"<pad>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for c in special_characters:\n",
    "    word2int[c] = len(word2int)\n",
    "    \n",
    "usage_ratio = round(len(word2int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(word2int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54459\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(word2int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in word2int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in embeddings, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(word2int). \n",
    "print(len(word_embedding_matrix), len(word2int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, pred=False):\n",
    "    if pred:\n",
    "        seq = []\n",
    "        for word in text.split():\n",
    "            if word in word2int:\n",
    "                seq.append(word2int[word])\n",
    "            else:\n",
    "                seq.append(word2int[\"<unk>\"])\n",
    "        return seq\n",
    "    else:\n",
    "        seq = []\n",
    "        for s in text:\n",
    "            temp_seq = []\n",
    "            for word in s.split():\n",
    "                if word in word2int:\n",
    "                    temp_seq.append(word2int[word])\n",
    "                else:\n",
    "                    temp_seq.append(word2int[\"<unk>\"])\n",
    "            seq.append(temp_seq)\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 12843544\n",
      "Total number of UNKs in headlines: 84761\n",
      "Percent of words that are UNK: 0.66%\n"
     ]
    }
   ],
   "source": [
    "# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "seq = convert_to_ints(df_balanced['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the ratings into OneHot representation\n",
    "ratings = df_balanced.stars.values.astype(int)\n",
    "ratings_cat = tf.keras.utils.to_categorical(ratings)\n",
    "X_train, X_test, y_train, y_test = train_test_split(seq, ratings_cat, test_size=0.2, random_state=9)\n",
    "with pd.HDFStore('x_y_test_train.h5') as h:\n",
    "    h['X_train'] = pd.DataFrame(X_train)\n",
    "    h['X_test'] = pd.DataFrame(X_test)\n",
    "    h['y_train'] = pd.DataFrame(y_train)\n",
    "    h['y_test'] = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Batches\n",
    "Gets batches. These will be called later to then fill our X and y placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(batch):\n",
    "    # Want to pad this way since tensorflow preprocessing pads with 0's, which can eventually lead to zero tensors\n",
    "    lengths = []\n",
    "    for text in batch:\n",
    "        lengths.append[len(text)]\n",
    "    max_length = max(lengths)\n",
    "    pad_text = tf.keras.preprocessing.sequence.pad_sequences(batch, \n",
    "                                                             maxlen=max_length, \n",
    "                                                             padding='post', \n",
    "                                                             value=word2int['<pad>'])\n",
    "    return pad_text\n",
    "\n",
    "def get_batches(x, y, batch_size):\n",
    "    # Make sure to not exceed amount of data\n",
    "    for batch_i in range(0, len(x)//batch_size):\n",
    "        start = batch_i * batch_size\n",
    "        end = start+batch_size\n",
    "        batch_x = x[start:end]\n",
    "        labels = y[start:end]\n",
    "        pad_batch_x = np.array(pad_batch(batch_x))\n",
    "        yield pad_texts_batch, labels\n",
    "        \n",
    "def get_test_batches(x, batch_size):\n",
    "    for batch_i in range(0, len(x)//batch_size):\n",
    "        start = batch_i * batch_size\n",
    "        end = start+batch_size\n",
    "        batch = x[start:end]\n",
    "        pad_batch = np.array(pad_batch(batch))\n",
    "        yield pad_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "if alreadyPickled == False:\n",
    "    \n",
    "    picklefiles(\"./data/pickles/balanced_reviews.p\",df_balanced)\n",
    "    picklefiles(\"./data/pickles/category_ratings.p\",ratings_cat)\n",
    "    picklefiles(\"./data/pickles/word_embedding_matrix.p\",word_embedding_matrix)\n",
    "    picklefiles(\"./data/pickles/word2int.p\", word2int)\n",
    "    \n",
    "if alreadyPickled == True:\n",
    "    \n",
    "    word_embedding_matrix = loadfiles(\"./data/word_embedding_matrix.p\")\n",
    "    ratings_cat = loadfiles(\"./data/good_pickles/category_ratings.p\")\n",
    "    df_rev_balanced = pd.read_csv('balanced_reviews.csv')\n",
    "    balanced_reviews = loadfiles(\"./data/good_pickles/balanced_reviews.p\")\n",
    "    word2int = loadfiles('word2int.p')\n",
    "    with pd.HDFStore('x_y_test_train.h5') as h:\n",
    "        X_train = h['X_train'].values\n",
    "        X_test = h['X_test'].values\n",
    "        y_train = h['y_train'].values\n",
    "        y_test = h['y_test'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Graph\n",
    "Here we start building our computational graph. We use a 2 layer GRU Recurrent Neural Network. We define placeholders for learning rate and dropout since these are variables that we could potentially want to vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    # Should be [batch_size x review length]\n",
    "#     with tf.name_scope(\"input_data\"):\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "#         tf.summary.scalar('input_data', input_data)\n",
    "        \n",
    "    # Should be [batch_size x num_classes]\n",
    "#     with tf.name_scope(\"labels\"):\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "#         tf.summary.scalar('labels', labels)\n",
    "        \n",
    "#     with tf.name_scope(\"lr\"):    \n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "#         tf.summary.scalar(\"lr\", labels)\n",
    "    \n",
    "#     with tf.name_scope(\"keep_prob\"):\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "#         tf.summary.scalar(\"keep_prob\", keep_prob)\n",
    "\n",
    "    return input_data, labels, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is built.\n",
      "./graph\n"
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        input_data, labels, lr, keep_prob = model_inputs()\n",
    "        weight = tf.Variable(tf.truncated_normal([rnn_size, num_classes], stddev=(1/np.sqrt(rnn_size*num_classes))))\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "\n",
    "    embeddings = word_embedding_matrix\n",
    "    embs = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "\n",
    "    with tf.name_scope(\"RNN_Layers\"):\n",
    "        \n",
    "        stacked_rnn = []\n",
    "        for layer in range(num_layers):\n",
    "            cell_fw = tf.contrib.rnn.GRUCell(rnn_size)\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw,\n",
    "                                                    output_keep_prob=keep_prob)\n",
    "            stacked_rnn.append(cell_fw)\n",
    "        multilayer_cell = tf.contrib.rnn.MultiRNNCell(stacked_rnn, state_is_tuple=True)\n",
    "\n",
    "        \n",
    "    with tf.name_scope(\"init_state\"):\n",
    "        initial_state = multilayer_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    with tf.name_scope(\"Forward_Pass\"):\n",
    "        output, final_state = tf.nn.dynamic_rnn(multilayer_cell,\n",
    "                                           embs,\n",
    "                                           dtype=tf.float32)\n",
    "\n",
    "    with tf.name_scope(\"Predictions\"):\n",
    "        last = output[:, -1, :]\n",
    "        predictions = tf.exp(tf.matmul(last, weight) + bias)\n",
    "        tf.summary.histogram('predictions', predictions)\n",
    "        \n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predictions, labels=labels))        \n",
    "        tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    # Optimizer \n",
    "    with tf.name_scope('train'):    \n",
    "        optimizer = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "    \n",
    "    # Predictions comes out as 6 output layer, so need to \"change\" to one hot\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correctPred = tf.equal(tf.argmax(predictions,1), tf.argmax(labels,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    export_nodes = ['input_data', 'labels', 'keep_prob', 'lr', 'initial_state', 'final_state',\n",
    "                    'accuracy', 'predictions', 'cost', 'optimizer', 'merged']\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "print(\"Graph is built.\")\n",
    "graph_location = \"./graph\"\n",
    "\n",
    "Graph = namedtuple('train_graph', export_nodes)\n",
    "local_dict = locals()\n",
    "graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "print(graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(train_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 5 # Make 3 update checks per epoch\n",
    "update_check = (len(seq)//batch_size//per_epoch)-1\n",
    "keep_probability = 0.75\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "I've been keeping track of the tensorboard summaries so it'll allow me to visualize the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "Finished first\n",
      "Epoch   1/5 Batch   20/2678 - Loss:  1.814, Acc:  0.141, Seconds: 7.31\n",
      "Epoch   1/5 Batch   40/2678 - Loss:  1.698, Acc:  0.125, Seconds: 6.71\n",
      "Epoch   1/5 Batch   60/2678 - Loss:  1.691, Acc:  0.094, Seconds: 21.51\n",
      "Epoch   1/5 Batch   80/2678 - Loss:  1.654, Acc:  0.344, Seconds: 12.70\n",
      "Epoch   1/5 Batch  100/2678 - Loss:  1.541, Acc:  0.328, Seconds: 20.12\n",
      "Epoch   1/5 Batch  120/2678 - Loss:  1.458, Acc:  0.266, Seconds: 11.65\n",
      "Epoch   1/5 Batch  140/2678 - Loss:  1.434, Acc:  0.391, Seconds: 13.07\n",
      "Epoch   1/5 Batch  160/2678 - Loss:  1.364, Acc:  0.391, Seconds: 13.97\n",
      "Epoch   1/5 Batch  180/2678 - Loss:  1.319, Acc:  0.391, Seconds: 16.54\n",
      "Epoch   1/5 Batch  200/2678 - Loss:  1.300, Acc:  0.375, Seconds: 14.11\n",
      "Epoch   1/5 Batch  220/2678 - Loss:  1.276, Acc:  0.406, Seconds: 9.64\n",
      "Epoch   1/5 Batch  240/2678 - Loss:  1.258, Acc:  0.578, Seconds: 14.09\n",
      "Epoch   1/5 Batch  260/2678 - Loss:  1.228, Acc:  0.453, Seconds: 10.74\n",
      "Epoch   1/5 Batch  280/2678 - Loss:  1.198, Acc:  0.453, Seconds: 12.46\n",
      "Epoch   1/5 Batch  300/2678 - Loss:  1.233, Acc:  0.516, Seconds: 9.58\n",
      "Epoch   1/5 Batch  320/2678 - Loss:  1.151, Acc:  0.500, Seconds: 16.89\n",
      "Epoch   1/5 Batch  340/2678 - Loss:  1.160, Acc:  0.500, Seconds: 20.11\n",
      "Epoch   1/5 Batch  360/2678 - Loss:  1.138, Acc:  0.500, Seconds: 12.10\n",
      "Epoch   1/5 Batch  380/2678 - Loss:  1.126, Acc:  0.469, Seconds: 8.28\n",
      "Epoch   1/5 Batch  400/2678 - Loss:  1.129, Acc:  0.562, Seconds: 8.40\n",
      "Epoch   1/5 Batch  420/2678 - Loss:  1.144, Acc:  0.484, Seconds: 14.40\n",
      "Epoch   1/5 Batch  440/2678 - Loss:  1.123, Acc:  0.469, Seconds: 11.40\n",
      "Epoch   1/5 Batch  460/2678 - Loss:  1.165, Acc:  0.484, Seconds: 13.25\n",
      "Epoch   1/5 Batch  480/2678 - Loss:  1.072, Acc:  0.578, Seconds: 9.17\n",
      "Epoch   1/5 Batch  500/2678 - Loss:  1.142, Acc:  0.469, Seconds: 9.72\n",
      "Epoch   1/5 Batch  520/2678 - Loss:  1.141, Acc:  0.594, Seconds: 8.85\n",
      "Epoch   1/5 Batch  540/2678 - Loss:  1.080, Acc:  0.500, Seconds: 11.45\n",
      "Epoch   1/5 Batch  560/2678 - Loss:  1.087, Acc:  0.562, Seconds: 9.04\n",
      "Epoch   1/5 Batch  580/2678 - Loss:  1.063, Acc:  0.625, Seconds: 9.10\n",
      "Epoch   1/5 Batch  600/2678 - Loss:  1.090, Acc:  0.438, Seconds: 11.72\n",
      "Epoch   1/5 Batch  620/2678 - Loss:  1.078, Acc:  0.484, Seconds: 16.01\n",
      "Epoch   1/5 Batch  640/2678 - Loss:  1.095, Acc:  0.469, Seconds: 10.26\n",
      "Epoch   1/5 Batch  660/2678 - Loss:  1.088, Acc:  0.562, Seconds: 16.15\n",
      "Average loss for this update: 1.256\n",
      "New Record!\n",
      "Epoch   1/5 Batch  680/2678 - Loss:  1.049, Acc:  0.516, Seconds: 21.09\n",
      "Epoch   1/5 Batch  700/2678 - Loss:  1.074, Acc:  0.547, Seconds: 10.30\n",
      "Epoch   1/5 Batch  720/2678 - Loss:  1.084, Acc:  0.578, Seconds: 8.68\n",
      "Epoch   1/5 Batch  740/2678 - Loss:  1.049, Acc:  0.547, Seconds: 8.78\n",
      "Epoch   1/5 Batch  760/2678 - Loss:  0.994, Acc:  0.609, Seconds: 15.45\n",
      "Epoch   1/5 Batch  780/2678 - Loss:  1.088, Acc:  0.484, Seconds: 12.94\n",
      "Epoch   1/5 Batch  800/2678 - Loss:  1.045, Acc:  0.594, Seconds: 9.52\n",
      "Epoch   1/5 Batch  820/2678 - Loss:  1.062, Acc:  0.422, Seconds: 13.02\n",
      "Epoch   1/5 Batch  840/2678 - Loss:  1.110, Acc:  0.484, Seconds: 9.86\n",
      "Epoch   1/5 Batch  860/2678 - Loss:  1.031, Acc:  0.531, Seconds: 8.64\n",
      "Epoch   1/5 Batch  880/2678 - Loss:  1.073, Acc:  0.641, Seconds: 14.61\n",
      "Epoch   1/5 Batch  900/2678 - Loss:  1.080, Acc:  0.562, Seconds: 9.66\n",
      "Epoch   1/5 Batch  920/2678 - Loss:  1.028, Acc:  0.484, Seconds: 11.08\n",
      "Epoch   1/5 Batch  940/2678 - Loss:  1.048, Acc:  0.484, Seconds: 12.19\n",
      "Epoch   1/5 Batch  960/2678 - Loss:  1.055, Acc:  0.422, Seconds: 17.37\n",
      "Epoch   1/5 Batch  980/2678 - Loss:  1.049, Acc:  0.531, Seconds: 10.10\n",
      "Epoch   1/5 Batch 1000/2678 - Loss:  1.052, Acc:  0.453, Seconds: 14.15\n",
      "Epoch   1/5 Batch 1020/2678 - Loss:  0.995, Acc:  0.578, Seconds: 11.06\n",
      "Epoch   1/5 Batch 1040/2678 - Loss:  1.054, Acc:  0.453, Seconds: 10.78\n",
      "Epoch   1/5 Batch 1060/2678 - Loss:  0.995, Acc:  0.578, Seconds: 20.34\n",
      "Epoch   1/5 Batch 1080/2678 - Loss:  1.052, Acc:  0.484, Seconds: 9.16\n",
      "Epoch   1/5 Batch 1100/2678 - Loss:  1.038, Acc:  0.531, Seconds: 6.97\n",
      "Epoch   1/5 Batch 1120/2678 - Loss:  1.024, Acc:  0.703, Seconds: 8.95\n",
      "Epoch   1/5 Batch 1140/2678 - Loss:  1.044, Acc:  0.562, Seconds: 17.37\n",
      "Epoch   1/5 Batch 1160/2678 - Loss:  1.062, Acc:  0.500, Seconds: 9.49\n",
      "Epoch   1/5 Batch 1180/2678 - Loss:  1.009, Acc:  0.438, Seconds: 10.26\n",
      "Epoch   1/5 Batch 1200/2678 - Loss:  1.011, Acc:  0.531, Seconds: 10.50\n",
      "Epoch   1/5 Batch 1220/2678 - Loss:  1.051, Acc:  0.516, Seconds: 13.15\n",
      "Epoch   1/5 Batch 1240/2678 - Loss:  1.044, Acc:  0.516, Seconds: 12.73\n",
      "Epoch   1/5 Batch 1260/2678 - Loss:  1.030, Acc:  0.562, Seconds: 13.48\n",
      "Epoch   1/5 Batch 1280/2678 - Loss:  0.996, Acc:  0.625, Seconds: 12.13\n",
      "Epoch   1/5 Batch 1300/2678 - Loss:  1.064, Acc:  0.531, Seconds: 11.46\n",
      "Epoch   1/5 Batch 1320/2678 - Loss:  0.993, Acc:  0.562, Seconds: 18.50\n",
      "Average loss for this update: 1.042\n",
      "New Record!\n",
      "Epoch   1/5 Batch 1340/2678 - Loss:  1.011, Acc:  0.531, Seconds: 11.53\n",
      "Epoch   1/5 Batch 1360/2678 - Loss:  0.995, Acc:  0.531, Seconds: 7.33\n",
      "Epoch   1/5 Batch 1380/2678 - Loss:  1.029, Acc:  0.531, Seconds: 13.14\n",
      "Epoch   1/5 Batch 1400/2678 - Loss:  1.055, Acc:  0.609, Seconds: 11.78\n",
      "Epoch   1/5 Batch 1420/2678 - Loss:  1.047, Acc:  0.516, Seconds: 10.92\n",
      "Epoch   1/5 Batch 1440/2678 - Loss:  0.987, Acc:  0.656, Seconds: 12.98\n",
      "Epoch   1/5 Batch 1460/2678 - Loss:  1.030, Acc:  0.609, Seconds: 16.33\n",
      "Epoch   1/5 Batch 1480/2678 - Loss:  1.024, Acc:  0.516, Seconds: 10.70\n",
      "Epoch   1/5 Batch 1500/2678 - Loss:  1.026, Acc:  0.516, Seconds: 9.18\n",
      "Epoch   1/5 Batch 1520/2678 - Loss:  1.038, Acc:  0.609, Seconds: 16.54\n",
      "Epoch   1/5 Batch 1540/2678 - Loss:  1.025, Acc:  0.547, Seconds: 6.39\n",
      "Epoch   1/5 Batch 1560/2678 - Loss:  1.010, Acc:  0.656, Seconds: 16.36\n",
      "Epoch   1/5 Batch 1580/2678 - Loss:  1.005, Acc:  0.609, Seconds: 14.94\n",
      "Epoch   1/5 Batch 1600/2678 - Loss:  1.032, Acc:  0.547, Seconds: 12.49\n",
      "Epoch   1/5 Batch 1620/2678 - Loss:  1.010, Acc:  0.547, Seconds: 12.84\n",
      "Epoch   1/5 Batch 1640/2678 - Loss:  0.989, Acc:  0.531, Seconds: 10.95\n",
      "Epoch   1/5 Batch 1660/2678 - Loss:  1.017, Acc:  0.547, Seconds: 8.36\n",
      "Epoch   1/5 Batch 1680/2678 - Loss:  1.008, Acc:  0.516, Seconds: 15.67\n",
      "Epoch   1/5 Batch 1700/2678 - Loss:  0.968, Acc:  0.547, Seconds: 10.60\n",
      "Epoch   1/5 Batch 1720/2678 - Loss:  1.022, Acc:  0.438, Seconds: 14.75\n",
      "Epoch   1/5 Batch 1740/2678 - Loss:  1.012, Acc:  0.672, Seconds: 11.50\n",
      "Epoch   1/5 Batch 1760/2678 - Loss:  1.014, Acc:  0.609, Seconds: 16.13\n",
      "Epoch   1/5 Batch 1780/2678 - Loss:  1.023, Acc:  0.656, Seconds: 11.59\n",
      "Epoch   1/5 Batch 1800/2678 - Loss:  0.943, Acc:  0.625, Seconds: 9.28\n",
      "Epoch   1/5 Batch 1820/2678 - Loss:  0.984, Acc:  0.484, Seconds: 15.95\n",
      "Epoch   1/5 Batch 1840/2678 - Loss:  0.996, Acc:  0.500, Seconds: 21.05\n",
      "Epoch   1/5 Batch 1860/2678 - Loss:  0.999, Acc:  0.641, Seconds: 10.95\n",
      "Epoch   1/5 Batch 1880/2678 - Loss:  1.025, Acc:  0.609, Seconds: 17.64\n",
      "Epoch   1/5 Batch 1900/2678 - Loss:  0.976, Acc:  0.531, Seconds: 13.86\n",
      "Epoch   1/5 Batch 1920/2678 - Loss:  1.017, Acc:  0.594, Seconds: 11.28\n",
      "Epoch   1/5 Batch 1940/2678 - Loss:  0.998, Acc:  0.578, Seconds: 11.26\n",
      "Epoch   1/5 Batch 1960/2678 - Loss:  1.012, Acc:  0.469, Seconds: 12.57\n",
      "Epoch   1/5 Batch 1980/2678 - Loss:  0.961, Acc:  0.625, Seconds: 18.74\n",
      "Epoch   1/5 Batch 2000/2678 - Loss:  1.027, Acc:  0.547, Seconds: 20.59\n",
      "Average loss for this update: 1.01\n",
      "New Record!\n",
      "Epoch   1/5 Batch 2020/2678 - Loss:  1.015, Acc:  0.422, Seconds: 9.63\n",
      "Epoch   1/5 Batch 2040/2678 - Loss:  1.006, Acc:  0.500, Seconds: 12.64\n",
      "Epoch   1/5 Batch 2060/2678 - Loss:  1.004, Acc:  0.578, Seconds: 20.98\n",
      "Epoch   1/5 Batch 2080/2678 - Loss:  0.969, Acc:  0.703, Seconds: 11.28\n",
      "Epoch   1/5 Batch 2100/2678 - Loss:  0.988, Acc:  0.594, Seconds: 14.45\n",
      "Epoch   1/5 Batch 2120/2678 - Loss:  0.981, Acc:  0.578, Seconds: 10.35\n",
      "Epoch   1/5 Batch 2140/2678 - Loss:  0.991, Acc:  0.656, Seconds: 10.82\n",
      "Epoch   1/5 Batch 2160/2678 - Loss:  0.959, Acc:  0.578, Seconds: 10.00\n",
      "Epoch   1/5 Batch 2180/2678 - Loss:  1.017, Acc:  0.594, Seconds: 19.83\n",
      "Epoch   1/5 Batch 2200/2678 - Loss:  0.996, Acc:  0.531, Seconds: 13.99\n",
      "Epoch   1/5 Batch 2220/2678 - Loss:  0.986, Acc:  0.688, Seconds: 14.75\n",
      "Epoch   1/5 Batch 2240/2678 - Loss:  1.028, Acc:  0.594, Seconds: 11.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/5 Batch 2260/2678 - Loss:  0.992, Acc:  0.562, Seconds: 15.83\n",
      "Epoch   1/5 Batch 2280/2678 - Loss:  0.981, Acc:  0.594, Seconds: 10.07\n",
      "Epoch   1/5 Batch 2300/2678 - Loss:  0.975, Acc:  0.656, Seconds: 15.80\n",
      "Epoch   1/5 Batch 2320/2678 - Loss:  0.985, Acc:  0.484, Seconds: 18.09\n",
      "Epoch   1/5 Batch 2340/2678 - Loss:  1.005, Acc:  0.656, Seconds: 21.45\n",
      "Epoch   1/5 Batch 2360/2678 - Loss:  1.001, Acc:  0.578, Seconds: 13.01\n",
      "Epoch   1/5 Batch 2380/2678 - Loss:  0.976, Acc:  0.609, Seconds: 9.23\n",
      "Epoch   1/5 Batch 2400/2678 - Loss:  0.949, Acc:  0.812, Seconds: 8.39\n",
      "Epoch   1/5 Batch 2420/2678 - Loss:  0.958, Acc:  0.594, Seconds: 12.08\n",
      "Epoch   1/5 Batch 2440/2678 - Loss:  0.984, Acc:  0.547, Seconds: 11.79\n",
      "Epoch   1/5 Batch 2460/2678 - Loss:  0.941, Acc:  0.578, Seconds: 21.18\n",
      "Epoch   1/5 Batch 2480/2678 - Loss:  1.010, Acc:  0.625, Seconds: 11.78\n",
      "Epoch   1/5 Batch 2500/2678 - Loss:  0.972, Acc:  0.547, Seconds: 16.40\n",
      "Epoch   1/5 Batch 2520/2678 - Loss:  0.990, Acc:  0.469, Seconds: 11.55\n",
      "Epoch   1/5 Batch 2540/2678 - Loss:  0.986, Acc:  0.609, Seconds: 19.58\n",
      "Epoch   1/5 Batch 2560/2678 - Loss:  0.984, Acc:  0.500, Seconds: 8.26\n",
      "Epoch   1/5 Batch 2580/2678 - Loss:  0.952, Acc:  0.656, Seconds: 19.26\n",
      "Epoch   1/5 Batch 2600/2678 - Loss:  0.990, Acc:  0.641, Seconds: 19.16\n",
      "Epoch   1/5 Batch 2620/2678 - Loss:  0.924, Acc:  0.625, Seconds: 12.38\n",
      "Epoch   1/5 Batch 2640/2678 - Loss:  0.962, Acc:  0.578, Seconds: 14.18\n",
      "Epoch   1/5 Batch 2660/2678 - Loss:  1.009, Acc:  0.594, Seconds: 14.66\n",
      "Average loss for this update: 0.983\n",
      "New Record!\n",
      "Starting\n",
      "Finished first\n",
      "Epoch   2/5 Batch   20/2678 - Loss:  1.048, Acc:  0.578, Seconds: 7.76\n",
      "Epoch   2/5 Batch   40/2678 - Loss:  1.018, Acc:  0.562, Seconds: 7.59\n",
      "Epoch   2/5 Batch   60/2678 - Loss:  0.942, Acc:  0.625, Seconds: 20.91\n",
      "Epoch   2/5 Batch   80/2678 - Loss:  0.953, Acc:  0.609, Seconds: 13.01\n",
      "Epoch   2/5 Batch  100/2678 - Loss:  0.995, Acc:  0.625, Seconds: 25.64\n",
      "Epoch   2/5 Batch  120/2678 - Loss:  0.982, Acc:  0.531, Seconds: 11.33\n",
      "Epoch   2/5 Batch  140/2678 - Loss:  0.969, Acc:  0.609, Seconds: 13.71\n",
      "Epoch   2/5 Batch  160/2678 - Loss:  0.976, Acc:  0.500, Seconds: 13.13\n",
      "Epoch   2/5 Batch  180/2678 - Loss:  0.953, Acc:  0.609, Seconds: 18.20\n",
      "Epoch   2/5 Batch  200/2678 - Loss:  0.932, Acc:  0.641, Seconds: 14.42\n",
      "Epoch   2/5 Batch  220/2678 - Loss:  0.982, Acc:  0.594, Seconds: 10.68\n",
      "Epoch   2/5 Batch  240/2678 - Loss:  0.976, Acc:  0.656, Seconds: 15.46\n",
      "Epoch   2/5 Batch  260/2678 - Loss:  0.966, Acc:  0.547, Seconds: 11.02\n",
      "Epoch   2/5 Batch  280/2678 - Loss:  0.957, Acc:  0.641, Seconds: 12.54\n",
      "Epoch   2/5 Batch  300/2678 - Loss:  0.950, Acc:  0.484, Seconds: 10.32\n",
      "Epoch   2/5 Batch  320/2678 - Loss:  0.946, Acc:  0.625, Seconds: 16.52\n",
      "Epoch   2/5 Batch  340/2678 - Loss:  0.941, Acc:  0.578, Seconds: 23.12\n",
      "Epoch   2/5 Batch  360/2678 - Loss:  0.961, Acc:  0.516, Seconds: 12.52\n",
      "Epoch   2/5 Batch  380/2678 - Loss:  0.951, Acc:  0.516, Seconds: 9.23\n",
      "Epoch   2/5 Batch  400/2678 - Loss:  0.956, Acc:  0.641, Seconds: 9.28\n",
      "Epoch   2/5 Batch  420/2678 - Loss:  0.979, Acc:  0.547, Seconds: 14.46\n",
      "Epoch   2/5 Batch  440/2678 - Loss:  0.945, Acc:  0.531, Seconds: 11.31\n",
      "Epoch   2/5 Batch  460/2678 - Loss:  0.953, Acc:  0.641, Seconds: 14.04\n",
      "Epoch   2/5 Batch  480/2678 - Loss:  0.935, Acc:  0.672, Seconds: 9.30\n",
      "Epoch   2/5 Batch  500/2678 - Loss:  0.980, Acc:  0.594, Seconds: 9.05\n",
      "Epoch   2/5 Batch  520/2678 - Loss:  0.949, Acc:  0.625, Seconds: 9.39\n",
      "Epoch   2/5 Batch  540/2678 - Loss:  0.928, Acc:  0.688, Seconds: 10.93\n",
      "Epoch   2/5 Batch  560/2678 - Loss:  0.941, Acc:  0.703, Seconds: 10.17\n",
      "Epoch   2/5 Batch  580/2678 - Loss:  0.944, Acc:  0.578, Seconds: 9.52\n",
      "Epoch   2/5 Batch  600/2678 - Loss:  0.953, Acc:  0.547, Seconds: 12.63\n",
      "Epoch   2/5 Batch  620/2678 - Loss:  0.940, Acc:  0.531, Seconds: 18.42\n",
      "Epoch   2/5 Batch  640/2678 - Loss:  0.974, Acc:  0.484, Seconds: 11.25\n",
      "Epoch   2/5 Batch  660/2678 - Loss:  0.939, Acc:  0.547, Seconds: 18.27\n",
      "Average loss for this update: 0.961\n",
      "New Record!\n",
      "Epoch   2/5 Batch  680/2678 - Loss:  0.951, Acc:  0.609, Seconds: 24.60\n",
      "Epoch   2/5 Batch  700/2678 - Loss:  0.949, Acc:  0.562, Seconds: 9.93\n",
      "Epoch   2/5 Batch  720/2678 - Loss:  0.960, Acc:  0.672, Seconds: 10.01\n",
      "Epoch   2/5 Batch  740/2678 - Loss:  0.919, Acc:  0.594, Seconds: 9.39\n",
      "Epoch   2/5 Batch  760/2678 - Loss:  0.914, Acc:  0.688, Seconds: 15.89\n",
      "Epoch   2/5 Batch  780/2678 - Loss:  0.977, Acc:  0.625, Seconds: 13.19\n",
      "Epoch   2/5 Batch  800/2678 - Loss:  0.915, Acc:  0.641, Seconds: 9.57\n",
      "Epoch   2/5 Batch  820/2678 - Loss:  0.948, Acc:  0.594, Seconds: 13.90\n",
      "Epoch   2/5 Batch  840/2678 - Loss:  0.966, Acc:  0.594, Seconds: 10.84\n",
      "Epoch   2/5 Batch  860/2678 - Loss:  0.935, Acc:  0.547, Seconds: 10.03\n",
      "Epoch   2/5 Batch  880/2678 - Loss:  0.952, Acc:  0.719, Seconds: 12.87\n",
      "Epoch   2/5 Batch  900/2678 - Loss:  0.983, Acc:  0.578, Seconds: 8.82\n",
      "Epoch   2/5 Batch  920/2678 - Loss:  0.964, Acc:  0.547, Seconds: 13.04\n",
      "Epoch   2/5 Batch  940/2678 - Loss:  0.952, Acc:  0.531, Seconds: 12.66\n",
      "Epoch   2/5 Batch  960/2678 - Loss:  0.960, Acc:  0.578, Seconds: 15.65\n",
      "Epoch   2/5 Batch  980/2678 - Loss:  0.942, Acc:  0.609, Seconds: 10.84\n",
      "Epoch   2/5 Batch 1000/2678 - Loss:  0.932, Acc:  0.484, Seconds: 15.47\n",
      "Epoch   2/5 Batch 1020/2678 - Loss:  0.893, Acc:  0.547, Seconds: 11.44\n",
      "Epoch   2/5 Batch 1040/2678 - Loss:  0.957, Acc:  0.547, Seconds: 8.76\n",
      "Epoch   2/5 Batch 1060/2678 - Loss:  0.900, Acc:  0.641, Seconds: 21.27\n",
      "Epoch   2/5 Batch 1080/2678 - Loss:  0.949, Acc:  0.609, Seconds: 9.44\n",
      "Epoch   2/5 Batch 1100/2678 - Loss:  0.969, Acc:  0.609, Seconds: 7.97\n",
      "Epoch   2/5 Batch 1120/2678 - Loss:  0.928, Acc:  0.672, Seconds: 9.60\n",
      "Epoch   2/5 Batch 1140/2678 - Loss:  0.918, Acc:  0.562, Seconds: 19.92\n",
      "Epoch   2/5 Batch 1160/2678 - Loss:  0.947, Acc:  0.562, Seconds: 10.59\n",
      "Epoch   2/5 Batch 1180/2678 - Loss:  0.883, Acc:  0.516, Seconds: 10.79\n",
      "Epoch   2/5 Batch 1200/2678 - Loss:  0.928, Acc:  0.609, Seconds: 11.22\n",
      "Epoch   2/5 Batch 1220/2678 - Loss:  0.964, Acc:  0.516, Seconds: 14.34\n",
      "Epoch   2/5 Batch 1240/2678 - Loss:  0.959, Acc:  0.469, Seconds: 13.99\n",
      "Epoch   2/5 Batch 1260/2678 - Loss:  0.926, Acc:  0.641, Seconds: 13.58\n",
      "Epoch   2/5 Batch 1280/2678 - Loss:  0.898, Acc:  0.641, Seconds: 10.12\n",
      "Epoch   2/5 Batch 1300/2678 - Loss:  0.974, Acc:  0.547, Seconds: 11.69\n",
      "Epoch   2/5 Batch 1320/2678 - Loss:  0.914, Acc:  0.609, Seconds: 19.54\n",
      "Average loss for this update: 0.939\n",
      "New Record!\n",
      "Epoch   2/5 Batch 1340/2678 - Loss:  0.920, Acc:  0.578, Seconds: 12.66\n",
      "Epoch   2/5 Batch 1360/2678 - Loss:  0.922, Acc:  0.594, Seconds: 8.46\n",
      "Epoch   2/5 Batch 1380/2678 - Loss:  0.932, Acc:  0.578, Seconds: 13.72\n",
      "Epoch   2/5 Batch 1400/2678 - Loss:  0.979, Acc:  0.672, Seconds: 12.27\n",
      "Epoch   2/5 Batch 1420/2678 - Loss:  0.942, Acc:  0.578, Seconds: 10.56\n",
      "Epoch   2/5 Batch 1440/2678 - Loss:  0.876, Acc:  0.641, Seconds: 11.89\n",
      "Epoch   2/5 Batch 1460/2678 - Loss:  0.968, Acc:  0.672, Seconds: 18.85\n",
      "Epoch   2/5 Batch 1480/2678 - Loss:  0.960, Acc:  0.531, Seconds: 10.93\n",
      "Epoch   2/5 Batch 1500/2678 - Loss:  0.956, Acc:  0.562, Seconds: 10.10\n",
      "Epoch   2/5 Batch 1520/2678 - Loss:  0.996, Acc:  0.547, Seconds: 17.81\n",
      "Epoch   2/5 Batch 1540/2678 - Loss:  0.940, Acc:  0.688, Seconds: 7.08\n",
      "Epoch   2/5 Batch 1560/2678 - Loss:  0.937, Acc:  0.641, Seconds: 16.83\n",
      "Epoch   2/5 Batch 1580/2678 - Loss:  0.921, Acc:  0.672, Seconds: 12.81\n",
      "Epoch   2/5 Batch 1600/2678 - Loss:  0.939, Acc:  0.594, Seconds: 12.57\n",
      "Epoch   2/5 Batch 1620/2678 - Loss:  0.948, Acc:  0.562, Seconds: 12.92\n",
      "Epoch   2/5 Batch 1640/2678 - Loss:  0.905, Acc:  0.688, Seconds: 10.65\n",
      "Epoch   2/5 Batch 1660/2678 - Loss:  0.927, Acc:  0.516, Seconds: 8.02\n",
      "Epoch   2/5 Batch 1680/2678 - Loss:  0.926, Acc:  0.641, Seconds: 13.62\n",
      "Epoch   2/5 Batch 1700/2678 - Loss:  0.909, Acc:  0.703, Seconds: 9.56\n",
      "Epoch   2/5 Batch 1720/2678 - Loss:  0.943, Acc:  0.531, Seconds: 13.93\n",
      "Epoch   2/5 Batch 1740/2678 - Loss:  0.906, Acc:  0.750, Seconds: 11.73\n",
      "Epoch   2/5 Batch 1760/2678 - Loss:  0.946, Acc:  0.609, Seconds: 17.64\n",
      "Epoch   2/5 Batch 1780/2678 - Loss:  0.962, Acc:  0.766, Seconds: 11.75\n",
      "Epoch   2/5 Batch 1800/2678 - Loss:  0.881, Acc:  0.641, Seconds: 9.40\n",
      "Epoch   2/5 Batch 1820/2678 - Loss:  0.933, Acc:  0.594, Seconds: 14.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/5 Batch 1840/2678 - Loss:  0.951, Acc:  0.500, Seconds: 20.61\n",
      "Epoch   2/5 Batch 1860/2678 - Loss:  0.933, Acc:  0.609, Seconds: 10.62\n",
      "Epoch   2/5 Batch 1880/2678 - Loss:  0.949, Acc:  0.547, Seconds: 19.15\n",
      "Epoch   2/5 Batch 1900/2678 - Loss:  0.905, Acc:  0.594, Seconds: 17.22\n",
      "Epoch   2/5 Batch 1920/2678 - Loss:  0.934, Acc:  0.609, Seconds: 13.92\n",
      "Epoch   2/5 Batch 1940/2678 - Loss:  0.921, Acc:  0.578, Seconds: 11.99\n",
      "Epoch   2/5 Batch 1960/2678 - Loss:  0.952, Acc:  0.609, Seconds: 12.52\n",
      "Epoch   2/5 Batch 1980/2678 - Loss:  0.881, Acc:  0.609, Seconds: 21.73\n",
      "Epoch   2/5 Batch 2000/2678 - Loss:  0.935, Acc:  0.688, Seconds: 19.76\n",
      "Average loss for this update: 0.934\n",
      "New Record!\n",
      "Epoch   2/5 Batch 2020/2678 - Loss:  0.940, Acc:  0.656, Seconds: 9.82\n",
      "Epoch   2/5 Batch 2040/2678 - Loss:  0.929, Acc:  0.516, Seconds: 12.22\n",
      "Epoch   2/5 Batch 2060/2678 - Loss:  0.933, Acc:  0.547, Seconds: 20.22\n",
      "Epoch   2/5 Batch 2080/2678 - Loss:  0.913, Acc:  0.766, Seconds: 12.96\n",
      "Epoch   2/5 Batch 2100/2678 - Loss:  0.934, Acc:  0.625, Seconds: 14.45\n",
      "Epoch   2/5 Batch 2120/2678 - Loss:  0.910, Acc:  0.609, Seconds: 9.65\n",
      "Epoch   2/5 Batch 2140/2678 - Loss:  0.920, Acc:  0.656, Seconds: 16.27\n",
      "Epoch   2/5 Batch 2160/2678 - Loss:  0.895, Acc:  0.641, Seconds: 9.85\n",
      "Epoch   2/5 Batch 2180/2678 - Loss:  0.913, Acc:  0.594, Seconds: 20.60\n",
      "Epoch   2/5 Batch 2200/2678 - Loss:  0.941, Acc:  0.656, Seconds: 13.30\n",
      "Epoch   2/5 Batch 2220/2678 - Loss:  0.932, Acc:  0.719, Seconds: 15.27\n",
      "Epoch   2/5 Batch 2240/2678 - Loss:  0.963, Acc:  0.562, Seconds: 13.36\n",
      "Epoch   2/5 Batch 2260/2678 - Loss:  0.919, Acc:  0.547, Seconds: 13.26\n",
      "Epoch   2/5 Batch 2280/2678 - Loss:  0.931, Acc:  0.609, Seconds: 9.57\n",
      "Epoch   2/5 Batch 2300/2678 - Loss:  0.905, Acc:  0.656, Seconds: 18.06\n",
      "Epoch   2/5 Batch 2320/2678 - Loss:  0.923, Acc:  0.562, Seconds: 13.81\n",
      "Epoch   2/5 Batch 2340/2678 - Loss:  0.955, Acc:  0.578, Seconds: 19.04\n",
      "Epoch   2/5 Batch 2360/2678 - Loss:  0.950, Acc:  0.656, Seconds: 12.45\n",
      "Epoch   2/5 Batch 2380/2678 - Loss:  0.938, Acc:  0.625, Seconds: 9.82\n",
      "Epoch   2/5 Batch 2400/2678 - Loss:  0.894, Acc:  0.812, Seconds: 9.13\n",
      "Epoch   2/5 Batch 2420/2678 - Loss:  0.911, Acc:  0.609, Seconds: 12.54\n",
      "Epoch   2/5 Batch 2440/2678 - Loss:  0.936, Acc:  0.594, Seconds: 11.40\n",
      "Epoch   2/5 Batch 2460/2678 - Loss:  0.883, Acc:  0.672, Seconds: 24.08\n",
      "Epoch   2/5 Batch 2480/2678 - Loss:  0.963, Acc:  0.672, Seconds: 14.05\n",
      "Epoch   2/5 Batch 2500/2678 - Loss:  0.914, Acc:  0.578, Seconds: 17.33\n",
      "Epoch   2/5 Batch 2520/2678 - Loss:  0.938, Acc:  0.438, Seconds: 12.67\n",
      "Epoch   2/5 Batch 2540/2678 - Loss:  0.935, Acc:  0.578, Seconds: 20.73\n",
      "Epoch   2/5 Batch 2560/2678 - Loss:  0.944, Acc:  0.516, Seconds: 9.50\n",
      "Epoch   2/5 Batch 2580/2678 - Loss:  0.915, Acc:  0.703, Seconds: 21.07\n",
      "Epoch   2/5 Batch 2600/2678 - Loss:  0.905, Acc:  0.672, Seconds: 20.34\n",
      "Epoch   2/5 Batch 2620/2678 - Loss:  0.867, Acc:  0.672, Seconds: 14.03\n",
      "Epoch   2/5 Batch 2640/2678 - Loss:  0.918, Acc:  0.531, Seconds: 14.46\n",
      "Epoch   2/5 Batch 2660/2678 - Loss:  0.950, Acc:  0.594, Seconds: 16.75\n",
      "Average loss for this update: 0.924\n",
      "New Record!\n",
      "Starting\n",
      "Finished first\n",
      "Epoch   3/5 Batch   20/2678 - Loss:  0.962, Acc:  0.578, Seconds: 9.40\n",
      "Epoch   3/5 Batch   40/2678 - Loss:  0.932, Acc:  0.625, Seconds: 8.78\n",
      "Epoch   3/5 Batch   60/2678 - Loss:  0.902, Acc:  0.672, Seconds: 23.51\n",
      "Epoch   3/5 Batch   80/2678 - Loss:  0.901, Acc:  0.641, Seconds: 15.11\n",
      "Epoch   3/5 Batch  100/2678 - Loss:  0.960, Acc:  0.672, Seconds: 22.22\n",
      "Epoch   3/5 Batch  120/2678 - Loss:  0.940, Acc:  0.562, Seconds: 14.20\n",
      "Epoch   3/5 Batch  140/2678 - Loss:  0.898, Acc:  0.609, Seconds: 14.49\n",
      "Epoch   3/5 Batch  160/2678 - Loss:  0.922, Acc:  0.531, Seconds: 15.94\n",
      "Epoch   3/5 Batch  180/2678 - Loss:  0.899, Acc:  0.609, Seconds: 21.30\n",
      "Epoch   3/5 Batch  200/2678 - Loss:  0.853, Acc:  0.719, Seconds: 19.07\n",
      "Epoch   3/5 Batch  220/2678 - Loss:  0.943, Acc:  0.703, Seconds: 11.88\n",
      "Epoch   3/5 Batch  240/2678 - Loss:  0.937, Acc:  0.562, Seconds: 17.70\n",
      "Epoch   3/5 Batch  260/2678 - Loss:  0.901, Acc:  0.641, Seconds: 13.39\n",
      "Epoch   3/5 Batch  280/2678 - Loss:  0.925, Acc:  0.625, Seconds: 15.18\n",
      "Epoch   3/5 Batch  300/2678 - Loss:  0.903, Acc:  0.594, Seconds: 14.57\n",
      "Epoch   3/5 Batch  320/2678 - Loss:  0.910, Acc:  0.625, Seconds: 25.39\n",
      "Epoch   3/5 Batch  340/2678 - Loss:  0.926, Acc:  0.594, Seconds: 27.24\n",
      "Epoch   3/5 Batch  360/2678 - Loss:  0.905, Acc:  0.578, Seconds: 17.05\n",
      "Epoch   3/5 Batch  380/2678 - Loss:  0.902, Acc:  0.531, Seconds: 10.85\n",
      "Epoch   3/5 Batch  400/2678 - Loss:  0.909, Acc:  0.594, Seconds: 13.43\n",
      "Epoch   3/5 Batch  420/2678 - Loss:  0.913, Acc:  0.594, Seconds: 22.92\n",
      "Epoch   3/5 Batch  440/2678 - Loss:  0.898, Acc:  0.625, Seconds: 18.52\n",
      "Epoch   3/5 Batch  460/2678 - Loss:  0.939, Acc:  0.547, Seconds: 20.73\n",
      "Epoch   3/5 Batch  480/2678 - Loss:  0.891, Acc:  0.656, Seconds: 15.95\n",
      "Epoch   3/5 Batch  500/2678 - Loss:  0.921, Acc:  0.594, Seconds: 14.85\n",
      "Epoch   3/5 Batch  520/2678 - Loss:  0.893, Acc:  0.672, Seconds: 16.38\n",
      "Epoch   3/5 Batch  540/2678 - Loss:  0.898, Acc:  0.734, Seconds: 19.63\n",
      "Epoch   3/5 Batch  560/2678 - Loss:  0.907, Acc:  0.609, Seconds: 18.45\n",
      "Epoch   3/5 Batch  580/2678 - Loss:  0.896, Acc:  0.625, Seconds: 16.69\n",
      "Epoch   3/5 Batch  600/2678 - Loss:  0.917, Acc:  0.562, Seconds: 20.40\n",
      "Epoch   3/5 Batch  620/2678 - Loss:  0.876, Acc:  0.609, Seconds: 28.16\n",
      "Epoch   3/5 Batch  640/2678 - Loss:  0.914, Acc:  0.531, Seconds: 19.95\n",
      "Epoch   3/5 Batch  660/2678 - Loss:  0.881, Acc:  0.609, Seconds: 30.13\n",
      "Average loss for this update: 0.911\n",
      "New Record!\n",
      "Epoch   3/5 Batch  680/2678 - Loss:  0.882, Acc:  0.688, Seconds: 33.81\n",
      "Epoch   3/5 Batch  700/2678 - Loss:  0.884, Acc:  0.562, Seconds: 17.45\n",
      "Epoch   3/5 Batch  720/2678 - Loss:  0.916, Acc:  0.609, Seconds: 17.21\n",
      "Epoch   3/5 Batch  740/2678 - Loss:  0.880, Acc:  0.703, Seconds: 16.43\n",
      "Epoch   3/5 Batch  760/2678 - Loss:  0.883, Acc:  0.719, Seconds: 28.49\n",
      "Epoch   3/5 Batch  780/2678 - Loss:  0.920, Acc:  0.625, Seconds: 21.33\n",
      "Epoch   3/5 Batch  800/2678 - Loss:  0.878, Acc:  0.656, Seconds: 17.62\n",
      "Epoch   3/5 Batch  820/2678 - Loss:  0.909, Acc:  0.609, Seconds: 21.88\n",
      "Epoch   3/5 Batch  840/2678 - Loss:  0.943, Acc:  0.594, Seconds: 16.65\n",
      "Epoch   3/5 Batch  860/2678 - Loss:  0.890, Acc:  0.609, Seconds: 15.61\n",
      "Epoch   3/5 Batch  880/2678 - Loss:  0.901, Acc:  0.703, Seconds: 17.81\n",
      "Epoch   3/5 Batch  900/2678 - Loss:  0.927, Acc:  0.594, Seconds: 14.26\n",
      "Epoch   3/5 Batch  920/2678 - Loss:  0.913, Acc:  0.609, Seconds: 19.12\n",
      "Epoch   3/5 Batch  940/2678 - Loss:  0.897, Acc:  0.500, Seconds: 18.48\n",
      "Epoch   3/5 Batch  960/2678 - Loss:  0.911, Acc:  0.578, Seconds: 18.99\n",
      "Epoch   3/5 Batch  980/2678 - Loss:  0.892, Acc:  0.656, Seconds: 11.04\n",
      "Epoch   3/5 Batch 1000/2678 - Loss:  0.893, Acc:  0.578, Seconds: 22.59\n",
      "Epoch   3/5 Batch 1020/2678 - Loss:  0.855, Acc:  0.578, Seconds: 16.92\n",
      "Epoch   3/5 Batch 1040/2678 - Loss:  0.909, Acc:  0.562, Seconds: 11.91\n",
      "Epoch   3/5 Batch 1060/2678 - Loss:  0.865, Acc:  0.672, Seconds: 28.51\n",
      "Epoch   3/5 Batch 1080/2678 - Loss:  0.916, Acc:  0.594, Seconds: 13.43\n",
      "Epoch   3/5 Batch 1100/2678 - Loss:  0.909, Acc:  0.547, Seconds: 11.96\n",
      "Epoch   3/5 Batch 1120/2678 - Loss:  0.894, Acc:  0.703, Seconds: 13.59\n",
      "Epoch   3/5 Batch 1140/2678 - Loss:  0.890, Acc:  0.547, Seconds: 25.79\n",
      "Epoch   3/5 Batch 1160/2678 - Loss:  0.907, Acc:  0.562, Seconds: 12.99\n",
      "Epoch   3/5 Batch 1180/2678 - Loss:  0.860, Acc:  0.656, Seconds: 14.03\n",
      "Epoch   3/5 Batch 1200/2678 - Loss:  0.903, Acc:  0.609, Seconds: 19.26\n",
      "Epoch   3/5 Batch 1220/2678 - Loss:  0.939, Acc:  0.562, Seconds: 23.74\n",
      "Epoch   3/5 Batch 1240/2678 - Loss:  0.880, Acc:  0.625, Seconds: 23.14\n",
      "Epoch   3/5 Batch 1260/2678 - Loss:  0.902, Acc:  0.688, Seconds: 24.59\n",
      "Epoch   3/5 Batch 1280/2678 - Loss:  0.841, Acc:  0.672, Seconds: 16.50\n",
      "Epoch   3/5 Batch 1300/2678 - Loss:  0.946, Acc:  0.594, Seconds: 17.63\n",
      "Epoch   3/5 Batch 1320/2678 - Loss:  0.859, Acc:  0.609, Seconds: 30.78\n",
      "Average loss for this update: 0.897\n",
      "New Record!\n",
      "Epoch   3/5 Batch 1340/2678 - Loss:  0.894, Acc:  0.656, Seconds: 18.71\n",
      "Epoch   3/5 Batch 1360/2678 - Loss:  0.880, Acc:  0.672, Seconds: 11.16\n",
      "Epoch   3/5 Batch 1380/2678 - Loss:  0.895, Acc:  0.594, Seconds: 21.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3/5 Batch 1400/2678 - Loss:  0.950, Acc:  0.656, Seconds: 19.95\n",
      "Epoch   3/5 Batch 1420/2678 - Loss:  0.890, Acc:  0.656, Seconds: 15.35\n",
      "Epoch   3/5 Batch 1440/2678 - Loss:  0.838, Acc:  0.688, Seconds: 17.46\n",
      "Epoch   3/5 Batch 1460/2678 - Loss:  0.944, Acc:  0.625, Seconds: 20.28\n",
      "Epoch   3/5 Batch 1480/2678 - Loss:  0.917, Acc:  0.672, Seconds: 13.33\n",
      "Epoch   3/5 Batch 1500/2678 - Loss:  0.908, Acc:  0.516, Seconds: 11.54\n",
      "Epoch   3/5 Batch 1520/2678 - Loss:  0.964, Acc:  0.547, Seconds: 18.25\n",
      "Epoch   3/5 Batch 1540/2678 - Loss:  0.894, Acc:  0.672, Seconds: 8.05\n",
      "Epoch   3/5 Batch 1560/2678 - Loss:  0.902, Acc:  0.672, Seconds: 18.25\n",
      "Epoch   3/5 Batch 1580/2678 - Loss:  0.885, Acc:  0.547, Seconds: 14.84\n",
      "Epoch   3/5 Batch 1600/2678 - Loss:  0.898, Acc:  0.672, Seconds: 14.66\n",
      "Epoch   3/5 Batch 1620/2678 - Loss:  0.925, Acc:  0.578, Seconds: 15.58\n",
      "Epoch   3/5 Batch 1640/2678 - Loss:  0.867, Acc:  0.703, Seconds: 11.80\n",
      "Epoch   3/5 Batch 1660/2678 - Loss:  0.892, Acc:  0.547, Seconds: 9.41\n",
      "Epoch   3/5 Batch 1680/2678 - Loss:  0.884, Acc:  0.578, Seconds: 16.15\n",
      "Epoch   3/5 Batch 1700/2678 - Loss:  0.877, Acc:  0.688, Seconds: 11.42\n",
      "Epoch   3/5 Batch 1720/2678 - Loss:  0.879, Acc:  0.531, Seconds: 15.19\n",
      "Epoch   3/5 Batch 1740/2678 - Loss:  0.874, Acc:  0.750, Seconds: 12.84\n",
      "Epoch   3/5 Batch 1760/2678 - Loss:  0.899, Acc:  0.609, Seconds: 17.16\n",
      "Epoch   3/5 Batch 1780/2678 - Loss:  0.924, Acc:  0.688, Seconds: 12.32\n",
      "Epoch   3/5 Batch 1800/2678 - Loss:  0.833, Acc:  0.609, Seconds: 10.65\n",
      "Epoch   3/5 Batch 1820/2678 - Loss:  0.899, Acc:  0.594, Seconds: 18.00\n",
      "Epoch   3/5 Batch 1840/2678 - Loss:  0.945, Acc:  0.438, Seconds: 24.00\n",
      "Epoch   3/5 Batch 1860/2678 - Loss:  0.906, Acc:  0.625, Seconds: 12.35\n",
      "Epoch   3/5 Batch 1880/2678 - Loss:  0.895, Acc:  0.594, Seconds: 19.56\n",
      "Epoch   3/5 Batch 1900/2678 - Loss:  0.862, Acc:  0.594, Seconds: 16.25\n",
      "Epoch   3/5 Batch 1920/2678 - Loss:  0.892, Acc:  0.594, Seconds: 14.05\n",
      "Epoch   3/5 Batch 1940/2678 - Loss:  0.913, Acc:  0.578, Seconds: 13.28\n",
      "Epoch   3/5 Batch 1960/2678 - Loss:  0.926, Acc:  0.641, Seconds: 13.12\n",
      "Epoch   3/5 Batch 1980/2678 - Loss:  0.857, Acc:  0.531, Seconds: 21.42\n",
      "Epoch   3/5 Batch 2000/2678 - Loss:  0.904, Acc:  0.609, Seconds: 22.49\n",
      "Average loss for this update: 0.898\n",
      "No Improvement.\n",
      "Epoch   3/5 Batch 2020/2678 - Loss:  0.904, Acc:  0.625, Seconds: 10.86\n",
      "Epoch   3/5 Batch 2040/2678 - Loss:  0.903, Acc:  0.500, Seconds: 15.02\n",
      "Epoch   3/5 Batch 2060/2678 - Loss:  0.881, Acc:  0.578, Seconds: 20.78\n",
      "Epoch   3/5 Batch 2080/2678 - Loss:  0.883, Acc:  0.672, Seconds: 12.31\n",
      "Epoch   3/5 Batch 2100/2678 - Loss:  0.896, Acc:  0.641, Seconds: 16.47\n",
      "Epoch   3/5 Batch 2120/2678 - Loss:  0.884, Acc:  0.641, Seconds: 10.56\n",
      "Epoch   3/5 Batch 2140/2678 - Loss:  0.873, Acc:  0.719, Seconds: 11.87\n",
      "Epoch   3/5 Batch 2160/2678 - Loss:  0.850, Acc:  0.703, Seconds: 11.74\n",
      "Epoch   3/5 Batch 2180/2678 - Loss:  0.897, Acc:  0.641, Seconds: 23.58\n",
      "Epoch   3/5 Batch 2200/2678 - Loss:  0.907, Acc:  0.672, Seconds: 16.39\n",
      "Epoch   3/5 Batch 2220/2678 - Loss:  0.901, Acc:  0.656, Seconds: 17.11\n",
      "Epoch   3/5 Batch 2240/2678 - Loss:  0.932, Acc:  0.625, Seconds: 13.32\n",
      "Epoch   3/5 Batch 2260/2678 - Loss:  0.871, Acc:  0.594, Seconds: 14.68\n",
      "Epoch   3/5 Batch 2280/2678 - Loss:  0.878, Acc:  0.672, Seconds: 11.60\n",
      "Epoch   3/5 Batch 2300/2678 - Loss:  0.879, Acc:  0.688, Seconds: 16.95\n",
      "Epoch   3/5 Batch 2320/2678 - Loss:  0.892, Acc:  0.547, Seconds: 16.92\n",
      "Epoch   3/5 Batch 2340/2678 - Loss:  0.921, Acc:  0.625, Seconds: 24.38\n",
      "Epoch   3/5 Batch 2360/2678 - Loss:  0.927, Acc:  0.719, Seconds: 15.50\n",
      "Epoch   3/5 Batch 2380/2678 - Loss:  0.904, Acc:  0.688, Seconds: 10.41\n",
      "Epoch   3/5 Batch 2400/2678 - Loss:  0.861, Acc:  0.750, Seconds: 10.32\n",
      "Epoch   3/5 Batch 2420/2678 - Loss:  0.845, Acc:  0.703, Seconds: 14.39\n",
      "Epoch   3/5 Batch 2440/2678 - Loss:  0.908, Acc:  0.656, Seconds: 12.84\n",
      "Epoch   3/5 Batch 2460/2678 - Loss:  0.850, Acc:  0.688, Seconds: 24.07\n",
      "Epoch   3/5 Batch 2480/2678 - Loss:  0.942, Acc:  0.688, Seconds: 14.57\n",
      "Epoch   3/5 Batch 2500/2678 - Loss:  0.872, Acc:  0.547, Seconds: 18.67\n",
      "Epoch   3/5 Batch 2520/2678 - Loss:  0.883, Acc:  0.469, Seconds: 12.76\n",
      "Epoch   3/5 Batch 2540/2678 - Loss:  0.894, Acc:  0.656, Seconds: 21.42\n",
      "Epoch   3/5 Batch 2560/2678 - Loss:  0.907, Acc:  0.500, Seconds: 9.10\n",
      "Epoch   3/5 Batch 2580/2678 - Loss:  0.884, Acc:  0.656, Seconds: 21.47\n",
      "Epoch   3/5 Batch 2600/2678 - Loss:  0.883, Acc:  0.531, Seconds: 20.38\n",
      "Epoch   3/5 Batch 2620/2678 - Loss:  0.812, Acc:  0.688, Seconds: 14.70\n",
      "Epoch   3/5 Batch 2640/2678 - Loss:  0.882, Acc:  0.672, Seconds: 14.79\n",
      "Epoch   3/5 Batch 2660/2678 - Loss:  0.914, Acc:  0.594, Seconds: 16.80\n",
      "Average loss for this update: 0.888\n",
      "New Record!\n",
      "Starting\n",
      "Finished first\n",
      "Epoch   4/5 Batch   20/2678 - Loss:  0.922, Acc:  0.594, Seconds: 9.02\n",
      "Epoch   4/5 Batch   40/2678 - Loss:  0.904, Acc:  0.578, Seconds: 8.05\n",
      "Epoch   4/5 Batch   60/2678 - Loss:  0.859, Acc:  0.766, Seconds: 25.26\n",
      "Epoch   4/5 Batch   80/2678 - Loss:  0.865, Acc:  0.688, Seconds: 14.90\n",
      "Epoch   4/5 Batch  100/2678 - Loss:  0.922, Acc:  0.625, Seconds: 21.20\n",
      "Epoch   4/5 Batch  120/2678 - Loss:  0.911, Acc:  0.516, Seconds: 13.25\n",
      "Epoch   4/5 Batch  140/2678 - Loss:  0.883, Acc:  0.594, Seconds: 14.73\n",
      "Epoch   4/5 Batch  160/2678 - Loss:  0.889, Acc:  0.562, Seconds: 15.63\n",
      "Epoch   4/5 Batch  180/2678 - Loss:  0.857, Acc:  0.703, Seconds: 20.91\n",
      "Epoch   4/5 Batch  200/2678 - Loss:  0.832, Acc:  0.688, Seconds: 16.92\n",
      "Epoch   4/5 Batch  220/2678 - Loss:  0.930, Acc:  0.672, Seconds: 11.58\n",
      "Epoch   4/5 Batch  240/2678 - Loss:  0.911, Acc:  0.656, Seconds: 17.53\n",
      "Epoch   4/5 Batch  260/2678 - Loss:  0.874, Acc:  0.641, Seconds: 11.96\n",
      "Epoch   4/5 Batch  280/2678 - Loss:  0.875, Acc:  0.656, Seconds: 13.43\n",
      "Epoch   4/5 Batch  300/2678 - Loss:  0.888, Acc:  0.594, Seconds: 11.29\n",
      "Epoch   4/5 Batch  320/2678 - Loss:  0.884, Acc:  0.609, Seconds: 18.97\n",
      "Epoch   4/5 Batch  340/2678 - Loss:  0.876, Acc:  0.656, Seconds: 23.56\n",
      "Epoch   4/5 Batch  360/2678 - Loss:  0.877, Acc:  0.594, Seconds: 14.73\n",
      "Epoch   4/5 Batch  380/2678 - Loss:  0.877, Acc:  0.578, Seconds: 9.86\n",
      "Epoch   4/5 Batch  400/2678 - Loss:  0.887, Acc:  0.547, Seconds: 9.58\n",
      "Epoch   4/5 Batch  420/2678 - Loss:  0.853, Acc:  0.547, Seconds: 17.13\n",
      "Epoch   4/5 Batch  440/2678 - Loss:  0.861, Acc:  0.688, Seconds: 13.62\n",
      "Epoch   4/5 Batch  460/2678 - Loss:  0.888, Acc:  0.594, Seconds: 13.99\n",
      "Epoch   4/5 Batch  480/2678 - Loss:  0.854, Acc:  0.719, Seconds: 11.99\n",
      "Epoch   4/5 Batch  500/2678 - Loss:  0.892, Acc:  0.594, Seconds: 10.62\n",
      "Epoch   4/5 Batch  520/2678 - Loss:  0.858, Acc:  0.672, Seconds: 10.75\n",
      "Epoch   4/5 Batch  540/2678 - Loss:  0.852, Acc:  0.766, Seconds: 12.89\n",
      "Epoch   4/5 Batch  560/2678 - Loss:  0.885, Acc:  0.609, Seconds: 12.34\n",
      "Epoch   4/5 Batch  580/2678 - Loss:  0.858, Acc:  0.641, Seconds: 10.86\n",
      "Epoch   4/5 Batch  600/2678 - Loss:  0.897, Acc:  0.641, Seconds: 13.10\n",
      "Epoch   4/5 Batch  620/2678 - Loss:  0.842, Acc:  0.609, Seconds: 18.23\n",
      "Epoch   4/5 Batch  640/2678 - Loss:  0.900, Acc:  0.500, Seconds: 12.23\n",
      "Epoch   4/5 Batch  660/2678 - Loss:  0.868, Acc:  0.656, Seconds: 20.58\n",
      "Average loss for this update: 0.88\n",
      "New Record!\n",
      "Epoch   4/5 Batch  680/2678 - Loss:  0.869, Acc:  0.688, Seconds: 25.74\n",
      "Epoch   4/5 Batch  700/2678 - Loss:  0.878, Acc:  0.609, Seconds: 11.42\n",
      "Epoch   4/5 Batch  720/2678 - Loss:  0.890, Acc:  0.688, Seconds: 10.99\n",
      "Epoch   4/5 Batch  740/2678 - Loss:  0.875, Acc:  0.703, Seconds: 11.07\n",
      "Epoch   4/5 Batch  760/2678 - Loss:  0.852, Acc:  0.703, Seconds: 19.32\n",
      "Epoch   4/5 Batch  780/2678 - Loss:  0.893, Acc:  0.562, Seconds: 15.74\n",
      "Epoch   4/5 Batch  800/2678 - Loss:  0.866, Acc:  0.688, Seconds: 11.38\n",
      "Epoch   4/5 Batch  820/2678 - Loss:  0.885, Acc:  0.594, Seconds: 16.51\n",
      "Epoch   4/5 Batch  840/2678 - Loss:  0.905, Acc:  0.531, Seconds: 12.03\n",
      "Epoch   4/5 Batch  860/2678 - Loss:  0.864, Acc:  0.578, Seconds: 10.71\n",
      "Epoch   4/5 Batch  880/2678 - Loss:  0.876, Acc:  0.641, Seconds: 13.19\n",
      "Epoch   4/5 Batch  900/2678 - Loss:  0.896, Acc:  0.625, Seconds: 10.67\n",
      "Epoch   4/5 Batch  920/2678 - Loss:  0.880, Acc:  0.656, Seconds: 13.42\n",
      "Epoch   4/5 Batch  940/2678 - Loss:  0.864, Acc:  0.531, Seconds: 14.74\n",
      "Epoch   4/5 Batch  960/2678 - Loss:  0.901, Acc:  0.688, Seconds: 18.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4/5 Batch  980/2678 - Loss:  0.880, Acc:  0.641, Seconds: 10.29\n",
      "Epoch   4/5 Batch 1000/2678 - Loss:  0.860, Acc:  0.500, Seconds: 18.88\n",
      "Epoch   4/5 Batch 1020/2678 - Loss:  0.838, Acc:  0.672, Seconds: 13.71\n",
      "Epoch   4/5 Batch 1040/2678 - Loss:  0.878, Acc:  0.656, Seconds: 9.91\n",
      "Epoch   4/5 Batch 1060/2678 - Loss:  0.841, Acc:  0.766, Seconds: 22.48\n",
      "Epoch   4/5 Batch 1080/2678 - Loss:  0.876, Acc:  0.609, Seconds: 10.46\n",
      "Epoch   4/5 Batch 1100/2678 - Loss:  0.878, Acc:  0.516, Seconds: 8.72\n",
      "Epoch   4/5 Batch 1120/2678 - Loss:  0.858, Acc:  0.703, Seconds: 10.97\n",
      "Epoch   4/5 Batch 1140/2678 - Loss:  0.850, Acc:  0.516, Seconds: 20.71\n",
      "Epoch   4/5 Batch 1160/2678 - Loss:  0.872, Acc:  0.609, Seconds: 10.73\n",
      "Epoch   4/5 Batch 1180/2678 - Loss:  0.823, Acc:  0.594, Seconds: 11.82\n",
      "Epoch   4/5 Batch 1200/2678 - Loss:  0.857, Acc:  0.609, Seconds: 12.59\n",
      "Epoch   4/5 Batch 1220/2678 - Loss:  0.935, Acc:  0.516, Seconds: 17.23\n",
      "Epoch   4/5 Batch 1240/2678 - Loss:  0.876, Acc:  0.562, Seconds: 15.34\n",
      "Epoch   4/5 Batch 1260/2678 - Loss:  0.878, Acc:  0.688, Seconds: 16.44\n",
      "Epoch   4/5 Batch 1280/2678 - Loss:  0.830, Acc:  0.641, Seconds: 12.68\n",
      "Epoch   4/5 Batch 1300/2678 - Loss:  0.912, Acc:  0.625, Seconds: 13.60\n",
      "Epoch   4/5 Batch 1320/2678 - Loss:  0.847, Acc:  0.531, Seconds: 22.76\n",
      "Average loss for this update: 0.872\n",
      "New Record!\n",
      "Epoch   4/5 Batch 1340/2678 - Loss:  0.856, Acc:  0.656, Seconds: 14.38\n",
      "Epoch   4/5 Batch 1360/2678 - Loss:  0.871, Acc:  0.562, Seconds: 8.77\n",
      "Epoch   4/5 Batch 1380/2678 - Loss:  0.879, Acc:  0.594, Seconds: 15.86\n",
      "Epoch   4/5 Batch 1400/2678 - Loss:  0.916, Acc:  0.609, Seconds: 14.69\n",
      "Epoch   4/5 Batch 1420/2678 - Loss:  0.889, Acc:  0.719, Seconds: 12.06\n",
      "Epoch   4/5 Batch 1440/2678 - Loss:  0.817, Acc:  0.578, Seconds: 12.67\n",
      "Epoch   4/5 Batch 1460/2678 - Loss:  0.895, Acc:  0.734, Seconds: 19.66\n",
      "Epoch   4/5 Batch 1480/2678 - Loss:  0.880, Acc:  0.719, Seconds: 13.39\n",
      "Epoch   4/5 Batch 1500/2678 - Loss:  0.881, Acc:  0.500, Seconds: 11.63\n",
      "Epoch   4/5 Batch 1520/2678 - Loss:  0.904, Acc:  0.609, Seconds: 20.37\n",
      "Epoch   4/5 Batch 1540/2678 - Loss:  0.872, Acc:  0.719, Seconds: 8.14\n",
      "Epoch   4/5 Batch 1560/2678 - Loss:  0.868, Acc:  0.641, Seconds: 19.32\n",
      "Epoch   4/5 Batch 1580/2678 - Loss:  0.870, Acc:  0.688, Seconds: 14.85\n",
      "Epoch   4/5 Batch 1600/2678 - Loss:  0.902, Acc:  0.688, Seconds: 14.45\n",
      "Epoch   4/5 Batch 1620/2678 - Loss:  0.878, Acc:  0.531, Seconds: 16.22\n",
      "Epoch   4/5 Batch 1640/2678 - Loss:  0.837, Acc:  0.781, Seconds: 11.87\n",
      "Epoch   4/5 Batch 1660/2678 - Loss:  0.867, Acc:  0.641, Seconds: 8.69\n",
      "Epoch   4/5 Batch 1680/2678 - Loss:  0.857, Acc:  0.609, Seconds: 16.19\n",
      "Epoch   4/5 Batch 1700/2678 - Loss:  0.849, Acc:  0.641, Seconds: 10.63\n",
      "Epoch   4/5 Batch 1720/2678 - Loss:  0.864, Acc:  0.594, Seconds: 16.22\n",
      "Epoch   4/5 Batch 1740/2678 - Loss:  0.850, Acc:  0.766, Seconds: 13.17\n",
      "Epoch   4/5 Batch 1760/2678 - Loss:  0.887, Acc:  0.672, Seconds: 17.94\n",
      "Epoch   4/5 Batch 1780/2678 - Loss:  0.879, Acc:  0.750, Seconds: 12.73\n",
      "Epoch   4/5 Batch 1800/2678 - Loss:  0.823, Acc:  0.672, Seconds: 9.51\n",
      "Epoch   4/5 Batch 1820/2678 - Loss:  0.859, Acc:  0.578, Seconds: 16.91\n",
      "Epoch   4/5 Batch 1840/2678 - Loss:  0.921, Acc:  0.500, Seconds: 23.65\n",
      "Epoch   4/5 Batch 1860/2678 - Loss:  0.882, Acc:  0.641, Seconds: 11.02\n",
      "Epoch   4/5 Batch 1880/2678 - Loss:  0.879, Acc:  0.641, Seconds: 19.20\n",
      "Epoch   4/5 Batch 1900/2678 - Loss:  0.830, Acc:  0.672, Seconds: 13.77\n",
      "Epoch   4/5 Batch 1920/2678 - Loss:  0.868, Acc:  0.734, Seconds: 12.86\n",
      "Epoch   4/5 Batch 1940/2678 - Loss:  0.885, Acc:  0.625, Seconds: 12.73\n",
      "Epoch   4/5 Batch 1960/2678 - Loss:  0.881, Acc:  0.641, Seconds: 11.63\n",
      "Epoch   4/5 Batch 1980/2678 - Loss:  0.830, Acc:  0.562, Seconds: 18.51\n",
      "Epoch   4/5 Batch 2000/2678 - Loss:  0.887, Acc:  0.641, Seconds: 20.13\n",
      "Average loss for this update: 0.872\n",
      "No Improvement.\n",
      "Epoch   4/5 Batch 2020/2678 - Loss:  0.872, Acc:  0.656, Seconds: 9.86\n",
      "Epoch   4/5 Batch 2040/2678 - Loss:  0.861, Acc:  0.516, Seconds: 12.92\n",
      "Epoch   4/5 Batch 2060/2678 - Loss:  0.879, Acc:  0.531, Seconds: 19.11\n",
      "Epoch   4/5 Batch 2080/2678 - Loss:  0.844, Acc:  0.766, Seconds: 11.36\n",
      "Epoch   4/5 Batch 2100/2678 - Loss:  0.864, Acc:  0.641, Seconds: 14.28\n",
      "Epoch   4/5 Batch 2120/2678 - Loss:  0.865, Acc:  0.625, Seconds: 9.10\n",
      "Epoch   4/5 Batch 2140/2678 - Loss:  0.856, Acc:  0.719, Seconds: 11.43\n",
      "Epoch   4/5 Batch 2160/2678 - Loss:  0.835, Acc:  0.688, Seconds: 10.00\n",
      "Epoch   4/5 Batch 2180/2678 - Loss:  0.882, Acc:  0.688, Seconds: 21.42\n",
      "Epoch   4/5 Batch 2200/2678 - Loss:  0.874, Acc:  0.625, Seconds: 14.05\n",
      "Epoch   4/5 Batch 2220/2678 - Loss:  0.902, Acc:  0.672, Seconds: 15.76\n",
      "Epoch   4/5 Batch 2240/2678 - Loss:  0.885, Acc:  0.594, Seconds: 12.19\n",
      "Epoch   4/5 Batch 2260/2678 - Loss:  0.837, Acc:  0.562, Seconds: 12.91\n",
      "Epoch   4/5 Batch 2280/2678 - Loss:  0.850, Acc:  0.656, Seconds: 10.85\n",
      "Epoch   4/5 Batch 2300/2678 - Loss:  0.854, Acc:  0.703, Seconds: 15.28\n",
      "Epoch   4/5 Batch 2320/2678 - Loss:  0.851, Acc:  0.453, Seconds: 14.67\n",
      "Epoch   4/5 Batch 2340/2678 - Loss:  0.895, Acc:  0.594, Seconds: 20.74\n",
      "Epoch   4/5 Batch 2360/2678 - Loss:  0.913, Acc:  0.656, Seconds: 13.14\n",
      "Epoch   4/5 Batch 2380/2678 - Loss:  0.884, Acc:  0.641, Seconds: 9.32\n",
      "Epoch   4/5 Batch 2400/2678 - Loss:  0.858, Acc:  0.828, Seconds: 8.88\n",
      "Epoch   4/5 Batch 2420/2678 - Loss:  0.829, Acc:  0.719, Seconds: 12.69\n",
      "Epoch   4/5 Batch 2440/2678 - Loss:  0.873, Acc:  0.688, Seconds: 12.33\n",
      "Epoch   4/5 Batch 2460/2678 - Loss:  0.842, Acc:  0.656, Seconds: 21.47\n",
      "Epoch   4/5 Batch 2480/2678 - Loss:  0.907, Acc:  0.641, Seconds: 12.25\n",
      "Epoch   4/5 Batch 2500/2678 - Loss:  0.848, Acc:  0.625, Seconds: 16.69\n",
      "Epoch   4/5 Batch 2520/2678 - Loss:  0.884, Acc:  0.438, Seconds: 11.09\n",
      "Epoch   4/5 Batch 2540/2678 - Loss:  0.862, Acc:  0.578, Seconds: 19.16\n",
      "Epoch   4/5 Batch 2560/2678 - Loss:  0.870, Acc:  0.562, Seconds: 8.47\n",
      "Epoch   4/5 Batch 2580/2678 - Loss:  0.852, Acc:  0.703, Seconds: 19.83\n",
      "Epoch   4/5 Batch 2600/2678 - Loss:  0.864, Acc:  0.641, Seconds: 19.16\n",
      "Epoch   4/5 Batch 2620/2678 - Loss:  0.799, Acc:  0.656, Seconds: 13.34\n",
      "Epoch   4/5 Batch 2640/2678 - Loss:  0.868, Acc:  0.672, Seconds: 13.53\n",
      "Epoch   4/5 Batch 2660/2678 - Loss:  0.882, Acc:  0.594, Seconds: 16.15\n",
      "Average loss for this update: 0.865\n",
      "New Record!\n",
      "Starting\n",
      "Finished first\n",
      "Epoch   5/5 Batch   20/2678 - Loss:  0.892, Acc:  0.656, Seconds: 7.63\n",
      "Epoch   5/5 Batch   40/2678 - Loss:  0.877, Acc:  0.609, Seconds: 7.82\n",
      "Epoch   5/5 Batch   60/2678 - Loss:  0.827, Acc:  0.734, Seconds: 21.06\n",
      "Epoch   5/5 Batch   80/2678 - Loss:  0.837, Acc:  0.656, Seconds: 12.58\n",
      "Epoch   5/5 Batch  100/2678 - Loss:  0.906, Acc:  0.656, Seconds: 18.43\n",
      "Epoch   5/5 Batch  120/2678 - Loss:  0.869, Acc:  0.562, Seconds: 11.66\n",
      "Epoch   5/5 Batch  140/2678 - Loss:  0.868, Acc:  0.531, Seconds: 14.10\n",
      "Epoch   5/5 Batch  160/2678 - Loss:  0.857, Acc:  0.516, Seconds: 12.63\n",
      "Epoch   5/5 Batch  180/2678 - Loss:  0.832, Acc:  0.609, Seconds: 17.68\n",
      "Epoch   5/5 Batch  200/2678 - Loss:  0.790, Acc:  0.625, Seconds: 15.27\n",
      "Epoch   5/5 Batch  220/2678 - Loss:  0.882, Acc:  0.688, Seconds: 9.28\n",
      "Epoch   5/5 Batch  240/2678 - Loss:  0.897, Acc:  0.594, Seconds: 15.43\n",
      "Epoch   5/5 Batch  260/2678 - Loss:  0.853, Acc:  0.672, Seconds: 11.30\n",
      "Epoch   5/5 Batch  280/2678 - Loss:  0.856, Acc:  0.703, Seconds: 12.21\n",
      "Epoch   5/5 Batch  300/2678 - Loss:  0.864, Acc:  0.656, Seconds: 9.65\n",
      "Epoch   5/5 Batch  320/2678 - Loss:  0.868, Acc:  0.578, Seconds: 16.90\n",
      "Epoch   5/5 Batch  340/2678 - Loss:  0.863, Acc:  0.516, Seconds: 19.85\n",
      "Epoch   5/5 Batch  360/2678 - Loss:  0.844, Acc:  0.672, Seconds: 12.08\n",
      "Epoch   5/5 Batch  380/2678 - Loss:  0.840, Acc:  0.500, Seconds: 9.03\n",
      "Epoch   5/5 Batch  400/2678 - Loss:  0.843, Acc:  0.609, Seconds: 8.59\n",
      "Epoch   5/5 Batch  420/2678 - Loss:  0.823, Acc:  0.641, Seconds: 14.51\n",
      "Epoch   5/5 Batch  440/2678 - Loss:  0.824, Acc:  0.594, Seconds: 11.83\n",
      "Epoch   5/5 Batch  460/2678 - Loss:  0.856, Acc:  0.594, Seconds: 12.90\n",
      "Epoch   5/5 Batch  480/2678 - Loss:  0.838, Acc:  0.672, Seconds: 10.93\n",
      "Epoch   5/5 Batch  500/2678 - Loss:  0.858, Acc:  0.609, Seconds: 10.17\n",
      "Epoch   5/5 Batch  520/2678 - Loss:  0.836, Acc:  0.719, Seconds: 9.18\n",
      "Epoch   5/5 Batch  540/2678 - Loss:  0.852, Acc:  0.703, Seconds: 11.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/5 Batch  560/2678 - Loss:  0.878, Acc:  0.609, Seconds: 10.53\n",
      "Epoch   5/5 Batch  580/2678 - Loss:  0.842, Acc:  0.641, Seconds: 9.58\n",
      "Epoch   5/5 Batch  600/2678 - Loss:  0.870, Acc:  0.594, Seconds: 13.35\n",
      "Epoch   5/5 Batch  620/2678 - Loss:  0.826, Acc:  0.641, Seconds: 16.96\n",
      "Epoch   5/5 Batch  640/2678 - Loss:  0.852, Acc:  0.578, Seconds: 10.46\n",
      "Epoch   5/5 Batch  660/2678 - Loss:  0.828, Acc:  0.672, Seconds: 17.83\n",
      "Average loss for this update: 0.853\n",
      "New Record!\n",
      "Epoch   5/5 Batch  680/2678 - Loss:  0.840, Acc:  0.766, Seconds: 22.05\n",
      "Epoch   5/5 Batch  700/2678 - Loss:  0.855, Acc:  0.547, Seconds: 9.80\n",
      "Epoch   5/5 Batch  720/2678 - Loss:  0.870, Acc:  0.656, Seconds: 9.47\n",
      "Epoch   5/5 Batch  740/2678 - Loss:  0.817, Acc:  0.719, Seconds: 8.62\n",
      "Epoch   5/5 Batch  760/2678 - Loss:  0.827, Acc:  0.750, Seconds: 15.99\n",
      "Epoch   5/5 Batch  780/2678 - Loss:  0.875, Acc:  0.562, Seconds: 14.35\n",
      "Epoch   5/5 Batch  800/2678 - Loss:  0.841, Acc:  0.766, Seconds: 9.94\n",
      "Epoch   5/5 Batch  820/2678 - Loss:  0.863, Acc:  0.641, Seconds: 14.93\n",
      "Epoch   5/5 Batch  840/2678 - Loss:  0.875, Acc:  0.656, Seconds: 11.05\n",
      "Epoch   5/5 Batch  860/2678 - Loss:  0.846, Acc:  0.625, Seconds: 9.41\n",
      "Epoch   5/5 Batch  880/2678 - Loss:  0.853, Acc:  0.656, Seconds: 12.64\n",
      "Epoch   5/5 Batch  900/2678 - Loss:  0.876, Acc:  0.578, Seconds: 9.77\n",
      "Epoch   5/5 Batch  920/2678 - Loss:  0.867, Acc:  0.531, Seconds: 10.83\n",
      "Epoch   5/5 Batch  940/2678 - Loss:  0.856, Acc:  0.531, Seconds: 12.60\n",
      "Epoch   5/5 Batch  960/2678 - Loss:  0.874, Acc:  0.641, Seconds: 16.83\n",
      "Epoch   5/5 Batch  980/2678 - Loss:  0.870, Acc:  0.641, Seconds: 9.94\n",
      "Epoch   5/5 Batch 1000/2678 - Loss:  0.832, Acc:  0.547, Seconds: 15.97\n",
      "Epoch   5/5 Batch 1020/2678 - Loss:  0.810, Acc:  0.609, Seconds: 12.91\n",
      "Epoch   5/5 Batch 1040/2678 - Loss:  0.874, Acc:  0.625, Seconds: 8.88\n",
      "Epoch   5/5 Batch 1060/2678 - Loss:  0.830, Acc:  0.656, Seconds: 21.30\n",
      "Epoch   5/5 Batch 1080/2678 - Loss:  0.852, Acc:  0.625, Seconds: 10.21\n",
      "Epoch   5/5 Batch 1100/2678 - Loss:  0.842, Acc:  0.562, Seconds: 7.54\n",
      "Epoch   5/5 Batch 1120/2678 - Loss:  0.833, Acc:  0.797, Seconds: 9.61\n",
      "Epoch   5/5 Batch 1140/2678 - Loss:  0.829, Acc:  0.625, Seconds: 19.07\n",
      "Epoch   5/5 Batch 1160/2678 - Loss:  0.869, Acc:  0.594, Seconds: 9.74\n",
      "Epoch   5/5 Batch 1180/2678 - Loss:  0.822, Acc:  0.719, Seconds: 9.63\n",
      "Epoch   5/5 Batch 1200/2678 - Loss:  0.855, Acc:  0.641, Seconds: 11.22\n",
      "Epoch   5/5 Batch 1220/2678 - Loss:  0.906, Acc:  0.578, Seconds: 14.97\n",
      "Epoch   5/5 Batch 1240/2678 - Loss:  0.867, Acc:  0.562, Seconds: 13.69\n",
      "Epoch   5/5 Batch 1260/2678 - Loss:  0.841, Acc:  0.781, Seconds: 13.85\n",
      "Epoch   5/5 Batch 1280/2678 - Loss:  0.803, Acc:  0.672, Seconds: 11.83\n",
      "Epoch   5/5 Batch 1300/2678 - Loss:  0.903, Acc:  0.625, Seconds: 12.22\n",
      "Epoch   5/5 Batch 1320/2678 - Loss:  0.823, Acc:  0.625, Seconds: 19.65\n",
      "Average loss for this update: 0.851\n",
      "New Record!\n",
      "Epoch   5/5 Batch 1340/2678 - Loss:  0.843, Acc:  0.672, Seconds: 12.78\n",
      "Epoch   5/5 Batch 1360/2678 - Loss:  0.845, Acc:  0.656, Seconds: 7.91\n",
      "Epoch   5/5 Batch 1380/2678 - Loss:  0.857, Acc:  0.562, Seconds: 14.95\n",
      "Epoch   5/5 Batch 1400/2678 - Loss:  0.883, Acc:  0.641, Seconds: 12.85\n",
      "Epoch   5/5 Batch 1420/2678 - Loss:  0.847, Acc:  0.641, Seconds: 11.06\n",
      "Epoch   5/5 Batch 1440/2678 - Loss:  0.791, Acc:  0.688, Seconds: 11.02\n",
      "Epoch   5/5 Batch 1460/2678 - Loss:  0.888, Acc:  0.703, Seconds: 18.53\n",
      "Epoch   5/5 Batch 1480/2678 - Loss:  0.860, Acc:  0.688, Seconds: 13.61\n",
      "Epoch   5/5 Batch 1500/2678 - Loss:  0.870, Acc:  0.516, Seconds: 9.83\n",
      "Epoch   5/5 Batch 1520/2678 - Loss:  0.876, Acc:  0.578, Seconds: 18.94\n",
      "Epoch   5/5 Batch 1540/2678 - Loss:  0.825, Acc:  0.734, Seconds: 7.07\n",
      "Epoch   5/5 Batch 1560/2678 - Loss:  0.858, Acc:  0.656, Seconds: 16.82\n",
      "Epoch   5/5 Batch 1580/2678 - Loss:  0.860, Acc:  0.750, Seconds: 13.15\n",
      "Epoch   5/5 Batch 1600/2678 - Loss:  0.847, Acc:  0.672, Seconds: 12.99\n",
      "Epoch   5/5 Batch 1620/2678 - Loss:  0.868, Acc:  0.578, Seconds: 13.73\n",
      "Epoch   5/5 Batch 1640/2678 - Loss:  0.813, Acc:  0.766, Seconds: 10.81\n",
      "Epoch   5/5 Batch 1660/2678 - Loss:  0.847, Acc:  0.641, Seconds: 7.95\n",
      "Epoch   5/5 Batch 1680/2678 - Loss:  0.839, Acc:  0.609, Seconds: 13.77\n",
      "Epoch   5/5 Batch 1700/2678 - Loss:  0.838, Acc:  0.641, Seconds: 9.44\n",
      "Epoch   5/5 Batch 1720/2678 - Loss:  0.836, Acc:  0.578, Seconds: 13.68\n",
      "Epoch   5/5 Batch 1740/2678 - Loss:  0.827, Acc:  0.750, Seconds: 11.93\n",
      "Epoch   5/5 Batch 1760/2678 - Loss:  0.849, Acc:  0.641, Seconds: 16.25\n",
      "Epoch   5/5 Batch 1780/2678 - Loss:  0.844, Acc:  0.719, Seconds: 11.06\n",
      "Epoch   5/5 Batch 1800/2678 - Loss:  0.791, Acc:  0.609, Seconds: 10.73\n",
      "Epoch   5/5 Batch 1820/2678 - Loss:  0.832, Acc:  0.625, Seconds: 16.47\n",
      "Epoch   5/5 Batch 1840/2678 - Loss:  0.877, Acc:  0.547, Seconds: 23.28\n",
      "Epoch   5/5 Batch 1860/2678 - Loss:  0.863, Acc:  0.625, Seconds: 11.90\n",
      "Epoch   5/5 Batch 1880/2678 - Loss:  0.850, Acc:  0.625, Seconds: 17.53\n",
      "Epoch   5/5 Batch 1900/2678 - Loss:  0.805, Acc:  0.609, Seconds: 13.26\n",
      "Epoch   5/5 Batch 1920/2678 - Loss:  0.832, Acc:  0.656, Seconds: 12.87\n",
      "Epoch   5/5 Batch 1940/2678 - Loss:  0.852, Acc:  0.672, Seconds: 10.72\n",
      "Epoch   5/5 Batch 1960/2678 - Loss:  0.859, Acc:  0.688, Seconds: 11.84\n",
      "Epoch   5/5 Batch 1980/2678 - Loss:  0.791, Acc:  0.578, Seconds: 19.81\n",
      "Epoch   5/5 Batch 2000/2678 - Loss:  0.857, Acc:  0.719, Seconds: 20.74\n",
      "Average loss for this update: 0.845\n",
      "New Record!\n",
      "Epoch   5/5 Batch 2020/2678 - Loss:  0.848, Acc:  0.672, Seconds: 10.03\n",
      "Epoch   5/5 Batch 2040/2678 - Loss:  0.845, Acc:  0.547, Seconds: 14.21\n",
      "Epoch   5/5 Batch 2060/2678 - Loss:  0.855, Acc:  0.562, Seconds: 19.70\n",
      "Epoch   5/5 Batch 2080/2678 - Loss:  0.826, Acc:  0.672, Seconds: 12.20\n",
      "Epoch   5/5 Batch 2100/2678 - Loss:  0.821, Acc:  0.719, Seconds: 14.91\n",
      "Epoch   5/5 Batch 2120/2678 - Loss:  0.857, Acc:  0.641, Seconds: 8.99\n",
      "Epoch   5/5 Batch 2140/2678 - Loss:  0.821, Acc:  0.688, Seconds: 11.30\n",
      "Epoch   5/5 Batch 2160/2678 - Loss:  0.820, Acc:  0.625, Seconds: 10.13\n",
      "Epoch   5/5 Batch 2180/2678 - Loss:  0.846, Acc:  0.703, Seconds: 20.70\n",
      "Epoch   5/5 Batch 2200/2678 - Loss:  0.853, Acc:  0.609, Seconds: 13.91\n",
      "Epoch   5/5 Batch 2220/2678 - Loss:  0.845, Acc:  0.688, Seconds: 15.69\n",
      "Epoch   5/5 Batch 2240/2678 - Loss:  0.852, Acc:  0.641, Seconds: 12.13\n",
      "Epoch   5/5 Batch 2260/2678 - Loss:  0.823, Acc:  0.625, Seconds: 12.62\n",
      "Epoch   5/5 Batch 2280/2678 - Loss:  0.823, Acc:  0.641, Seconds: 10.57\n",
      "Epoch   5/5 Batch 2300/2678 - Loss:  0.838, Acc:  0.688, Seconds: 15.50\n",
      "Epoch   5/5 Batch 2320/2678 - Loss:  0.847, Acc:  0.531, Seconds: 14.19\n",
      "Epoch   5/5 Batch 2340/2678 - Loss:  0.861, Acc:  0.625, Seconds: 20.56\n",
      "Epoch   5/5 Batch 2360/2678 - Loss:  0.870, Acc:  0.750, Seconds: 13.13\n",
      "Epoch   5/5 Batch 2380/2678 - Loss:  0.869, Acc:  0.688, Seconds: 9.34\n",
      "Epoch   5/5 Batch 2400/2678 - Loss:  0.827, Acc:  0.812, Seconds: 8.76\n",
      "Epoch   5/5 Batch 2420/2678 - Loss:  0.807, Acc:  0.688, Seconds: 13.56\n",
      "Epoch   5/5 Batch 2440/2678 - Loss:  0.850, Acc:  0.656, Seconds: 11.18\n",
      "Epoch   5/5 Batch 2460/2678 - Loss:  0.827, Acc:  0.609, Seconds: 21.36\n",
      "Epoch   5/5 Batch 2480/2678 - Loss:  0.900, Acc:  0.609, Seconds: 12.28\n",
      "Epoch   5/5 Batch 2500/2678 - Loss:  0.814, Acc:  0.625, Seconds: 16.77\n",
      "Epoch   5/5 Batch 2520/2678 - Loss:  0.862, Acc:  0.531, Seconds: 11.54\n",
      "Epoch   5/5 Batch 2540/2678 - Loss:  0.847, Acc:  0.625, Seconds: 20.29\n",
      "Epoch   5/5 Batch 2560/2678 - Loss:  0.842, Acc:  0.562, Seconds: 8.71\n",
      "Epoch   5/5 Batch 2580/2678 - Loss:  0.825, Acc:  0.672, Seconds: 19.56\n",
      "Epoch   5/5 Batch 2600/2678 - Loss:  0.835, Acc:  0.656, Seconds: 19.14\n",
      "Epoch   5/5 Batch 2620/2678 - Loss:  0.767, Acc:  0.688, Seconds: 12.30\n",
      "Epoch   5/5 Batch 2640/2678 - Loss:  0.824, Acc:  0.688, Seconds: 12.88\n",
      "Epoch   5/5 Batch 2660/2678 - Loss:  0.864, Acc:  0.609, Seconds: 15.09\n",
      "Average loss for this update: 0.84\n",
      "New Record!\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"./saves/best_model.ckpt\" \n",
    "if IS_TRAINING:\n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #     loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "    #     loader.restore(sess, checkpoint)\n",
    "\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('./summaries' + '/train', sess.graph)\n",
    "\n",
    "        for epoch_i in range(1, epochs+1):\n",
    "            state = sess.run(graph.initial_state)\n",
    "\n",
    "            update_loss = 0\n",
    "            batch_loss = 0\n",
    "\n",
    "            for batch_i, (x, y, length) in enumerate(get_batches(X_train, y_train, batch_size)):\n",
    "                if batch_i == 1:\n",
    "                    print(\"Starting\")\n",
    "                feed = {graph.input_data: x,\n",
    "                        graph.labels: y,\n",
    "                        graph.keep_prob: keep_probability,\n",
    "                        graph.initial_state: state, \n",
    "                        graph.lr: learning_rate}\n",
    "                start_time = time.time()\n",
    "                summary, loss, acc, state, _ = sess.run([graph.merged, \n",
    "                                                         graph.cost, \n",
    "                                                         graph.accuracy, \n",
    "                                                         graph.final_state, \n",
    "                                                         graph.optimizer], \n",
    "                                                        feed_dict=feed)\n",
    "                if batch_i == 1:\n",
    "                    print(\"Finished first\")\n",
    "\n",
    "                train_writer.add_summary(summary, epoch_i*batch_i + batch_i)\n",
    "\n",
    "                batch_loss += loss\n",
    "                update_loss += loss\n",
    "                end_time = time.time()\n",
    "                batch_time = end_time - start_time\n",
    "\n",
    "                if batch_i % display_step == 0 and batch_i > 0:\n",
    "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Acc: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(epoch_i,\n",
    "                                  epochs, \n",
    "                                  batch_i, \n",
    "                                  len(X_train) // batch_size, \n",
    "                                  batch_loss / display_step,\n",
    "                                  acc,\n",
    "                                  batch_time*display_step))\n",
    "                    batch_loss = 0\n",
    "\n",
    "                if batch_i % update_check == 0 and batch_i > 0:\n",
    "                    print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                    summary_update_loss.append(update_loss)\n",
    "\n",
    "                    # If the update loss is at a new minimum, save the model\n",
    "                    if update_loss <= min(summary_update_loss):\n",
    "                        print('New Record!') \n",
    "                        stop_early = 0\n",
    "                        saver = tf.train.Saver() \n",
    "                        saver.save(sess, checkpoint)\n",
    "\n",
    "                    else:\n",
    "                        print(\"No Improvement.\")\n",
    "                        stop_early += 1\n",
    "                        if stop_early == stop:\n",
    "                            break\n",
    "                    update_loss = 0\n",
    "\n",
    "\n",
    "            # Reduce learning rate, but not below its minimum value\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "\n",
    "            if stop_early == stop:\n",
    "                print(\"Stopping Training.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Data\n",
    "This part of the code is allocated to testing the data. On top of recording accuracy results, I also generate a confusion matrix. Since reviews are subjective and aren't concretely one rating or another, a confusion matrix helps visualize your results a lot better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_TESTING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saves/best_model.ckpt\n",
      "Total Batches: 669\n",
      "20 batches\n",
      "40 batches\n",
      "60 batches\n",
      "80 batches\n",
      "100 batches\n",
      "120 batches\n",
      "140 batches\n",
      "160 batches\n",
      "180 batches\n",
      "200 batches\n",
      "220 batches\n",
      "240 batches\n",
      "260 batches\n",
      "280 batches\n",
      "300 batches\n",
      "320 batches\n",
      "340 batches\n",
      "360 batches\n",
      "380 batches\n",
      "400 batches\n",
      "420 batches\n",
      "440 batches\n",
      "460 batches\n",
      "480 batches\n",
      "500 batches\n",
      "520 batches\n",
      "540 batches\n",
      "560 batches\n",
      "580 batches\n",
      "600 batches\n",
      "620 batches\n",
      "640 batches\n",
      "660 batches\n"
     ]
    }
   ],
   "source": [
    "if IS_TESTING:\n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        checkpoint = \"./saves/best_model.ckpt\"  \n",
    "\n",
    "        all_preds = []\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            # Load the model\n",
    "            saver.restore(sess, checkpoint)\n",
    "            test_state = sess.run(graph.initial_state)\n",
    "            print(\"Total Batches: %d\"%(len(X_test)//batch_size))\n",
    "            for ii, x in enumerate(get_test_batches(X_test, batch_size), 1):\n",
    "                if ii%20==0:\n",
    "                    print(\"%d batches\"%(ii))\n",
    "                feed = {graph.input_data: x,\n",
    "                        graph.keep_prob: keep_probability,\n",
    "                        graph.initial_state: state}\n",
    "\n",
    "                predictions = sess.run(graph.predictions, feed_dict=feed)\n",
    "                for i in range(len(predictions)):\n",
    "                    all_preds.append(predictions[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42816, 6)\n",
      "0.609188153961136\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c8d648d278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c8d5c60c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_preds = np.array(all_preds)\n",
    "print(all_preds.shape)\n",
    "y_predictions = all_preds.argmax(axis=1)\n",
    "y_true = y_test.argmax(axis=1)\n",
    "y_true = y_true[:len(y_predictions)]\n",
    "\n",
    "cm = ConfusionMatrix(y_true, y_predictions)\n",
    "cm.plot(backend='seaborn', normalized=True)\n",
    "plt.title('Confusion Matrix Stars prediction')\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "test_correctPred = np.equal(y_predictions, y_true)\n",
    "test_accuracy = np.mean(test_correctPred.astype(float))\n",
    "\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out the network yourself!\n",
    "User can enter a review here and see how the network does in predicting his or her review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a review in englishThis place was alright. Food was lackluster but the atmosphere was great! Service was decent\n",
      "INFO:tensorflow:Restoring parameters from ./saves/best_model.ckpt\n",
      "\n",
      "You rated the restaurant: 2 stars!\n"
     ]
    }
   ],
   "source": [
    "load_files = True\n",
    "if load_files == True:\n",
    "    word_embedding_matrix = loadfiles(\"./data/word_embedding_matrix.p\")\n",
    "    word2int = loadfiles('word2int.p')\n",
    "\n",
    "pred_text = input(\"Please enter a review in english\")\n",
    "contractions = get_contractions()\n",
    "pred_text = clean_text(pred_text, contractions)\n",
    "pred_seq = convert_to_ints(pred_text, pred=True)\n",
    "pred_seq = np.tile(pred_seq, (batch_size, 1))\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    checkpoint = \"./saves/best_model.ckpt\"  \n",
    "    all_preds = []\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        # Load the model\n",
    "        saver.restore(sess, checkpoint)\n",
    "        test_state = sess.run(graph.initial_state)\n",
    "        feed = {graph.input_data: pred_seq,\n",
    "                graph.keep_prob: keep_probability,\n",
    "                graph.initial_state: state}\n",
    "\n",
    "        predictions = sess.run(graph.predictions, feed_dict=feed)\n",
    "        for i in range(len(predictions)):\n",
    "            all_preds.append(predictions[i,:])\n",
    "all_preds = np.array(all_preds)\n",
    "y_predictions = all_preds.argmax(axis=1)\n",
    "counts = np.bincount(y_predictions)\n",
    "print(\"\\nYou rated the restaurant: \" + str(np.argmax(counts)) + \" stars!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf15",
   "language": "python",
   "name": "tf15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
